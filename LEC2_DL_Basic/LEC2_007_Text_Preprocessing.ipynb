{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdbb873-3379-4fd2-a0c4-0c29c29c24bc",
   "metadata": {},
   "source": [
    "### Created on 2025\n",
    "### @author: S.W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed542d40-336c-4b76-8d91-dd90b7df9a18",
   "metadata": {},
   "source": [
    "## í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë²¡í„°í™” ì‹¤ìŠµ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf59346-1810-4737-9b0a-fc1e64b26233",
   "metadata": {},
   "source": [
    "### ë¼ì´ë¸Œë¦¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54064f3-5a8e-49c1-835b-4c7501aa4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2482f406-b095-43ec-9128-f55509254d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë²¡í„°í™” ì‹¤ìŠµ ===\n",
      "\n",
      "ìƒ˜í”Œ í…ìŠ¤íŠ¸:\n",
      "1. Natural language processing is a fascinating field of artificial intelligence.\n",
      "2. Machine learning algorithms can process and analyze large amounts of text data.\n",
      "3. Deep learning models have revolutionized natural language understanding.\n",
      "4. Text preprocessing is an essential step in NLP pipeline.\n",
      "5. Word embeddings capture semantic relationships between words.\n",
      "6. Transformers have become the dominant architecture in modern NLP.\n",
      "7. BERT and GPT models achieve state-of-the-art results in many tasks.\n",
      "8. Text classification and sentiment analysis are common NLP applications.\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ëª…ë ¹ì–´ (ì£¼ì„ ì œê±° í›„ ì‹¤í–‰)\n",
    "\"\"\"\n",
    "pip install nltk scikit-learn gensim transformers torch\n",
    "python -m nltk.downloader punkt stopwords\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„°\n",
    "sample_texts = [\n",
    "    \"Natural language processing is a fascinating field of artificial intelligence.\",\n",
    "    \"Machine learning algorithms can process and analyze large amounts of text data.\",\n",
    "    \"Deep learning models have revolutionized natural language understanding.\",\n",
    "    \"Text preprocessing is an essential step in NLP pipeline.\",\n",
    "    \"Word embeddings capture semantic relationships between words.\",\n",
    "    \"Transformers have become the dominant architecture in modern NLP.\",\n",
    "    \"BERT and GPT models achieve state-of-the-art results in many tasks.\",\n",
    "    \"Text classification and sentiment analysis are common NLP applications.\"\n",
    "]\n",
    "\n",
    "print(\"=== í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë²¡í„°í™” ì‹¤ìŠµ ===\\n\")\n",
    "print(\"ìƒ˜í”Œ í…ìŠ¤íŠ¸:\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67616e0-461e-4ef0-94e5-fbd6849dfbb7",
   "metadata": {},
   "source": [
    "### 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb334b63-60d4-43c6-a70b-b84c4acb5937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
      "==================================================\n",
      "ì „ì²˜ë¦¬ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤:\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ 1 ì „ì²˜ë¦¬:\n",
      "ì›ë³¸: Natural language processing is a fascinating field of artificial intelligence.\n",
      "    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: natural language processing is a fascinating field of artificial intelligence.\n",
      "    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: natural language processing is a fascinating field of artificial intelligence\n",
      "    3ë‹¨ê³„ - í† í°í™”: ['natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence']\n",
      "    ë¶ˆìš©ì–´ ì˜ˆì‹œ: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'is'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'a'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'of'\n",
      "    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "âœ… ìµœì¢… ê²°ê³¼: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "í† í° ê°œìˆ˜: 7ê°œ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ 2 ì „ì²˜ë¦¬:\n",
      "ì›ë³¸: Machine learning algorithms can process and analyze large amounts of text data.\n",
      "    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: machine learning algorithms can process and analyze large amounts of text data.\n",
      "    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: machine learning algorithms can process and analyze large amounts of text data\n",
      "    3ë‹¨ê³„ - í† í°í™”: ['machine', 'learning', 'algorithms', 'can', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'text', 'data']\n",
      "    ë¶ˆìš©ì–´ ì˜ˆì‹œ: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'can'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'and'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'of'\n",
      "    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "âœ… ìµœì¢… ê²°ê³¼: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "í† í° ê°œìˆ˜: 9ê°œ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ 3 ì „ì²˜ë¦¬:\n",
      "ì›ë³¸: Deep learning models have revolutionized natural language understanding.\n",
      "    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: deep learning models have revolutionized natural language understanding.\n",
      "    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: deep learning models have revolutionized natural language understanding\n",
      "    3ë‹¨ê³„ - í† í°í™”: ['deep', 'learning', 'models', 'have', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "    ë¶ˆìš©ì–´ ì˜ˆì‹œ: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'have'\n",
      "    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "âœ… ìµœì¢… ê²°ê³¼: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "í† í° ê°œìˆ˜: 7ê°œ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ 4 ì „ì²˜ë¦¬:\n",
      "ì›ë³¸: Text preprocessing is an essential step in NLP pipeline.\n",
      "    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: text preprocessing is an essential step in nlp pipeline.\n",
      "    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: text preprocessing is an essential step in nlp pipeline\n",
      "    3ë‹¨ê³„ - í† í°í™”: ['text', 'preprocessing', 'is', 'an', 'essential', 'step', 'in', 'nlp', 'pipeline']\n",
      "    ë¶ˆìš©ì–´ ì˜ˆì‹œ: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'is'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'an'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'in'\n",
      "    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "âœ… ìµœì¢… ê²°ê³¼: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "í† í° ê°œìˆ˜: 6ê°œ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ 5 ì „ì²˜ë¦¬:\n",
      "ì›ë³¸: Word embeddings capture semantic relationships between words.\n",
      "    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: word embeddings capture semantic relationships between words.\n",
      "    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: word embeddings capture semantic relationships between words\n",
      "    3ë‹¨ê³„ - í† í°í™”: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'between', 'words']\n",
      "    ë¶ˆìš©ì–´ ì˜ˆì‹œ: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'between'\n",
      "    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "âœ… ìµœì¢… ê²°ê³¼: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "í† í° ê°œìˆ˜: 6ê°œ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ 6 ì „ì²˜ë¦¬:\n",
      "ì›ë³¸: Transformers have become the dominant architecture in modern NLP.\n",
      "    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: transformers have become the dominant architecture in modern nlp.\n",
      "    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: transformers have become the dominant architecture in modern nlp\n",
      "    3ë‹¨ê³„ - í† í°í™”: ['transformers', 'have', 'become', 'the', 'dominant', 'architecture', 'in', 'modern', 'nlp']\n",
      "    ë¶ˆìš©ì–´ ì˜ˆì‹œ: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'have'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'the'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'in'\n",
      "    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "âœ… ìµœì¢… ê²°ê³¼: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "í† í° ê°œìˆ˜: 6ê°œ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ 7 ì „ì²˜ë¦¬:\n",
      "ì›ë³¸: BERT and GPT models achieve state-of-the-art results in many tasks.\n",
      "    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: bert and gpt models achieve state-of-the-art results in many tasks.\n",
      "    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: bert and gpt models achieve stateoftheart results in many tasks\n",
      "    3ë‹¨ê³„ - í† í°í™”: ['bert', 'and', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'in', 'many', 'tasks']\n",
      "    ë¶ˆìš©ì–´ ì˜ˆì‹œ: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'and'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'in'\n",
      "    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "âœ… ìµœì¢… ê²°ê³¼: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "í† í° ê°œìˆ˜: 8ê°œ\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ ë¬¸ì„œ 8 ì „ì²˜ë¦¬:\n",
      "ì›ë³¸: Text classification and sentiment analysis are common NLP applications.\n",
      "    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: text classification and sentiment analysis are common nlp applications.\n",
      "    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: text classification and sentiment analysis are common nlp applications\n",
      "    3ë‹¨ê³„ - í† í°í™”: ['text', 'classification', 'and', 'sentiment', 'analysis', 'are', 'common', 'nlp', 'applications']\n",
      "    ë¶ˆìš©ì–´ ì˜ˆì‹œ: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'and'\n",
      "    ì œê±°ëœ ë¶ˆìš©ì–´: 'are'\n",
      "    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "âœ… ìµœì¢… ê²°ê³¼: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "í† í° ê°œìˆ˜: 7ê°œ\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def basic_preprocessing(text):\n",
    "    \"\"\"\n",
    "    ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "    \n",
    "    Returns:\n",
    "        list: ì „ì²˜ë¦¬ëœ í† í° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1ë‹¨ê³„: ì†Œë¬¸ì ë³€í™˜ (ëŒ€ì†Œë¬¸ì í†µì¼ë¡œ ì¼ê´€ì„± í™•ë³´)\n",
    "    # ì˜ˆ: \"Natural\" -> \"natural\"\n",
    "    text = text.lower()\n",
    "    print(f\"    1ë‹¨ê³„ - ì†Œë¬¸ì ë³€í™˜: {text}\")\n",
    "    \n",
    "    # 2ë‹¨ê³„: êµ¬ë‘ì  ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    # ì •ê·œí‘œí˜„ì‹ [^\\w\\s]: ë¬¸ì, ìˆ«ì, ê³µë°±ì´ ì•„ë‹Œ ëª¨ë“  ê²ƒì„ ì œê±°\n",
    "    # ì˜ˆ: \"natural!\" -> \"natural\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    print(f\"    2ë‹¨ê³„ - êµ¬ë‘ì  ì œê±°: {text}\")\n",
    "    \n",
    "    # 3ë‹¨ê³„: í† í°í™” (ë¬¸ì¥ì„ ê°œë³„ ë‹¨ì–´ë¡œ ë¶„ë¦¬)\n",
    "    # word_tokenize: NLTKì˜ í† í°í™” í•¨ìˆ˜ ì‚¬ìš©\n",
    "    tokens = word_tokenize(text)\n",
    "    print(f\"    3ë‹¨ê³„ - í† í°í™”: {tokens}\")\n",
    "    \n",
    "    # 4ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±° (ì˜ë¯¸ê°€ ì ì€ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œê±°)\n",
    "    # ë¶ˆìš©ì–´ ì˜ˆ: the, is, a, an, and, or, but ë“±\n",
    "    try:\n",
    "        # NLTK ì˜ì–´ ë¶ˆìš©ì–´ ì‚¬ì „ ë¡œë“œ\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        print(f\"    ë¶ˆìš©ì–´ ì˜ˆì‹œ: {list(stop_words)[:10]}...\")\n",
    "        \n",
    "        # ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ í† í°ë§Œ ìœ ì§€\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                filtered_tokens.append(token)\n",
    "            else:\n",
    "                print(f\"    ì œê±°ëœ ë¶ˆìš©ì–´: '{token}'\")\n",
    "        \n",
    "        tokens = filtered_tokens\n",
    "        \n",
    "    except LookupError:\n",
    "        # NLTK ë¶ˆìš©ì–´ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì•ˆ ì²˜ë¦¬\n",
    "        print(\"    NLTK stopwords not downloaded. Using basic preprocessing.\")\n",
    "        print(\"    ëŒ€ì•ˆ: ê¸¸ì´ê°€ 2 ì´í•˜ì¸ ë‹¨ì–´ ì œê±°\")\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    print(f\"    4ë‹¨ê³„ - ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: {tokens}\")\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ê° ìƒ˜í”Œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "preprocessed_texts = []  # ì „ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "print(\"ì „ì²˜ë¦¬ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤:\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"ğŸ“ ë¬¸ì„œ {i+1} ì „ì²˜ë¦¬:\")\n",
    "    print(f\"ì›ë³¸: {text}\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ (ê° ë‹¨ê³„ë³„ ì§„í–‰ìƒí™©ì´ ì¶œë ¥ë¨)\n",
    "    tokens = basic_preprocessing(text)\n",
    "    \n",
    "    # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
    "    preprocessed_texts.append(tokens)\n",
    "    \n",
    "    print(f\"âœ… ìµœì¢… ê²°ê³¼: {tokens}\")\n",
    "    print(f\"í† í° ê°œìˆ˜: {len(tokens)}ê°œ\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b650271-02c5-4b0a-b975-44f9532bc139",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944eee61-8a1b-4468-be1e-1e947ffe6338",
   "metadata": {},
   "source": [
    "### 2. Bag of Words (BoW)\n",
    "\n",
    "Bag of Words (BoW) ê°œë…:\n",
    "- ë¬¸ì„œë¥¼ ë‹¨ì–´ì˜ ì§‘í•©(ê°€ë°©)ìœ¼ë¡œ í‘œí˜„\n",
    "- ë‹¨ì–´ì˜ ìˆœì„œëŠ” ë¬´ì‹œí•˜ê³ , ì¶œí˜„ ë¹ˆë„ë§Œ ê³ ë ¤\n",
    "- ê° ë¬¸ì„œë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ë¡œ ë³€í™˜\n",
    "- ë²¡í„°ì˜ ê° ì°¨ì›ì€ íŠ¹ì • ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bccf4627-9a00-4c09-a8a6-547b8fde557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2. Bag of Words (BoW)\n",
      "==================================================\n",
      "======================================================================\n",
      " 1ë‹¨ê³„: ì „ì²´ ë¬¸ì„œì—ì„œ ê³ ìœ  ë‹¨ì–´ ìˆ˜ì§‘\n",
      "  ë¬¸ì„œ 1ì˜ ë‹¨ì–´ë“¤: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  ë¬¸ì„œ 2ì˜ ë‹¨ì–´ë“¤: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "  ë¬¸ì„œ 3ì˜ ë‹¨ì–´ë“¤: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "  ë¬¸ì„œ 4ì˜ ë‹¨ì–´ë“¤: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "  ë¬¸ì„œ 5ì˜ ë‹¨ì–´ë“¤: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "  ë¬¸ì„œ 6ì˜ ë‹¨ì–´ë“¤: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "  ë¬¸ì„œ 7ì˜ ë‹¨ì–´ë“¤: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "  ë¬¸ì„œ 8ì˜ ë‹¨ì–´ë“¤: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "\n",
      "ğŸ“– ì–´íœ˜ ì‚¬ì „ ìƒì„± ì™„ë£Œ!\n",
      "  - ì´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: 48ê°œ\n",
      "  - ì–´íœ˜ ì‚¬ì „ (ì²˜ìŒ 15ê°œ): ['achieve', 'algorithms', 'amounts', 'analysis', 'analyze', 'applications', 'architecture', 'artificial', 'become', 'bert', 'capture', 'classification', 'common', 'data', 'deep']\n",
      "  - ... (ë‚˜ë¨¸ì§€ 33ê°œ)\n",
      "\n",
      " 2ë‹¨ê³„: ê° ë¬¸ì„œë¥¼ BoW ë²¡í„°ë¡œ ë³€í™˜\n",
      "\n",
      "  ğŸ“„ ë¬¸ì„œ 1 ì²˜ë¦¬ ì¤‘...\n",
      "    í† í°: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {'artificial': 1, 'fascinating': 1, 'field': 1, 'intelligence': 1, 'language': 1, 'natural': 1, 'processing': 1}\n",
      "    BoW ë²¡í„° ê¸¸ì´: 48\n",
      "    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: 7\n",
      "\n",
      "  ğŸ“„ ë¬¸ì„œ 2 ì²˜ë¦¬ ì¤‘...\n",
      "    í† í°: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {'algorithms': 1, 'amounts': 1, 'analyze': 1, 'data': 1, 'large': 1, 'learning': 1, 'machine': 1, 'process': 1, 'text': 1}\n",
      "    BoW ë²¡í„° ê¸¸ì´: 48\n",
      "    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: 9\n",
      "\n",
      "  ğŸ“„ ë¬¸ì„œ 3 ì²˜ë¦¬ ì¤‘...\n",
      "    í† í°: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {'deep': 1, 'language': 1, 'learning': 1, 'models': 1, 'natural': 1, 'revolutionized': 1, 'understanding': 1}\n",
      "    BoW ë²¡í„° ê¸¸ì´: 48\n",
      "    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: 7\n",
      "\n",
      "  ğŸ“„ ë¬¸ì„œ 4 ì²˜ë¦¬ ì¤‘...\n",
      "    í† í°: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {'essential': 1, 'nlp': 1, 'pipeline': 1, 'preprocessing': 1, 'step': 1, 'text': 1}\n",
      "    BoW ë²¡í„° ê¸¸ì´: 48\n",
      "    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: 6\n",
      "\n",
      "  ğŸ“„ ë¬¸ì„œ 5 ì²˜ë¦¬ ì¤‘...\n",
      "    í† í°: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {'capture': 1, 'embeddings': 1, 'relationships': 1, 'semantic': 1, 'word': 1, 'words': 1}\n",
      "    BoW ë²¡í„° ê¸¸ì´: 48\n",
      "    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: 6\n",
      "\n",
      "  ğŸ“„ ë¬¸ì„œ 6 ì²˜ë¦¬ ì¤‘...\n",
      "    í† í°: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {'architecture': 1, 'become': 1, 'dominant': 1, 'modern': 1, 'nlp': 1, 'transformers': 1}\n",
      "    BoW ë²¡í„° ê¸¸ì´: 48\n",
      "    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: 6\n",
      "\n",
      "  ğŸ“„ ë¬¸ì„œ 7 ì²˜ë¦¬ ì¤‘...\n",
      "    í† í°: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {'achieve': 1, 'bert': 1, 'gpt': 1, 'many': 1, 'models': 1, 'results': 1, 'stateoftheart': 1, 'tasks': 1}\n",
      "    BoW ë²¡í„° ê¸¸ì´: 48\n",
      "    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: 8\n",
      "\n",
      "  ğŸ“„ ë¬¸ì„œ 8 ì²˜ë¦¬ ì¤‘...\n",
      "    í† í°: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {'analysis': 1, 'applications': 1, 'classification': 1, 'common': 1, 'nlp': 1, 'sentiment': 1, 'text': 1}\n",
      "    BoW ë²¡í„° ê¸¸ì´: 48\n",
      "    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: 7\n",
      "\n",
      " BoW í–‰ë ¬ ìƒì„± ì™„ë£Œ!\n",
      " í–‰ë ¬ í¬ê¸°: (8, 48) (ë¬¸ì„œ ìˆ˜ Ã— ì–´íœ˜ í¬ê¸°)\n",
      " ì²« ë²ˆì§¸ ë¬¸ì„œì˜ BoW ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): [0 0 0 0 0 0 0 1 0 0]\n",
      " í¬ì†Œì„±(Sparsity): 85.4% (ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2. Bag of Words (BoW)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def create_bow_manual(texts):\n",
    "    \"\"\"\n",
    "    ìˆ˜ë™ìœ¼ë¡œ BoW ë²¡í„° ìƒì„±í•˜ê¸°\n",
    "    \n",
    "    Args:\n",
    "        texts (list): ì „ì²˜ë¦¬ëœ í† í° ë¦¬ìŠ¤íŠ¸ë“¤\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (BoW í–‰ë ¬, ì–´íœ˜ ì‚¬ì „)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" 1ë‹¨ê³„: ì „ì²´ ë¬¸ì„œì—ì„œ ê³ ìœ  ë‹¨ì–´ ìˆ˜ì§‘\")\n",
    "    # ëª¨ë“  ë¬¸ì„œì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ì§‘í•©ìœ¼ë¡œ ìˆ˜ì§‘\n",
    "    all_words = set()\n",
    "    for i, tokens in enumerate(texts):\n",
    "        print(f\"  ë¬¸ì„œ {i+1}ì˜ ë‹¨ì–´ë“¤: {tokens}\")\n",
    "        all_words.update(tokens)  # ì§‘í•©ì— ë‹¨ì–´ë“¤ ì¶”ê°€\n",
    "    \n",
    "    # ì–´íœ˜ ì‚¬ì „ ìƒì„± (ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬)\n",
    "    vocab = sorted(list(all_words))\n",
    "    print(f\"\\nğŸ“– ì–´íœ˜ ì‚¬ì „ ìƒì„± ì™„ë£Œ!\")\n",
    "    print(f\"  - ì´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(vocab)}ê°œ\")\n",
    "    print(f\"  - ì–´íœ˜ ì‚¬ì „ (ì²˜ìŒ 15ê°œ): {vocab[:15]}\")\n",
    "    if len(vocab) > 15:\n",
    "        print(f\"  - ... (ë‚˜ë¨¸ì§€ {len(vocab)-15}ê°œ)\")\n",
    "    \n",
    "    print(f\"\\n 2ë‹¨ê³„: ê° ë¬¸ì„œë¥¼ BoW ë²¡í„°ë¡œ ë³€í™˜\")\n",
    "    # ê° ë¬¸ì„œì— ëŒ€í•´ BoW ë²¡í„° ìƒì„±\n",
    "    bow_vectors = []\n",
    "    \n",
    "    for doc_idx, tokens in enumerate(texts):\n",
    "        print(f\"\\n  ğŸ“„ ë¬¸ì„œ {doc_idx+1} ì²˜ë¦¬ ì¤‘...\")\n",
    "        print(f\"    í† í°: {tokens}\")\n",
    "        \n",
    "        # ê° ì–´íœ˜ì— ëŒ€í•´ í•´ë‹¹ ë¬¸ì„œì—ì„œì˜ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°\n",
    "        vector = []\n",
    "        word_counts = {}\n",
    "        \n",
    "        for word in vocab:\n",
    "            count = tokens.count(word)  # í•´ë‹¹ ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜\n",
    "            vector.append(count)\n",
    "            if count > 0:  # ì¶œí˜„í•œ ë‹¨ì–´ë§Œ ê¸°ë¡\n",
    "                word_counts[word] = count\n",
    "        \n",
    "        bow_vectors.append(vector)\n",
    "        \n",
    "        # ì¶œí˜„í•œ ë‹¨ì–´ë“¤ë§Œ ì¶œë ¥\n",
    "        print(f\"    ì¶œí˜„í•œ ë‹¨ì–´ì™€ ë¹ˆë„: {word_counts}\")\n",
    "        print(f\"    BoW ë²¡í„° ê¸¸ì´: {len(vector)}\")\n",
    "        print(f\"    0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜: {sum(1 for x in vector if x > 0)}\")\n",
    "    \n",
    "    return np.array(bow_vectors), vocab\n",
    "\n",
    "print(\"=\" * 70)\n",
    "bow_manual, vocab_manual = create_bow_manual(preprocessed_texts)\n",
    "\n",
    "print(f\"\\n BoW í–‰ë ¬ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\" í–‰ë ¬ í¬ê¸°: {bow_manual.shape} (ë¬¸ì„œ ìˆ˜ Ã— ì–´íœ˜ í¬ê¸°)\")\n",
    "print(f\" ì²« ë²ˆì§¸ ë¬¸ì„œì˜ BoW ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): {bow_manual[0][:10]}\")\n",
    "\n",
    "# í¬ì†Œì„± ê³„ì‚°\n",
    "total_elements = bow_manual.shape[0] * bow_manual.shape[1]\n",
    "zero_elements = np.sum(bow_manual == 0)\n",
    "sparsity = (zero_elements / total_elements) * 100\n",
    "print(f\" í¬ì†Œì„±(Sparsity): {sparsity:.1f}% (ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c328a6-1ceb-4a58-b074-e645e564f285",
   "metadata": {},
   "source": [
    "**ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” Scikit-learnì˜ CountVectorizerë¥¼ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.<br>\n",
    "ë” íš¨ìœ¨ì ì´ê³  ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751bf00a-9392-43b7-b197-e149b5312df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“š Scikit-learnì„ ì‚¬ìš©í•œ BoW\n",
      "==================================================\n",
      "ğŸ”§ Scikit-learn BoW ì„¤ì •:\n",
      "  - ì–´íœ˜ í¬ê¸°: 48\n",
      "  - í–‰ë ¬ í¬ê¸°: (8, 48)\n",
      "  - í–‰ë ¬ íƒ€ì…: <class 'scipy.sparse._csr.csr_matrix'> (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í¬ì†Œ í–‰ë ¬)\n",
      "\n",
      "ğŸ” ì–´íœ˜ ì‚¬ì „ ë¹„êµ:\n",
      "  - ìˆ˜ë™ êµ¬í˜„: 48ê°œ ë‹¨ì–´\n",
      "  - Scikit-learn: 48ê°œ ë‹¨ì–´\n",
      "  - ë™ì¼í•œ ê²°ê³¼: True\n",
      "\n",
      "ğŸ“Š ê²°ê³¼ ë¹„êµ:\n",
      "  - ìˆ˜ë™ êµ¬í˜„ ì²« ë¬¸ì„œ: [0 0 0 0 0]...\n",
      "  - Scikit-learn ì²« ë¬¸ì„œ: [0 0 0 0 0]...\n",
      "  - ê²°ê³¼ ì¼ì¹˜: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“š Scikit-learnì„ ì‚¬ìš©í•œ BoW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scikit-learn CountVectorizer ì‚¬ìš©\n",
    "vectorizer_bow = CountVectorizer(\n",
    "    tokenizer=lambda x: x,  # ì´ë¯¸ í† í°í™”ëœ ë°ì´í„° ì‚¬ìš©\n",
    "    lowercase=False         # ì´ë¯¸ ì†Œë¬¸ì ë³€í™˜ ì™„ë£Œ\n",
    ")\n",
    "\n",
    "bow_sklearn = vectorizer_bow.fit_transform(preprocessed_texts)\n",
    "\n",
    "print(f\"ğŸ”§ Scikit-learn BoW ì„¤ì •:\")\n",
    "print(f\"  - ì–´íœ˜ í¬ê¸°: {len(vectorizer_bow.vocabulary_)}\")\n",
    "print(f\"  - í–‰ë ¬ í¬ê¸°: {bow_sklearn.shape}\")\n",
    "print(f\"  - í–‰ë ¬ íƒ€ì…: {type(bow_sklearn)} (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í¬ì†Œ í–‰ë ¬)\")\n",
    "\n",
    "# ì–´íœ˜ ì‚¬ì „ ë¹„êµ\n",
    "sklearn_vocab = sorted(vectorizer_bow.vocabulary_.keys())\n",
    "print(f\"\\nğŸ” ì–´íœ˜ ì‚¬ì „ ë¹„êµ:\")\n",
    "print(f\"  - ìˆ˜ë™ êµ¬í˜„: {len(vocab_manual)}ê°œ ë‹¨ì–´\")\n",
    "print(f\"  - Scikit-learn: {len(sklearn_vocab)}ê°œ ë‹¨ì–´\")\n",
    "print(f\"  - ë™ì¼í•œ ê²°ê³¼: {vocab_manual == sklearn_vocab}\")\n",
    "\n",
    "# í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ í–‰ë ¬ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ\n",
    "bow_sklearn_dense = bow_sklearn.toarray()\n",
    "print(f\"\\nğŸ“Š ê²°ê³¼ ë¹„êµ:\")\n",
    "print(f\"  - ìˆ˜ë™ êµ¬í˜„ ì²« ë¬¸ì„œ: {bow_manual[0][:5]}...\")\n",
    "print(f\"  - Scikit-learn ì²« ë¬¸ì„œ: {bow_sklearn_dense[0][:5]}...\")\n",
    "print(f\"  - ê²°ê³¼ ì¼ì¹˜: {np.array_equal(bow_manual, bow_sklearn_dense)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc43427-3b36-46be-b25a-4c50464c4ff4",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6362984-9b9e-4ebd-9b8b-89e5f78d1501",
   "metadata": {},
   "source": [
    "### 3. N-gram\n",
    "N-gram ê°œë…:\n",
    "- Nê°œì˜ ì—°ì†ëœ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ì·¨ê¸‰\n",
    "- ë‹¨ì–´ ê°„ì˜ ìˆœì„œì™€ ë§¥ë½ ì •ë³´ë¥¼ ì–´ëŠ ì •ë„ ë³´ì¡´\n",
    "- 1-gram(unigram): ê°œë³„ ë‹¨ì–´\n",
    "- 2-gram(bigram): ë‘ ê°œì˜ ì—°ì†ëœ ë‹¨ì–´\n",
    "- 3-gram(trigram): ì„¸ ê°œì˜ ì—°ì†ëœ ë‹¨ì–´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c2445b-d97b-480a-b0bf-656d164b5336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "3. N-gram\n",
      "==================================================\n",
      "\n",
      "ğŸ“š N-gram ê°œë…:\n",
      "- Nê°œì˜ ì—°ì†ëœ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ì·¨ê¸‰\n",
      "- ë‹¨ì–´ ê°„ì˜ ìˆœì„œì™€ ë§¥ë½ ì •ë³´ë¥¼ ì–´ëŠ ì •ë„ ë³´ì¡´\n",
      "- 1-gram(unigram): ê°œë³„ ë‹¨ì–´\n",
      "- 2-gram(bigram): ë‘ ê°œì˜ ì—°ì†ëœ ë‹¨ì–´\n",
      "- 3-gram(trigram): ì„¸ ê°œì˜ ì—°ì†ëœ ë‹¨ì–´\n",
      "\n",
      "ğŸ¯ ìƒ˜í”Œ ë¬¸ì„œ ë¶„ì„:\n",
      "ì›ë³¸ ë¬¸ì„œ: Natural language processing is a fascinating field of artificial intelligence.\n",
      "ì „ì²˜ë¦¬ëœ í† í°: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "\n",
      "========================================\n",
      "ğŸ“ 1-gram (Unigram) ìƒì„±\n",
      "========================================\n",
      "ğŸ” 1-gram ìƒì„± ê³¼ì •:\n",
      "  ì…ë ¥ í† í°: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  í† í° ê°œìˆ˜: 7\n",
      "  ìœ„ì¹˜ 0: ['natural'] â†’ 'natural'\n",
      "  ìœ„ì¹˜ 1: ['language'] â†’ 'language'\n",
      "  ìœ„ì¹˜ 2: ['processing'] â†’ 'processing'\n",
      "  ìœ„ì¹˜ 3: ['fascinating'] â†’ 'fascinating'\n",
      "  ìœ„ì¹˜ 4: ['field'] â†’ 'field'\n",
      "  ìœ„ì¹˜ 5: ['artificial'] â†’ 'artificial'\n",
      "  ìœ„ì¹˜ 6: ['intelligence'] â†’ 'intelligence'\n",
      "  ìƒì„±ëœ 1-gram ê°œìˆ˜: 7\n",
      "\n",
      "========================================\n",
      "ğŸ“ 2-gram (Bigram) ìƒì„±\n",
      "========================================\n",
      "ğŸ” 2-gram ìƒì„± ê³¼ì •:\n",
      "  ì…ë ¥ í† í°: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  í† í° ê°œìˆ˜: 7\n",
      "  ìœ„ì¹˜ 0: ['natural', 'language'] â†’ 'natural language'\n",
      "  ìœ„ì¹˜ 1: ['language', 'processing'] â†’ 'language processing'\n",
      "  ìœ„ì¹˜ 2: ['processing', 'fascinating'] â†’ 'processing fascinating'\n",
      "  ìœ„ì¹˜ 3: ['fascinating', 'field'] â†’ 'fascinating field'\n",
      "  ìœ„ì¹˜ 4: ['field', 'artificial'] â†’ 'field artificial'\n",
      "  ìœ„ì¹˜ 5: ['artificial', 'intelligence'] â†’ 'artificial intelligence'\n",
      "  ìƒì„±ëœ 2-gram ê°œìˆ˜: 6\n",
      "\n",
      "========================================\n",
      "ğŸ“ 3-gram (Trigram) ìƒì„±\n",
      "========================================\n",
      "ğŸ” 3-gram ìƒì„± ê³¼ì •:\n",
      "  ì…ë ¥ í† í°: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  í† í° ê°œìˆ˜: 7\n",
      "  ìœ„ì¹˜ 0: ['natural', 'language', 'processing'] â†’ 'natural language processing'\n",
      "  ìœ„ì¹˜ 1: ['language', 'processing', 'fascinating'] â†’ 'language processing fascinating'\n",
      "  ìœ„ì¹˜ 2: ['processing', 'fascinating', 'field'] â†’ 'processing fascinating field'\n",
      "  ìœ„ì¹˜ 3: ['fascinating', 'field', 'artificial'] â†’ 'fascinating field artificial'\n",
      "  ìœ„ì¹˜ 4: ['field', 'artificial', 'intelligence'] â†’ 'field artificial intelligence'\n",
      "  ìƒì„±ëœ 3-gram ê°œìˆ˜: 5\n",
      "\n",
      "ğŸ“Š N-gram ë¶„ì„ ê²°ê³¼:\n",
      "  ì›ë³¸ í† í° ìˆ˜: 7\n",
      "  1-gram ìˆ˜: 7 (= í† í° ìˆ˜)\n",
      "  2-gram ìˆ˜: 6 (= í† í° ìˆ˜ - 1)\n",
      "  3-gram ìˆ˜: 5 (= í† í° ìˆ˜ - 2)\n",
      "\n",
      "ğŸ” ìƒì„±ëœ N-gram ì˜ˆì‹œ:\n",
      "  1-gram: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  2-gram: ['natural language', 'language processing', 'processing fascinating', 'fascinating field', 'field artificial', 'artificial intelligence']\n",
      "  3-gram: ['natural language processing', 'language processing fascinating', 'processing fascinating field', 'fascinating field artificial', 'field artificial intelligence']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3. N-gram\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“š N-gram ê°œë…:\n",
    "- Nê°œì˜ ì—°ì†ëœ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ì·¨ê¸‰\n",
    "- ë‹¨ì–´ ê°„ì˜ ìˆœì„œì™€ ë§¥ë½ ì •ë³´ë¥¼ ì–´ëŠ ì •ë„ ë³´ì¡´\n",
    "- 1-gram(unigram): ê°œë³„ ë‹¨ì–´\n",
    "- 2-gram(bigram): ë‘ ê°œì˜ ì—°ì†ëœ ë‹¨ì–´\n",
    "- 3-gram(trigram): ì„¸ ê°œì˜ ì—°ì†ëœ ë‹¨ì–´\n",
    "\"\"\")\n",
    "\n",
    "def create_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    N-gram ìƒì„± í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): í† í° ë¦¬ìŠ¤íŠ¸\n",
    "        n (int): N-gramì˜ N ê°’\n",
    "    \n",
    "    Returns:\n",
    "        list: N-gram ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” {n}-gram ìƒì„± ê³¼ì •:\")\n",
    "    print(f\"  ì…ë ¥ í† í°: {tokens}\")\n",
    "    print(f\"  í† í° ê°œìˆ˜: {len(tokens)}\")\n",
    "    \n",
    "    ngrams = []\n",
    "    \n",
    "    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ N-gram ìƒì„±\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        # ië²ˆì§¸ë¶€í„° i+në²ˆì§¸ê¹Œì§€ì˜ í† í°ë“¤ì„ ê²°í•©\n",
    "        ngram_tokens = tokens[i:i+n]\n",
    "        ngram = ' '.join(ngram_tokens)\n",
    "        ngrams.append(ngram)\n",
    "        print(f\"  ìœ„ì¹˜ {i}: {ngram_tokens} â†’ '{ngram}'\")\n",
    "    \n",
    "    print(f\"  ìƒì„±ëœ {n}-gram ê°œìˆ˜: {len(ngrams)}\")\n",
    "    return ngrams\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë¬¸ì„œë¡œ N-gram ì˜ˆì‹œ ìƒì„±\n",
    "sample_tokens = preprocessed_texts[0]\n",
    "print(f\"ğŸ¯ ìƒ˜í”Œ ë¬¸ì„œ ë¶„ì„:\")\n",
    "print(f\"ì›ë³¸ ë¬¸ì„œ: {sample_texts[0]}\")\n",
    "print(f\"ì „ì²˜ë¦¬ëœ í† í°: {sample_tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ğŸ“ 1-gram (Unigram) ìƒì„±\")\n",
    "print(\"=\"*40)\n",
    "unigrams = create_ngrams(sample_tokens, 1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ğŸ“ 2-gram (Bigram) ìƒì„±\")\n",
    "print(\"=\"*40)\n",
    "bigrams = create_ngrams(sample_tokens, 2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ğŸ“ 3-gram (Trigram) ìƒì„±\")\n",
    "print(\"=\"*40)\n",
    "trigrams = create_ngrams(sample_tokens, 3)\n",
    "\n",
    "# N-gram íŠ¹ì„± ë¶„ì„\n",
    "print(f\"\\nğŸ“Š N-gram ë¶„ì„ ê²°ê³¼:\")\n",
    "print(f\"  ì›ë³¸ í† í° ìˆ˜: {len(sample_tokens)}\")\n",
    "print(f\"  1-gram ìˆ˜: {len(unigrams)} (= í† í° ìˆ˜)\")\n",
    "print(f\"  2-gram ìˆ˜: {len(bigrams)} (= í† í° ìˆ˜ - 1)\")\n",
    "print(f\"  3-gram ìˆ˜: {len(trigrams)} (= í† í° ìˆ˜ - 2)\")\n",
    "\n",
    "print(f\"\\nğŸ” ìƒì„±ëœ N-gram ì˜ˆì‹œ:\")\n",
    "print(f\"  1-gram: {unigrams}\")\n",
    "print(f\"  2-gram: {bigrams}\")\n",
    "print(f\"  3-gram: {trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a83681-1087-4bb8-a05f-71fbfe99f030",
   "metadata": {},
   "source": [
    "#### Scikit-learnì„ ì‚¬ìš©í•œ N-gram\n",
    "N-gram ë²¡í„°í™” ì„¤ì •:\n",
    "- ngram_range=(1, 3): 1-gramë¶€í„° 3-gramê¹Œì§€ ëª¨ë‘ í¬í•¨\n",
    "- max_features=50: ìµœëŒ€ 50ê°œì˜ íŠ¹ì„±ë§Œ ì„ íƒ (ë¹ˆë„ê°€ ë†’ì€ ìˆœ)\n",
    "<br>\n",
    "\n",
    "**ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” Scikit-learnì˜ CountVectorizerì—ì„œ ngram_range ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ N-gramì„ ìƒì„±í•©ë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71864f3-f736-418f-901e-46f18734d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š N-gram í–‰ë ¬ ì •ë³´:\n",
      "  - í–‰ë ¬ í¬ê¸°: (8, 50)\n",
      "  - ë¬¸ì„œ ìˆ˜: 8\n",
      "  - N-gram íŠ¹ì„± ìˆ˜: 50\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ngram = CountVectorizer(\n",
    "    ngram_range=(1, 3),    # 1-gramë¶€í„° 3-gramê¹Œì§€\n",
    "    max_features=50,       # ìµœëŒ€ íŠ¹ì„± ìˆ˜ ì œí•œ (ì°¨ì› í­ë°œ ë°©ì§€)\n",
    "    stop_words='english'   # ì˜ì–´ ë¶ˆìš©ì–´ ìë™ ì œê±°\n",
    ")\n",
    "\n",
    "# ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ N-gram ìƒì„±\n",
    "ngram_matrix = vectorizer_ngram.fit_transform(sample_texts)\n",
    "\n",
    "print(f\"\\nğŸ“Š N-gram í–‰ë ¬ ì •ë³´:\")\n",
    "print(f\"  - í–‰ë ¬ í¬ê¸°: {ngram_matrix.shape}\")\n",
    "print(f\"  - ë¬¸ì„œ ìˆ˜: {ngram_matrix.shape[0]}\")\n",
    "print(f\"  - N-gram íŠ¹ì„± ìˆ˜: {ngram_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfbddfa4-7d7a-413a-a341-a2a522a6daca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ N-gram íŠ¹ì„± ì˜ˆì‹œ (ì²˜ìŒ 20ê°œ):\n",
      "   1. 'achieve' (1-gram)\n",
      "   2. 'language' (1-gram)\n",
      "   3. 'learning' (1-gram)\n",
      "   4. 'learning algorithms process' (3-gram)\n",
      "   5. 'learning models' (2-gram)\n",
      "   6. 'learning models revolutionized' (3-gram)\n",
      "   7. 'machine' (1-gram)\n",
      "   8. 'machine learning' (2-gram)\n",
      "   9. 'machine learning algorithms' (3-gram)\n",
      "  10. 'models' (1-gram)\n",
      "  11. 'models achieve' (2-gram)\n",
      "  12. 'models achieve state' (3-gram)\n",
      "  13. 'models revolutionized' (2-gram)\n",
      "  14. 'models revolutionized natural' (3-gram)\n",
      "  15. 'modern' (1-gram)\n",
      "  16. 'modern nlp' (2-gram)\n",
      "  17. 'natural' (1-gram)\n",
      "  18. 'natural language' (2-gram)\n",
      "  19. 'natural language processing' (3-gram)\n",
      "  20. 'natural language understanding' (3-gram)\n",
      "  ... (ë‚˜ë¨¸ì§€ 30ê°œ)\n"
     ]
    }
   ],
   "source": [
    "# ìƒì„±ëœ N-gram íŠ¹ì„± í™•ì¸\n",
    "feature_names = vectorizer_ngram.get_feature_names_out()\n",
    "print(f\"ìƒì„±ëœ N-gram íŠ¹ì„± ì˜ˆì‹œ (ì²˜ìŒ 20ê°œ):\")\n",
    "for i, feature in enumerate(feature_names[:20]):\n",
    "    feature_type = \"1-gram\" if ' ' not in feature else f\"{len(feature.split())}-gram\"\n",
    "    print(f\"  {i+1:2d}. '{feature}' ({feature_type})\")\n",
    "\n",
    "if len(feature_names) > 20:\n",
    "    print(f\"  ... (ë‚˜ë¨¸ì§€ {len(feature_names)-20}ê°œ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c883ea4-70d1-4461-b641-d632779f4c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²« ë²ˆì§¸ ë¬¸ì„œì—ì„œ ì¶œí˜„í•œ N-gram:\n",
      "ì›ë³¸: Natural language processing is a fascinating field of artificial intelligence.\n",
      "  'language' (1-gram): 1íšŒ\n",
      "  'natural' (1-gram): 1íšŒ\n",
      "  'natural language' (2-gram): 1íšŒ\n",
      "  'natural language processing' (3-gram): 1íšŒ\n",
      "  'processing' (1-gram): 1íšŒ\n",
      "  'processing fascinating' (2-gram): 1íšŒ\n",
      "  'processing fascinating field' (3-gram): 1íšŒ\n"
     ]
    }
   ],
   "source": [
    "# ì²« ë²ˆì§¸ ë¬¸ì„œì˜ N-gram ë¶„ì„\n",
    "first_doc_vector = ngram_matrix[0].toarray().flatten()\n",
    "non_zero_indices = np.where(first_doc_vector > 0)[0]\n",
    "\n",
    "print(f\"ì²« ë²ˆì§¸ ë¬¸ì„œì—ì„œ ì¶œí˜„í•œ N-gram:\")\n",
    "print(f\"ì›ë³¸: {sample_texts[0]}\")\n",
    "for idx in non_zero_indices[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
    "    feature = feature_names[idx]\n",
    "    count = first_doc_vector[idx]\n",
    "    feature_type = \"1-gram\" if ' ' not in feature else f\"{len(feature.split())}-gram\"\n",
    "    print(f\"  '{feature}' ({feature_type}): {count}íšŒ\")\n",
    "\n",
    "if len(non_zero_indices) > 10:\n",
    "    print(f\"  ... (ë‚˜ë¨¸ì§€ {len(non_zero_indices)-10}ê°œ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f84d5-b207-477e-8710-2cd44ed11a60",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84825214-7f5f-4d4a-8d23-735617528540",
   "metadata": {},
   "source": [
    "### 4. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "TF-IDF ê°œë…:\n",
    "- TF (Term Frequency): íŠ¹ì • ë¬¸ì„œì—ì„œ ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„\n",
    "- IDF (Inverse Document Frequency): ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¨ì–´ì˜ í¬ê·€ì„±\n",
    "- TF-IDF = TF Ã— IDF\n",
    "- ëª©ì : ìì£¼ ë‚˜ì˜¤ì§€ë§Œ íŠ¹ë³„í•œ ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì¶¤\n",
    "- ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ee4f6d-07e1-48cd-b0d1-1b2f980fd068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ TF-IDF ê³„ì‚° ì˜ˆì‹œ (ì²« 3ê°œ ë¬¸ì„œ)\n",
      "ì›ë³¸ ë¬¸ì„œë“¤:\n",
      "  ë¬¸ì„œ 1:\n",
      "    ì›ë³¸: Natural language processing is a fascinating field of artificial intelligence.\n",
      "    í† í°: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  ë¬¸ì„œ 2:\n",
      "    ì›ë³¸: Machine learning algorithms can process and analyze large amounts of text data.\n",
      "    í† í°: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "  ë¬¸ì„œ 3:\n",
      "    ì›ë³¸: Deep learning models have revolutionized natural language understanding.\n",
      "    í† í°: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n"
     ]
    }
   ],
   "source": [
    "# ì²« 3ê°œ ë¬¸ì„œë¡œ ì˜ˆì‹œ ì‹¤í–‰ (ì¶œë ¥ì„ ê°„ê²°í•˜ê²Œ í•˜ê¸° ìœ„í•´)\n",
    "sample_docs = preprocessed_texts[:3]\n",
    "print(\"\\nğŸ¯ TF-IDF ê³„ì‚° ì˜ˆì‹œ (ì²« 3ê°œ ë¬¸ì„œ)\")\n",
    "print(\"ì›ë³¸ ë¬¸ì„œë“¤:\")\n",
    "for i in range(3):\n",
    "    print(f\"  ë¬¸ì„œ {i+1}:\")\n",
    "    print(f\"    ì›ë³¸: {sample_texts[i]}\")\n",
    "    print(f\"    í† í°: {sample_docs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7234d31e-97f8-456c-a2ba-e9207e2ef000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "4. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "==================================================\n",
      "ğŸ” TF-IDF ê³„ì‚° ì‹œì‘\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š IDF ê³„ì‚° ê³¼ì •:\n",
      "  ì „ì²´ ë¬¸ì„œ ìˆ˜: 3\n",
      "  ì „ì²´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: 20\n",
      "  'algorithms': log(3/1) = 1.0986\n",
      "  'amounts': log(3/1) = 1.0986\n",
      "  'analyze': log(3/1) = 1.0986\n",
      "  'artificial': log(3/1) = 1.0986\n",
      "  'data': log(3/1) = 1.0986\n",
      "  'deep': log(3/1) = 1.0986\n",
      "  'fascinating': log(3/1) = 1.0986\n",
      "  'field': log(3/1) = 1.0986\n",
      "  'intelligence': log(3/1) = 1.0986\n",
      "  'language': log(3/2) = 0.4055\n",
      "  'large': log(3/1) = 1.0986\n",
      "  'learning': log(3/2) = 0.4055\n",
      "  'machine': log(3/1) = 1.0986\n",
      "  'models': log(3/1) = 1.0986\n",
      "  'natural': log(3/2) = 0.4055\n",
      "  'process': log(3/1) = 1.0986\n",
      "  'processing': log(3/1) = 1.0986\n",
      "  'revolutionized': log(3/1) = 1.0986\n",
      "  'text': log(3/1) = 1.0986\n",
      "  'understanding': log(3/1) = 1.0986\n",
      "\n",
      "ğŸ“‹ IDF ê°’ ìš”ì•½:\n",
      "  'algorithms': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'amounts': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'analyze': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'artificial': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'data': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'deep': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'fascinating': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'field': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'intelligence': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "  'large': 1.0986 (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ 1 TF-IDF ê³„ì‚°:\n",
      "  í† í°: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "ğŸ“Š TF ê³„ì‚° ê³¼ì •:\n",
      "  ì…ë ¥ í† í°: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  ì´ ë‹¨ì–´ ìˆ˜: 7\n",
      "  ë‹¨ì–´ë³„ ì¶œí˜„ íšŸìˆ˜: {'natural': 1, 'language': 1, 'processing': 1, 'fascinating': 1, 'field': 1, 'artificial': 1, 'intelligence': 1}\n",
      "  'natural': 1/7 = 0.1429\n",
      "  'language': 1/7 = 0.1429\n",
      "  'processing': 1/7 = 0.1429\n",
      "  'fascinating': 1/7 = 0.1429\n",
      "  'field': 1/7 = 0.1429\n",
      "  'artificial': 1/7 = 0.1429\n",
      "  'intelligence': 1/7 = 0.1429\n",
      "\n",
      "  TF-IDF = TF Ã— IDF ê³„ì‚°:\n",
      "    'natural': 0.1429 Ã— 0.4055 = 0.0579\n",
      "    'language': 0.1429 Ã— 0.4055 = 0.0579\n",
      "    'processing': 0.1429 Ã— 1.0986 = 0.1569\n",
      "    'fascinating': 0.1429 Ã— 1.0986 = 0.1569\n",
      "    'field': 0.1429 Ã— 1.0986 = 0.1569\n",
      "    'artificial': 0.1429 Ã— 1.0986 = 0.1569\n",
      "    'intelligence': 0.1429 Ã— 1.0986 = 0.1569\n",
      "  ğŸ’¡ ë¬¸ì„œ 1ì˜ í•µì‹¬ ë‹¨ì–´ (TF-IDF ê¸°ì¤€):\n",
      "    'processing': 0.1569\n",
      "    'fascinating': 0.1569\n",
      "    'field': 0.1569\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ 2 TF-IDF ê³„ì‚°:\n",
      "  í† í°: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "ğŸ“Š TF ê³„ì‚° ê³¼ì •:\n",
      "  ì…ë ¥ í† í°: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "  ì´ ë‹¨ì–´ ìˆ˜: 9\n",
      "  ë‹¨ì–´ë³„ ì¶œí˜„ íšŸìˆ˜: {'machine': 1, 'learning': 1, 'algorithms': 1, 'process': 1, 'analyze': 1, 'large': 1, 'amounts': 1, 'text': 1, 'data': 1}\n",
      "  'machine': 1/9 = 0.1111\n",
      "  'learning': 1/9 = 0.1111\n",
      "  'algorithms': 1/9 = 0.1111\n",
      "  'process': 1/9 = 0.1111\n",
      "  'analyze': 1/9 = 0.1111\n",
      "  'large': 1/9 = 0.1111\n",
      "  'amounts': 1/9 = 0.1111\n",
      "  'text': 1/9 = 0.1111\n",
      "  'data': 1/9 = 0.1111\n",
      "\n",
      "  TF-IDF = TF Ã— IDF ê³„ì‚°:\n",
      "    'machine': 0.1111 Ã— 1.0986 = 0.1221\n",
      "    'learning': 0.1111 Ã— 0.4055 = 0.0451\n",
      "    'algorithms': 0.1111 Ã— 1.0986 = 0.1221\n",
      "    'process': 0.1111 Ã— 1.0986 = 0.1221\n",
      "    'analyze': 0.1111 Ã— 1.0986 = 0.1221\n",
      "    'large': 0.1111 Ã— 1.0986 = 0.1221\n",
      "    'amounts': 0.1111 Ã— 1.0986 = 0.1221\n",
      "    'text': 0.1111 Ã— 1.0986 = 0.1221\n",
      "    'data': 0.1111 Ã— 1.0986 = 0.1221\n",
      "  ğŸ’¡ ë¬¸ì„œ 2ì˜ í•µì‹¬ ë‹¨ì–´ (TF-IDF ê¸°ì¤€):\n",
      "    'machine': 0.1221\n",
      "    'algorithms': 0.1221\n",
      "    'process': 0.1221\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ 3 TF-IDF ê³„ì‚°:\n",
      "  í† í°: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "ğŸ“Š TF ê³„ì‚° ê³¼ì •:\n",
      "  ì…ë ¥ í† í°: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "  ì´ ë‹¨ì–´ ìˆ˜: 7\n",
      "  ë‹¨ì–´ë³„ ì¶œí˜„ íšŸìˆ˜: {'deep': 1, 'learning': 1, 'models': 1, 'revolutionized': 1, 'natural': 1, 'language': 1, 'understanding': 1}\n",
      "  'deep': 1/7 = 0.1429\n",
      "  'learning': 1/7 = 0.1429\n",
      "  'models': 1/7 = 0.1429\n",
      "  'revolutionized': 1/7 = 0.1429\n",
      "  'natural': 1/7 = 0.1429\n",
      "  'language': 1/7 = 0.1429\n",
      "  'understanding': 1/7 = 0.1429\n",
      "\n",
      "  TF-IDF = TF Ã— IDF ê³„ì‚°:\n",
      "    'deep': 0.1429 Ã— 1.0986 = 0.1569\n",
      "    'learning': 0.1429 Ã— 0.4055 = 0.0579\n",
      "    'models': 0.1429 Ã— 1.0986 = 0.1569\n",
      "    'revolutionized': 0.1429 Ã— 1.0986 = 0.1569\n",
      "    'natural': 0.1429 Ã— 0.4055 = 0.0579\n",
      "    'language': 0.1429 Ã— 0.4055 = 0.0579\n",
      "    'understanding': 0.1429 Ã— 1.0986 = 0.1569\n",
      "  ğŸ’¡ ë¬¸ì„œ 3ì˜ í•µì‹¬ ë‹¨ì–´ (TF-IDF ê¸°ì¤€):\n",
      "    'deep': 0.1569\n",
      "    'models': 0.1569\n",
      "    'revolutionized': 0.1569\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"4. TF-IDF (Term Frequency-Inverse Document Frequency)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def calculate_tf(tokens):\n",
    "    \"\"\"\n",
    "    TF (Term Frequency) ê³„ì‚°í•˜ê¸°\n",
    "    TF = (íŠ¹ì • ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜) / (ë¬¸ì„œì˜ ì´ ë‹¨ì–´ ìˆ˜)\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š TF ê³„ì‚° ê³¼ì •:\")\n",
    "    print(f\"  ì…ë ¥ í† í°: {tokens}\")\n",
    "    \n",
    "    tf_dict = {}\n",
    "    total_count = len(tokens)\n",
    "    print(f\"  ì´ ë‹¨ì–´ ìˆ˜: {total_count}\")\n",
    "    \n",
    "    # ê° ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°\n",
    "    word_counts = Counter(tokens)\n",
    "    print(f\"  ë‹¨ì–´ë³„ ì¶œí˜„ íšŸìˆ˜: {dict(word_counts)}\")\n",
    "    \n",
    "    # TF ê°’ ê³„ì‚° \n",
    "    for token, count in word_counts.items():\n",
    "        tf_value = count / total_count\n",
    "        tf_dict[token] = tf_value\n",
    "        print(f\"  '{token}': {count}/{total_count} = {tf_value:.4f}\")\n",
    "    \n",
    "    return tf_dict\n",
    "\n",
    "def calculate_idf(documents):\n",
    "    \"\"\"\n",
    "    IDF (Inverse Document Frequency) ê³„ì‚°í•˜ê¸°\n",
    "    IDF = log(ì „ì²´ ë¬¸ì„œ ìˆ˜ / í•´ë‹¹ ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ì„œ ìˆ˜)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š IDF ê³„ì‚° ê³¼ì •:\")\n",
    "    \n",
    "    idf_dict = {}\n",
    "    total_documents = len(documents)\n",
    "    print(f\"  ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_documents}\")\n",
    "    \n",
    "    # ëª¨ë“  ê³ ìœ  ë‹¨ì–´ ìˆ˜ì§‘\n",
    "    all_words = set()\n",
    "    for doc in documents:\n",
    "        all_words.update(doc)\n",
    "    \n",
    "    print(f\"  ì „ì²´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(all_words)}\")\n",
    "    \n",
    "    # ê° ë‹¨ì–´ì— ëŒ€í•´ IDF ê³„ì‚°\n",
    "    for word in sorted(all_words):\n",
    "        # í•´ë‹¹ ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ì„œ ìˆ˜ ê³„ì‚°\n",
    "        containing_docs = 0\n",
    "        for doc in documents:\n",
    "            if word in doc:\n",
    "                containing_docs += 1\n",
    "        \n",
    "        # IDF ê³„ì‚°\n",
    "        idf_value = math.log(total_documents / containing_docs)\n",
    "        idf_dict[word] = idf_value\n",
    "        \n",
    "        print(f\"  '{word}': log({total_documents}/{containing_docs}) = {idf_value:.4f}\")\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "def calculate_tfidf_manual(documents):\n",
    "    \"\"\"\n",
    "    TF-IDF ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•˜ê¸°\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” TF-IDF ê³„ì‚° ì‹œì‘\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1ë‹¨ê³„: IDF ê³„ì‚° (ì „ì²´ ë¬¸ì„œì— ëŒ€í•´)\n",
    "    idf = calculate_idf(documents)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ IDF ê°’ ìš”ì•½:\")\n",
    "    sorted_idf = sorted(idf.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_idf[:10]:\n",
    "        print(f\"  '{word}': {score:.4f} (í¬ê·€í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°’)\")\n",
    "    \n",
    "    # 2ë‹¨ê³„: ê° ë¬¸ì„œë³„ TF-IDF ê³„ì‚°\n",
    "    tfidf_documents = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        print(f\"\\nğŸ“„ ë¬¸ì„œ {doc_idx+1} TF-IDF ê³„ì‚°:\")\n",
    "        print(f\"  í† í°: {doc}\")\n",
    "        \n",
    "        # TF ê³„ì‚°\n",
    "        tf = calculate_tf(doc)\n",
    "        \n",
    "        # TF-IDF ê³„ì‚°\n",
    "        tfidf_doc = {}\n",
    "        print(f\"\\n  TF-IDF = TF Ã— IDF ê³„ì‚°:\")\n",
    "        for word, tf_val in tf.items():\n",
    "            tfidf_val = tf_val * idf[word]\n",
    "            tfidf_doc[word] = tfidf_val\n",
    "            print(f\"    '{word}': {tf_val:.4f} Ã— {idf[word]:.4f} = {tfidf_val:.4f}\")\n",
    "        \n",
    "        tfidf_documents.append(tfidf_doc)\n",
    "        \n",
    "        # í•´ë‹¹ ë¬¸ì„œì˜ ì¤‘ìš” ë‹¨ì–´ ì¶œë ¥\n",
    "        sorted_tfidf = sorted(tfidf_doc.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"  ğŸ’¡ ë¬¸ì„œ {doc_idx+1}ì˜ í•µì‹¬ ë‹¨ì–´ (TF-IDF ê¸°ì¤€):\")\n",
    "        for word, score in sorted_tfidf[:3]:\n",
    "            print(f\"    '{word}': {score:.4f}\")\n",
    "    \n",
    "    return tfidf_documents, idf\n",
    "\n",
    "tfidf_manual, idf_scores = calculate_tfidf_manual(sample_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d248ccb-0547-4423-aa8e-dc4a92666c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“Š TF-IDF ê²°ê³¼ ë¶„ì„\n",
      "==================================================\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ 1 ë¶„ì„:\n",
      "  ì›ë³¸: Natural language processing is a fascinating field of artificial intelligence.\n",
      "  TF-IDF ìƒìœ„ ë‹¨ì–´:\n",
      "    1. 'processing': 0.1569\n",
      "    2. 'fascinating': 0.1569\n",
      "    3. 'field': 0.1569\n",
      "    4. 'artificial': 0.1569\n",
      "    5. 'intelligence': 0.1569\n",
      "    6. 'natural': 0.0579\n",
      "    7. 'language': 0.0579\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ 2 ë¶„ì„:\n",
      "  ì›ë³¸: Machine learning algorithms can process and analyze large amounts of text data.\n",
      "  TF-IDF ìƒìœ„ ë‹¨ì–´:\n",
      "    1. 'machine': 0.1221\n",
      "    2. 'algorithms': 0.1221\n",
      "    3. 'process': 0.1221\n",
      "    4. 'analyze': 0.1221\n",
      "    5. 'large': 0.1221\n",
      "    6. 'amounts': 0.1221\n",
      "    7. 'text': 0.1221\n",
      "    8. 'data': 0.1221\n",
      "    9. 'learning': 0.0451\n",
      "\n",
      "ğŸ“„ ë¬¸ì„œ 3 ë¶„ì„:\n",
      "  ì›ë³¸: Deep learning models have revolutionized natural language understanding.\n",
      "  TF-IDF ìƒìœ„ ë‹¨ì–´:\n",
      "    1. 'deep': 0.1569\n",
      "    2. 'models': 0.1569\n",
      "    3. 'revolutionized': 0.1569\n",
      "    4. 'understanding': 0.1569\n",
      "    5. 'learning': 0.0579\n",
      "    6. 'natural': 0.0579\n",
      "    7. 'language': 0.0579\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“Š TF-IDF ê²°ê³¼ ë¶„ì„\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ê° ë¬¸ì„œë³„ ìƒìœ„ ë‹¨ì–´ ë¶„ì„\n",
    "for doc_idx in range(len(tfidf_manual)):\n",
    "    print(f\"\\nğŸ“„ ë¬¸ì„œ {doc_idx+1} ë¶„ì„:\")\n",
    "    print(f\"  ì›ë³¸: {sample_texts[doc_idx]}\")\n",
    "    \n",
    "    tfidf_doc = tfidf_manual[doc_idx]\n",
    "    sorted_tfidf = sorted(tfidf_doc.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"  TF-IDF ìƒìœ„ ë‹¨ì–´:\")\n",
    "    for rank, (word, score) in enumerate(sorted_tfidf, 1):\n",
    "        print(f\"    {rank}. '{word}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9df0b-44ef-47fa-acd0-21d2daad495a",
   "metadata": {},
   "source": [
    "#### Scikit-learnì„ ì‚¬ìš©í•œ TF-IDF\n",
    "\n",
    "**ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” Scikit-learnì˜ TfidfVectorizerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "ë” íš¨ìœ¨ì ì´ê³  ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c294b67-1ee3-42f7-8d63-2acb6979672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ëœ ë¬¸ìì—´ë“¤:\n",
      "  1. natural language processing fascinating field artificial intelligence\n",
      "  2. machine learning algorithms process analyze large amounts text data\n",
      "  3. deep learning models revolutionized natural language understanding\n",
      "  4. text preprocessing essential step nlp pipeline\n",
      "  5. word embeddings capture semantic relationships words\n",
      "  6. transformers become dominant architecture modern nlp\n",
      "  7. bert gpt models achieve stateoftheart results many tasks\n",
      "  8. text classification sentiment analysis common nlp applications\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²˜ë¦¬ëœ í† í°ì„ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜ (Scikit-learnì´ ë¬¸ìì—´ì„ ë°›ê¸° ë•Œë¬¸)\n",
    "preprocessed_strings = []\n",
    "for tokens in preprocessed_texts:\n",
    "    text_string = ' '.join(tokens)\n",
    "    preprocessed_strings.append(text_string)\n",
    "\n",
    "print(\"ì „ì²˜ë¦¬ëœ ë¬¸ìì—´ë“¤:\")\n",
    "for i, text in enumerate(preprocessed_strings):\n",
    "    print(f\"  {i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d40ad4d0-85fb-43ab-81b1-196590ad70c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ Scikit-learn TF-IDF ê²°ê³¼:\n",
      "  - í–‰ë ¬ í¬ê¸°: (8, 48) (ë¬¸ì„œ ìˆ˜ Ã— ë‹¨ì–´ ìˆ˜)\n",
      "  - ì–´íœ˜ í¬ê¸°: 48ê°œ\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn TfidfVectorizer ì‚¬ìš©\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,          # ì´ë¯¸ ì†Œë¬¸ì ë³€í™˜ ì™„ë£Œ\n",
    "    stop_words=None,          # ì´ë¯¸ ë¶ˆìš©ì–´ ì œê±° ì™„ë£Œ\n",
    "    norm='l2',               # L2 ì •ê·œí™” \n",
    "    use_idf=True,            # IDF ì‚¬ìš©\n",
    "    smooth_idf=True,         # ì•ˆì •ì ì¸ ê³„ì‚°ì„ ìœ„í•œ ìŠ¤ë¬´ë”©\n",
    "    sublinear_tf=False       # ê¸°ë³¸ TF ì‚¬ìš©\n",
    ")\n",
    "\n",
    "tfidf_sklearn = tfidf_vectorizer.fit_transform(preprocessed_strings)\n",
    "\n",
    "print(f\"\\nğŸ”§ Scikit-learn TF-IDF ê²°ê³¼:\")\n",
    "print(f\"  - í–‰ë ¬ í¬ê¸°: {tfidf_sklearn.shape} (ë¬¸ì„œ ìˆ˜ Ã— ë‹¨ì–´ ìˆ˜)\")\n",
    "print(f\"  - ì–´íœ˜ í¬ê¸°: {len(tfidf_vectorizer.vocabulary_)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "641c278f-07fb-42ff-8302-9b80119957ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“– ì „ì²´ ì–´íœ˜:\n",
      "  0: 'achieve'\n",
      "  1: 'algorithms'\n",
      "  2: 'amounts'\n",
      "  3: 'analysis'\n",
      "  4: 'analyze'\n",
      "  5: 'applications'\n",
      "  6: 'architecture'\n",
      "  7: 'artificial'\n",
      "  8: 'become'\n",
      "  9: 'bert'\n",
      "  10: 'capture'\n",
      "  11: 'classification'\n",
      "  12: 'common'\n",
      "  13: 'data'\n",
      "  14: 'deep'\n",
      "  15: 'dominant'\n",
      "  16: 'embeddings'\n",
      "  17: 'essential'\n",
      "  18: 'fascinating'\n",
      "  19: 'field'\n",
      "  20: 'gpt'\n",
      "  21: 'intelligence'\n",
      "  22: 'language'\n",
      "  23: 'large'\n",
      "  24: 'learning'\n",
      "  25: 'machine'\n",
      "  26: 'many'\n",
      "  27: 'models'\n",
      "  28: 'modern'\n",
      "  29: 'natural'\n",
      "  30: 'nlp'\n",
      "  31: 'pipeline'\n",
      "  32: 'preprocessing'\n",
      "  33: 'process'\n",
      "  34: 'processing'\n",
      "  35: 'relationships'\n",
      "  36: 'results'\n",
      "  37: 'revolutionized'\n",
      "  38: 'semantic'\n",
      "  39: 'sentiment'\n",
      "  40: 'stateoftheart'\n",
      "  41: 'step'\n",
      "  42: 'tasks'\n",
      "  43: 'text'\n",
      "  44: 'transformers'\n",
      "  45: 'understanding'\n",
      "  46: 'word'\n",
      "  47: 'words'\n"
     ]
    }
   ],
   "source": [
    "# ì–´íœ˜ í™•ì¸\n",
    "print(f\"\\nğŸ“– ì „ì²´ ì–´íœ˜:\")\n",
    "vocabulary = tfidf_vectorizer.vocabulary_\n",
    "sorted_vocab = sorted(vocabulary.items(), key=lambda x: x[1])\n",
    "for word, idx in sorted_vocab:\n",
    "    print(f\"  {idx}: '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fddd4b62-dd23-43b0-884e-697cc972da0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Scikit-learn IDF ê°’:\n",
      "  'achieve': 2.5041\n",
      "  'algorithms': 2.5041\n",
      "  'amounts': 2.5041\n",
      "  'analysis': 2.5041\n",
      "  'analyze': 2.5041\n",
      "  'applications': 2.5041\n",
      "  'architecture': 2.5041\n",
      "  'artificial': 2.5041\n",
      "  'become': 2.5041\n",
      "  'bert': 2.5041\n",
      "  'capture': 2.5041\n",
      "  'classification': 2.5041\n",
      "  'common': 2.5041\n",
      "  'data': 2.5041\n",
      "  'deep': 2.5041\n",
      "  'dominant': 2.5041\n",
      "  'embeddings': 2.5041\n",
      "  'essential': 2.5041\n",
      "  'fascinating': 2.5041\n",
      "  'field': 2.5041\n",
      "  'gpt': 2.5041\n",
      "  'intelligence': 2.5041\n",
      "  'language': 2.0986\n",
      "  'large': 2.5041\n",
      "  'learning': 2.0986\n",
      "  'machine': 2.5041\n",
      "  'many': 2.5041\n",
      "  'models': 2.0986\n",
      "  'modern': 2.5041\n",
      "  'natural': 2.0986\n",
      "  'nlp': 1.8109\n",
      "  'pipeline': 2.5041\n",
      "  'preprocessing': 2.5041\n",
      "  'process': 2.5041\n",
      "  'processing': 2.5041\n",
      "  'relationships': 2.5041\n",
      "  'results': 2.5041\n",
      "  'revolutionized': 2.5041\n",
      "  'semantic': 2.5041\n",
      "  'sentiment': 2.5041\n",
      "  'stateoftheart': 2.5041\n",
      "  'step': 2.5041\n",
      "  'tasks': 2.5041\n",
      "  'text': 1.8109\n",
      "  'transformers': 2.5041\n",
      "  'understanding': 2.5041\n",
      "  'word': 2.5041\n",
      "  'words': 2.5041\n",
      "\n",
      "ğŸ“ˆ ê° ë¬¸ì„œì˜ TF-IDF ê°’:\n",
      "\n",
      "ë¬¸ì„œ 1:\n",
      "  ì›ë³¸: Natural language processing is a fascinating field of artificial intelligence.\n",
      "  1. 'artificial': 0.3951\n",
      "  2. 'fascinating': 0.3951\n",
      "  3. 'field': 0.3951\n",
      "  4. 'intelligence': 0.3951\n",
      "  5. 'processing': 0.3951\n",
      "  6. 'language': 0.3312\n",
      "  7. 'natural': 0.3312\n",
      "\n",
      "ë¬¸ì„œ 2:\n",
      "  ì›ë³¸: Machine learning algorithms can process and analyze large amounts of text data.\n",
      "  1. 'algorithms': 0.3487\n",
      "  2. 'amounts': 0.3487\n",
      "  3. 'analyze': 0.3487\n",
      "  4. 'data': 0.3487\n",
      "  5. 'large': 0.3487\n",
      "  6. 'machine': 0.3487\n",
      "  7. 'process': 0.3487\n",
      "  8. 'learning': 0.2922\n",
      "  9. 'text': 0.2522\n",
      "\n",
      "ë¬¸ì„œ 3:\n",
      "  ì›ë³¸: Deep learning models have revolutionized natural language understanding.\n",
      "  1. 'deep': 0.4149\n",
      "  2. 'revolutionized': 0.4149\n",
      "  3. 'understanding': 0.4149\n",
      "  4. 'language': 0.3477\n",
      "  5. 'learning': 0.3477\n",
      "  6. 'models': 0.3477\n",
      "  7. 'natural': 0.3477\n"
     ]
    }
   ],
   "source": [
    "# IDF ê°’ í™•ì¸\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "\n",
    "print(f\"\\nğŸ“Š Scikit-learn IDF ê°’:\")\n",
    "for i, word in enumerate(feature_names):\n",
    "    print(f\"  '{word}': {idf_values[i]:.4f}\")\n",
    "\n",
    "# ê° ë¬¸ì„œì˜ TF-IDF ê°’ í™•ì¸ (ì²« 3ê°œ ë¬¸ì„œ)\n",
    "print(f\"\\nğŸ“ˆ ê° ë¬¸ì„œì˜ TF-IDF ê°’:\")\n",
    "for doc_idx in range(min(3, tfidf_sklearn.shape[0])):\n",
    "    doc_tfidf = tfidf_sklearn[doc_idx].toarray().flatten()\n",
    "    \n",
    "    print(f\"\\në¬¸ì„œ {doc_idx+1}:\")\n",
    "    print(f\"  ì›ë³¸: {sample_texts[doc_idx]}\")\n",
    "    \n",
    "    # 0ì´ ì•„ë‹Œ ê°’ë“¤ë§Œ ì¶œë ¥\n",
    "    word_scores = []\n",
    "    for i, score in enumerate(doc_tfidf):\n",
    "        if score > 0:\n",
    "            word = feature_names[i]\n",
    "            word_scores.append((word, score))\n",
    "    \n",
    "    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for rank, (word, score) in enumerate(word_scores, 1):\n",
    "        print(f\"  {rank}. '{word}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b19181-cbf6-4cc9-92e4-a4d8cd16e22e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750ed43-4772-4aa3-9ba1-c84bcafacd00",
   "metadata": {},
   "source": [
    "### 5. Word2Vec (Word to Vector)\n",
    "Word2Vec ê°œë…:\n",
    "- ë‹¨ì–´ë¥¼ ê³ ì • í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ \n",
    "- ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ë“¤ì´ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜ë¨\n",
    "- TF-IDFì™€ ë‹¬ë¦¬ ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í•™ìŠµí•¨\n",
    "- ë‘ ê°€ì§€ ëª¨ë¸: CBOW (ì£¼ë³€ ë‹¨ì–´ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ ì˜ˆì¸¡), Skip-gram (ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ ì˜ˆì¸¡)\n",
    "\n",
    "Word2Vecì˜ í•µì‹¬ ì•„ì´ë””ì–´:\n",
    "- \"ë¹„ìŠ·í•œ ë§¥ë½ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤\"\n",
    "- ì˜ˆ: \"ê°•ì•„ì§€ëŠ” ê·€ì—½ë‹¤\", \"ê³ ì–‘ì´ëŠ” ê·€ì—½ë‹¤\" â†’ 'ê°•ì•„ì§€'ì™€ 'ê³ ì–‘ì´'ëŠ” ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê°€ì§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b596f4-8637-49b9-8bfb-33e21f0055b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Word2Vec í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„° í™•ì¸:\n",
      "ì „ì²˜ë¦¬ëœ ë¬¸ì„œë“¤:\n",
      "  ë¬¸ì„œ 1: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "    í† í° ìˆ˜: 7ê°œ\n",
      "  ë¬¸ì„œ 2: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "    í† í° ìˆ˜: 9ê°œ\n",
      "  ë¬¸ì„œ 3: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "    í† í° ìˆ˜: 7ê°œ\n",
      "  ë¬¸ì„œ 4: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "    í† í° ìˆ˜: 6ê°œ\n",
      "  ë¬¸ì„œ 5: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "    í† í° ìˆ˜: 6ê°œ\n",
      "  ë¬¸ì„œ 6: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "    í† í° ìˆ˜: 6ê°œ\n",
      "  ë¬¸ì„œ 7: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "    í† í° ìˆ˜: 8ê°œ\n",
      "  ë¬¸ì„œ 8: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "    í† í° ìˆ˜: 7ê°œ\n",
      "\n",
      "ğŸ“Š ì „ì²´ ì–´íœ˜ í†µê³„:\n",
      "  ì´ í† í° ìˆ˜: 56ê°œ\n",
      "  ê³ ìœ  ë‹¨ì–´ ìˆ˜: 48ê°œ\n",
      "  ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ 5ê°œ:\n",
      "    'text': 3ë²ˆ\n",
      "    'nlp': 3ë²ˆ\n",
      "    'natural': 2ë²ˆ\n",
      "    'language': 2ë²ˆ\n",
      "    'learning': 2ë²ˆ\n"
     ]
    }
   ],
   "source": [
    "def show_preprocessing_result(preprocessed_texts):\n",
    "    \"\"\"ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í•œë²ˆ í™•ì¸\"\"\"\n",
    "    print(\"ğŸ” Word2Vec í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„° í™•ì¸:\")\n",
    "    print(\"ì „ì²˜ë¦¬ëœ ë¬¸ì„œë“¤:\")\n",
    "    for i, tokens in enumerate(preprocessed_texts):\n",
    "        print(f\"  ë¬¸ì„œ {i+1}: {tokens}\")\n",
    "        print(f\"    í† í° ìˆ˜: {len(tokens)}ê°œ\")\n",
    "    \n",
    "    # ì „ì²´ ì–´íœ˜ í†µê³„\n",
    "    all_words = []\n",
    "    for tokens in preprocessed_texts:\n",
    "        all_words.extend(tokens)\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    print(f\"\\nğŸ“Š ì „ì²´ ì–´íœ˜ í†µê³„:\")\n",
    "    print(f\"  ì´ í† í° ìˆ˜: {len(all_words)}ê°œ\")\n",
    "    print(f\"  ê³ ìœ  ë‹¨ì–´ ìˆ˜: {len(word_counts)}ê°œ\")\n",
    "    print(f\"  ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ 5ê°œ:\")\n",
    "    for word, count in word_counts.most_common(5):\n",
    "        print(f\"    '{word}': {count}ë²ˆ\")\n",
    "\n",
    "show_preprocessing_result(preprocessed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee8c10c-820f-461b-bef3-8e5a7f48f03f",
   "metadata": {},
   "source": [
    "Word2Vec ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…:\n",
    "- vector_size=100   : ê° ë‹¨ì–´ë¥¼ 100ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„\n",
    "- window=5         : ì¤‘ì‹¬ ë‹¨ì–´ ê¸°ì¤€ ì•ë’¤ 5ê°œ ë‹¨ì–´ê¹Œì§€ ê³ ë ¤\n",
    "- min_count=1      : ìµœì†Œ 1ë²ˆ ì´ìƒ ë‚˜íƒ€ë‚œ ë‹¨ì–´ë§Œ í•™ìŠµ (ë°ì´í„°ê°€ ì ì–´ì„œ 1ë¡œ ì„¤ì •)\n",
    "- workers=4        : 4ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ë³‘ë ¬ í•™ìŠµ\n",
    "- sg=0            : CBOW ëª¨ë¸ ì‚¬ìš© (0=CBOW, 1=Skip-gram)\n",
    "- epochs=10       : ì „ì²´ ë°ì´í„°ë¥¼ 10ë²ˆ ë°˜ë³µ í•™ìŠµ\n",
    "<br>\n",
    "\n",
    "CBOW vs Skip-gram:\n",
    "<br>\n",
    "\n",
    "CBOW (Continuous Bag of Words):\n",
    "- ì£¼ë³€ ë‹¨ì–´ë“¤ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡\n",
    "- ì˜ˆ: 'ìì—°ì–´ [?] ëŠ” í¥ë¯¸ë¡­ë‹¤' â†’ 'ì²˜ë¦¬'ë¥¼ ì˜ˆì¸¡\n",
    "- ë¹ ë¥¸ í•™ìŠµ, ë¹ˆë²ˆí•œ ë‹¨ì–´ì— ì¢‹ìŒ\n",
    "\n",
    "<br>\n",
    "\n",
    "Skip-gram:\n",
    "- ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡\n",
    "- ì˜ˆ: 'ì²˜ë¦¬' â†’ 'ìì—°ì–´', 'ëŠ”', 'í¥ë¯¸ë¡­ë‹¤' ì˜ˆì¸¡\n",
    "- ì •í™•í•œ ë²¡í„°, í¬ê·€ ë‹¨ì–´ì— ì¢‹ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a961f09c-6db1-4945-95fc-fc88fcf6d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Word2Vec ëª¨ë¸ í•™ìŠµ ì¤‘...\n",
      "âœ… Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š Word2Vec ëª¨ë¸ ì •ë³´:\n",
      "  ì–´íœ˜ í¬ê¸°: 48ê°œ\n",
      "  ë²¡í„° ì°¨ì›: 100ì°¨ì›\n",
      "  í•™ìŠµëœ ë‹¨ì–´ë“¤: ['nlp', 'text', 'models', 'language', 'learning', 'natural', 'many', 'process', 'revolutionized', 'tasks', 'deep', 'data', 'classification', 'amounts', 'large', 'analyze', 'algorithms', 'results', 'sentiment', 'machine', 'intelligence', 'artificial', 'field', 'fascinating', 'processing', 'analysis', 'understanding', 'preprocessing', 'essential', 'step', 'stateoftheart', 'achieve', 'gpt', 'bert', 'modern', 'architecture', 'dominant', 'become', 'transformers', 'words', 'relationships', 'semantic', 'capture', 'embeddings', 'word', 'pipeline', 'common', 'applications']\n",
      "\n",
      "ğŸ” ë‹¨ì–´ ë²¡í„° ì˜ˆì‹œ:\n",
      "  'nlp' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 100ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [-0.00053441  0.0002342   0.00509681  0.00901455 -0.00930748 -0.00712235\n",
      "  0.00645791  0.0089834  -0.00501605 -0.00377127]\n",
      "    ë²¡í„° í¬ê¸°(norm): 0.0566\n",
      "\n",
      "  'text' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 100ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [-0.00861682  0.00366286  0.00518951  0.00575312  0.00745169 -0.00618391\n",
      "  0.00111267  0.00605572 -0.00285139 -0.00618342]\n",
      "    ë²¡í„° í¬ê¸°(norm): 0.0580\n",
      "\n",
      "  'models' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 100ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [ 7.9089732e-05  3.0848815e-03 -6.8244315e-03 -1.3741386e-03\n",
      "  7.6535218e-03  7.3492429e-03 -3.6750515e-03  2.6406990e-03\n",
      " -8.3153769e-03  6.1992793e-03]\n",
      "    ë²¡í„° í¬ê¸°(norm): 0.0569\n",
      "\n",
      "  'language' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 100ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [-0.00825263  0.0093145  -0.00020367 -0.00196852  0.00460115 -0.00409642\n",
      "  0.00274841  0.00694611  0.00605521 -0.0075093 ]\n",
      "    ë²¡í„° í¬ê¸°(norm): 0.0580\n",
      "\n",
      "  'learning' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 100ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [-0.00713939  0.00123792 -0.00717642 -0.00223109  0.00370713  0.00582198\n",
      "  0.00120219  0.00211002 -0.00411433  0.00722092]\n",
      "    ë²¡í„° í¬ê¸°(norm): 0.0578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec ëª¨ë¸ í•™ìŠµ\n",
    "print(\"ğŸ”„ Word2Vec ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=preprocessed_texts,  # í•™ìŠµ ë°ì´í„° (í† í°í™”ëœ ë¬¸ì¥ë“¤)\n",
    "    vector_size=100,              # ì„ë² ë”© ë²¡í„° ì°¨ì› ìˆ˜\n",
    "    window=5,                     # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°\n",
    "    min_count=1,                  # ìµœì†Œ ì¶œí˜„ ë¹ˆë„ (ë°ì´í„°ê°€ ì ì–´ì„œ 1ë¡œ ì„¤ì •)\n",
    "    workers=4,                    # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜\n",
    "    sg=0,                        # 0=CBOW, 1=Skip-gram\n",
    "    epochs=10                    # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜\n",
    ")\n",
    "\n",
    "print(\"âœ… Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Word2Vec ëª¨ë¸ ì •ë³´:\")\n",
    "print(f\"  ì–´íœ˜ í¬ê¸°: {len(w2v_model.wv)}ê°œ\")\n",
    "print(f\"  ë²¡í„° ì°¨ì›: {w2v_model.wv.vector_size}ì°¨ì›\")\n",
    "print(f\"  í•™ìŠµëœ ë‹¨ì–´ë“¤: {list(w2v_model.wv.index_to_key)}\")\n",
    "\n",
    "print(f\"\\nğŸ” ë‹¨ì–´ ë²¡í„° ì˜ˆì‹œ:\")\n",
    "# í•™ìŠµëœ ëª¨ë“  ë‹¨ì–´ì˜ ë²¡í„° í™•ì¸\n",
    "for i, word in enumerate(w2v_model.wv.index_to_key[:5]):  # ì²˜ìŒ 5ê°œ ë‹¨ì–´ë§Œ\n",
    "    vector = w2v_model.wv[word]\n",
    "    print(f\"  '{word}' ë²¡í„°:\")\n",
    "    print(f\"    ì „ì²´ ì°¨ì›: {len(vector)}ì°¨ì›\")\n",
    "    print(f\"    ì²˜ìŒ 10ì°¨ì›: {vector[:10]}\")\n",
    "    print(f\"    ë²¡í„° í¬ê¸°(norm): {np.linalg.norm(vector):.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfe66ac0-b44f-44d5-bb8e-de2f4ed59735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°:\n",
      "ìœ ì‚¬ë„ = ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë‘ ë²¡í„° ê°„ ê°ë„ë¡œ ì¸¡ì •)\n",
      "  1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ë ¨ ì—†ìŒ\n",
      "\n",
      "ğŸ’¡ ë‹¨ì–´ ìŒë³„ ìœ ì‚¬ë„:\n",
      "  'natural' â†” 'nlp': -0.0597\n",
      "  'natural' â†” 'text': 0.0095\n",
      "  'natural' â†” 'models': 0.0649\n",
      "  'natural' â†” 'language': 0.1319\n",
      "  'natural' â†” 'learning': 0.1391\n",
      "  'natural' â†” 'many': 0.0192\n",
      "  'natural' â†” 'process': -0.0578\n",
      "  'natural' â†” 'revolutionized': 0.0610\n"
     ]
    }
   ],
   "source": [
    "def calculate_word_similarity(w2v_model):\n",
    "    \"\"\"ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚° ë° ì„¤ëª…\"\"\"\n",
    "    print(\"ğŸ“ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°:\")\n",
    "    print(\"ìœ ì‚¬ë„ = ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë‘ ë²¡í„° ê°„ ê°ë„ë¡œ ì¸¡ì •)\")\n",
    "    print(\"  1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ë ¨ ì—†ìŒ\")\n",
    "    print()\n",
    "    \n",
    "    # í•™ìŠµëœ ë‹¨ì–´ë“¤ ì¤‘ì—ì„œ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    vocab = list(w2v_model.wv.index_to_key)\n",
    "    \n",
    "    if len(vocab) >= 2:\n",
    "        print(\"ğŸ’¡ ë‹¨ì–´ ìŒë³„ ìœ ì‚¬ë„:\")\n",
    "        \n",
    "        # ëª‡ ê°€ì§€ ë‹¨ì–´ ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        word_pairs = []\n",
    "        \n",
    "        # 'natural'ê³¼ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ ë¹„êµ\n",
    "        if 'natural' in vocab:\n",
    "            for word in vocab:\n",
    "                if word != 'natural':\n",
    "                    word_pairs.append(('natural', word))\n",
    "        \n",
    "        # 'learning'ê³¼ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ ë¹„êµ  \n",
    "        if 'learning' in vocab:\n",
    "            for word in vocab:\n",
    "                if word != 'learning' and ('learning', word) not in word_pairs:\n",
    "                    word_pairs.append(('learning', word))\n",
    "        \n",
    "        # ì²˜ìŒ ëª‡ ê°œë§Œ ë³´ì—¬ì£¼ê¸°\n",
    "        for word1, word2 in word_pairs[:8]:\n",
    "            try:\n",
    "                similarity = w2v_model.wv.similarity(word1, word2)\n",
    "                print(f\"  '{word1}' â†” '{word2}': {similarity:.4f}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"  '{word1}' â†” '{word2}': ê³„ì‚° ë¶ˆê°€ ({e})\")\n",
    "    else:\n",
    "        print(\"ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ì¶©ë¶„í•œ ì–´íœ˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "calculate_word_similarity(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e09d64fc-b5b9-45a9-b049-eaa54b0077ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°:\n",
      "ê° ë‹¨ì–´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“ 'learning'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'amounts': 0.2534\n",
      "  2. 'understanding': 0.2008\n",
      "  3. 'transformers': 0.1953\n",
      "\n",
      "ğŸ“ 'natural'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'data': 0.1671\n",
      "  2. 'stateoftheart': 0.1632\n",
      "  3. 'learning': 0.1391\n",
      "\n",
      "ğŸ“ 'language'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'large': 0.1783\n",
      "  2. 'dominant': 0.1638\n",
      "  3. 'pipeline': 0.1496\n",
      "\n",
      "ğŸ“ 'processing'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'semantic': 0.1924\n",
      "  2. 'capture': 0.1566\n",
      "  3. 'deep': 0.1020\n",
      "\n",
      "ğŸ“ 'data'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'stateoftheart': 0.1819\n",
      "  2. 'models': 0.1729\n",
      "  3. 'natural': 0.1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_similar_words(w2v_model):\n",
    "    \"\"\"ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°\"\"\"\n",
    "    print(f\"\\nğŸ” ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°:\")\n",
    "    print(\"ê° ë‹¨ì–´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.\")\n",
    "    print()\n",
    "    \n",
    "    vocab = list(w2v_model.wv.index_to_key)\n",
    "    \n",
    "    # ëª‡ ê°œ ë‹¨ì–´ì— ëŒ€í•´ ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°\n",
    "    target_words = ['learning', 'natural', 'language', 'processing', 'data']\n",
    "    \n",
    "    for target_word in target_words:\n",
    "        if target_word in vocab:\n",
    "            print(f\"ğŸ“ '{target_word}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\")\n",
    "            try:\n",
    "                # ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ 3ê°œ ë‹¨ì–´ ì°¾ê¸°\n",
    "                similar_words = w2v_model.wv.most_similar(target_word, topn=3)\n",
    "                \n",
    "                if similar_words:\n",
    "                    for rank, (word, score) in enumerate(similar_words, 1):\n",
    "                        print(f\"  {rank}. '{word}': {score:.4f}\")\n",
    "                else:\n",
    "                    print(\"  ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  ì˜¤ë¥˜: {e}\")\n",
    "            print()\n",
    "\n",
    "find_similar_words(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81cc93d1-6ade-4d7e-ab94-35cd6d75f7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® ë²¡í„° ì—°ì‚° (ë‹¨ì–´ ê´€ê³„ í•™ìŠµ):\n",
      "Word2Vecì˜ ë†€ë¼ìš´ íŠ¹ì„±: ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥!\n",
      "ì˜ˆ: king - man + woman = queen (ì™• - ë‚¨ì + ì—¬ì = ì—¬ì™•)\n",
      "\n",
      "ğŸ”¬ ìš°ë¦¬ ë°ì´í„°ë¡œ ë²¡í„° ì—°ì‚° ì‹¤í—˜:\n",
      "  'natural' - 'language' + 'processing' â‰ˆ 'semantic' (0.2005)\n",
      "  'machine' - 'learning' + 'data' â‰ˆ 'process' (0.2366)\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_vector_arithmetic(w2v_model):\n",
    "    \"\"\"ë²¡í„° ì—°ì‚° ë°ëª¨\"\"\"\n",
    "    print(\"ğŸ§® ë²¡í„° ì—°ì‚° (ë‹¨ì–´ ê´€ê³„ í•™ìŠµ):\")\n",
    "    print(\"Word2Vecì˜ ë†€ë¼ìš´ íŠ¹ì„±: ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥!\")\n",
    "    print(\"ì˜ˆ: king - man + woman = queen (ì™• - ë‚¨ì + ì—¬ì = ì—¬ì™•)\")\n",
    "    print()\n",
    "    \n",
    "    vocab = list(w2v_model.wv.index_to_key)\n",
    "    \n",
    "    # ìš°ë¦¬ ë°ì´í„°ì—ì„œ ê°€ëŠ¥í•œ ë²¡í„° ì—°ì‚° ì‹œë„\n",
    "    if len(vocab) >= 3:\n",
    "        print(\"ğŸ”¬ ìš°ë¦¬ ë°ì´í„°ë¡œ ë²¡í„° ì—°ì‚° ì‹¤í—˜:\")\n",
    "        \n",
    "        # ëª‡ ê°€ì§€ ì¡°í•© ì‹œë„\n",
    "        combinations = [\n",
    "            ('natural', 'language', 'processing'),\n",
    "            ('machine', 'learning', 'data'),\n",
    "            ('deep', 'learning', 'model')\n",
    "        ]\n",
    "        \n",
    "        for word1, word2, word3 in combinations:\n",
    "            if all(word in vocab for word in [word1, word2, word3]):\n",
    "                try:\n",
    "                    # word1 - word2 + word3ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°\n",
    "                    result = w2v_model.wv.most_similar(\n",
    "                        positive=[word1, word3], \n",
    "                        negative=[word2], \n",
    "                        topn=1\n",
    "                    )\n",
    "                    if result:\n",
    "                        result_word, score = result[0]\n",
    "                        print(f\"  '{word1}' - '{word2}' + '{word3}' â‰ˆ '{result_word}' ({score:.4f})\")\n",
    "                except:\n",
    "                    print(f\"  '{word1}' - '{word2}' + '{word3}': ê³„ì‚° ì‹¤íŒ¨\")\n",
    "    \n",
    "    if not any(all(word in vocab for word in combo) for combo in combinations):\n",
    "        print(\"ë²¡í„° ì—°ì‚°ì„ ìœ„í•œ ì¶©ë¶„í•œ ê´€ë ¨ ë‹¨ì–´ë“¤ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµí•˜ë©´ ë” í¥ë¯¸ë¡œìš´ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "demonstrate_vector_arithmetic(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a95ccd-fd96-4f32-982b-928e9244dc0c",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7377f79-1ef9-42d8-85c6-e2f0cd2563e9",
   "metadata": {},
   "source": [
    "### 6. GloVe (ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©)\n",
    "\n",
    "GloVe ê°œë…:\n",
    "- Stanfordì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•\n",
    "- Word2Vecê³¼ ë‹¬ë¦¬ ì „ì—­ì (Global) í†µê³„ ì •ë³´ë¥¼ í™œìš©\n",
    "- ë‹¨ì–´-ë‹¨ì–´ ë™ì‹œì¶œí˜„ í–‰ë ¬(Co-occurrence Matrix)ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ\n",
    "- Word2Vecì˜ ì˜ˆì¸¡ ê¸°ë°˜ ë°©ë²•ê³¼ ì „í†µì ì¸ í†µê³„ ë°©ë²•ì„ ê²°í•©\n",
    "\n",
    "GloVeì˜ í•µì‹¬ ì•„ì´ë””ì–´:\n",
    "- \"ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ì „ì²´ ë§ë­‰ì¹˜ì—ì„œì˜ ë™ì‹œì¶œí˜„ íŒ¨í„´ìœ¼ë¡œ ê²°ì •ëœë‹¤\"\n",
    "- ì˜ˆ: 'ì–¼ìŒ'ê³¼ 'ì¦ê¸°' ëª¨ë‘ 'ë¬¼'ê³¼ ìì£¼ ë‚˜íƒ€ë‚˜ì§€ë§Œ, 'ê³ ì²´'ëŠ” 'ì–¼ìŒ'ê³¼ ë” ê´€ë ¨ìˆìŒ\n",
    "- ì´ëŸ¬í•œ ë¹„ìœ¨ ì •ë³´ë¥¼ ë²¡í„°ë¡œ í•™ìŠµ\n",
    "\n",
    "Word2Vec vs GloVe:\n",
    "- Word2Vec: ì§€ì—­ì  ë¬¸ë§¥ ìœˆë„ìš° ë‚´ì—ì„œ ì˜ˆì¸¡ í•™ìŠµ\n",
    "- GloVe: ì „ì²´ ë§ë­‰ì¹˜ì˜ í†µê³„ì  ì •ë³´ë¥¼ ì§ì ‘ í™œìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cde18072-45eb-4f1a-8cd5-5df969cac5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ GloVe ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì‹œì‘...\n",
      "â±ï¸  ì²« ì‹¤í–‰ ì‹œ ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“¦ ëª¨ë¸ í¬ê¸°: ì•½ 50MB (glove-wiki-gigaword-50)\n",
      "\n",
      "ğŸ“¥ Gensim APIë¥¼ í†µí•´ GloVe ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "âœ… GloVe ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ GloVe ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì‹œì‘...\")\n",
    "print(\"â±ï¸  ì²« ì‹¤í–‰ ì‹œ ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"ğŸ“¦ ëª¨ë¸ í¬ê¸°: ì•½ 50MB (glove-wiki-gigaword-50)\")\n",
    "\n",
    "# ì‘ì€ GloVe ëª¨ë¸ ë¡œë“œ (50ì°¨ì›)\n",
    "print(\"\\nğŸ“¥ Gensim APIë¥¼ í†µí•´ GloVe ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "print(\"âœ… GloVe ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea2fbbe1-df5f-4a76-8f64-2b73897d190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š GloVe ëª¨ë¸ ì •ë³´:\n",
      "  ì–´íœ˜ í¬ê¸°: 400,000ê°œ ë‹¨ì–´\n",
      "  ë²¡í„° ì°¨ì›: 50ì°¨ì›\n",
      "  í•™ìŠµ ë°ì´í„°: Wikipedia + Gigaword (60ì–µ í† í°)\n",
      "\n",
      "ğŸ“– ì–´íœ˜ ì˜ˆì‹œ (ì²˜ìŒ 20ê°œ):\n",
      "   1. 'the'\n",
      "   2. ','\n",
      "   3. '.'\n",
      "   4. 'of'\n",
      "   5. 'to'\n",
      "   6. 'and'\n",
      "   7. 'in'\n",
      "   8. 'a'\n",
      "   9. '\"'\n",
      "  10. ''s'\n",
      "  11. 'for'\n",
      "  12. '-'\n",
      "  13. 'that'\n",
      "  14. 'on'\n",
      "  15. 'is'\n",
      "  16. 'was'\n",
      "  17. 'said'\n",
      "  18. 'with'\n",
      "  19. 'he'\n",
      "  20. 'as'\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š GloVe ëª¨ë¸ ì •ë³´:\")\n",
    "print(f\"  ì–´íœ˜ í¬ê¸°: {len(glove_model):,}ê°œ ë‹¨ì–´\")\n",
    "print(f\"  ë²¡í„° ì°¨ì›: 50ì°¨ì›\")\n",
    "print(f\"  í•™ìŠµ ë°ì´í„°: Wikipedia + Gigaword (60ì–µ í† í°)\")\n",
    "\n",
    "# ì–´íœ˜ ì˜ˆì‹œ í™•ì¸\n",
    "print(f\"\\nğŸ“– ì–´íœ˜ ì˜ˆì‹œ (ì²˜ìŒ 20ê°œ):\")\n",
    "vocab_sample = list(glove_model.index_to_key[:20])\n",
    "for i, word in enumerate(vocab_sample):\n",
    "    print(f\"  {i+1:2d}. '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bcf3b48-c9c5-4e25-bdb9-8f33e4484035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ë‹¨ì–´ ë²¡í„° ì˜ˆì‹œ:\n",
      "  ğŸ“ 'natural' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 50ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [ 0.44265  0.84765 -0.4598   0.67993  0.13841  0.39456 -0.17343 -0.64055\n",
      "  0.86439  0.81624]\n",
      "    ë²¡í„° í¬ê¸°: 5.0903\n",
      "    ìµœëŒ€ê°’: 3.3077, ìµœì†Œê°’: -0.9165\n",
      "\n",
      "  ğŸ“ 'language' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 50ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [-0.5799    -0.1101    -1.1557    -0.0029906 -0.20613    0.45289\n",
      " -0.16671   -1.0382    -0.99241    0.39884  ]\n",
      "    ë²¡í„° í¬ê¸°: 6.0993\n",
      "    ìµœëŒ€ê°’: 3.7163, ìµœì†Œê°’: -1.3878\n",
      "\n",
      "  ğŸ“ 'processing' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 50ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [ 1.6092e-01 -9.0221e-01  1.5797e-01  1.1776e+00 -6.2201e-04 -1.9004e-02\n",
      " -1.5081e-01 -5.8863e-01  1.5128e+00  4.2868e-01]\n",
      "    ë²¡í„° í¬ê¸°: 5.3926\n",
      "    ìµœëŒ€ê°’: 3.2094, ìµœì†Œê°’: -0.9775\n",
      "\n",
      "  ğŸ“ 'fascinating' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 50ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [ 0.90512   0.63951  -0.94111   0.33322   1.0375   -0.060236  0.043731\n",
      " -0.26376   0.074989  0.8521  ]\n",
      "    ë²¡í„° í¬ê¸°: 4.6380\n",
      "    ìµœëŒ€ê°’: 1.6140, ìµœì†Œê°’: -1.6849\n",
      "\n",
      "  ğŸ“ 'field' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 50ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [-0.49284  0.3731   0.15565  0.70044  0.77405 -0.48151 -1.2244  -0.02163\n",
      "  0.63856 -0.49535]\n",
      "    ë²¡í„° í¬ê¸°: 5.1761\n",
      "    ìµœëŒ€ê°’: 2.9194, ìµœì†Œê°’: -1.7308\n",
      "\n",
      "  ğŸ“ 'artificial' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 50ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [ 0.71561  0.70368 -0.65484  0.49875 -0.85243  1.1059  -0.1036  -0.88284\n",
      "  0.80456  0.71413]\n",
      "    ë²¡í„° í¬ê¸°: 4.5111\n",
      "    ìµœëŒ€ê°’: 2.3077, ìµœì†Œê°’: -0.8828\n",
      "\n",
      "  ğŸ“ 'intelligence' ë²¡í„°:\n",
      "    ì „ì²´ ì°¨ì›: 50ì°¨ì›\n",
      "    ì²˜ìŒ 10ì°¨ì›: [ 0.8782   -0.45171   0.96737   0.040347  0.76235  -0.63825   0.18944\n",
      " -0.26633   0.58874  -0.93608 ]\n",
      "    ë²¡í„° í¬ê¸°: 5.7777\n",
      "    ìµœëŒ€ê°’: 2.4132, ìµœì†Œê°’: -2.3939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_word_vectors(preprocessed_texts):\n",
    "    \"\"\"ë‹¨ì–´ ë²¡í„° ì˜ˆì‹œ\"\"\"\n",
    "    print(f\"\\nğŸ” ë‹¨ì–´ ë²¡í„° ì˜ˆì‹œ:\")\n",
    "\n",
    "    for word in preprocessed_texts:\n",
    "        if word in glove_model:\n",
    "            vector = glove_model[word]\n",
    "            print(f\"  ğŸ“ '{word}' ë²¡í„°:\")\n",
    "            print(f\"    ì „ì²´ ì°¨ì›: {len(vector)}ì°¨ì›\")\n",
    "            print(f\"    ì²˜ìŒ 10ì°¨ì›: {vector[:10]}\")\n",
    "            print(f\"    ë²¡í„° í¬ê¸°: {np.linalg.norm(vector):.4f}\")\n",
    "            print(f\"    ìµœëŒ€ê°’: {np.max(vector):.4f}, ìµœì†Œê°’: {np.min(vector):.4f}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"  âŒ '{word}'ëŠ” ì–´íœ˜ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "demonstrate_word_vectors(preprocessed_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3f18926-0f1c-4b7b-8807-628c3029121c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°:\n",
      "ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì´ ê¹Šì€ ë‹¨ì–´ë“¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ’¡ ë‹¨ì–´ ìŒë³„ ìœ ì‚¬ë„:\n",
      "  'artificial' â†” 'intelligence': 0.1664\n",
      "    â†’ ë‚®ì€ ìœ ì‚¬ë„ (ê´€ë ¨ì„± ì ìŒ)\n",
      "\n",
      "  'computer' â†” 'technology': 0.8526\n",
      "    â†’ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (ê°•í•œ ê´€ë ¨ì„±)\n",
      "\n",
      "  'language' â†” 'communication': 0.5878\n",
      "    â†’ ë†’ì€ ìœ ì‚¬ë„ (ê´€ë ¨ì„± ìˆìŒ)\n",
      "\n",
      "  'learning' â†” 'education': 0.7518\n",
      "    â†’ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (ê°•í•œ ê´€ë ¨ì„±)\n",
      "\n",
      "  'deep' â†” 'shallow': 0.7442\n",
      "    â†’ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (ê°•í•œ ê´€ë ¨ì„±)\n",
      "\n",
      "  'king' â†” 'queen': 0.7839\n",
      "    â†’ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (ê°•í•œ ê´€ë ¨ì„±)\n",
      "\n",
      "  'man' â†” 'woman': 0.8860\n",
      "    â†’ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (ê°•í•œ ê´€ë ¨ì„±)\n",
      "\n",
      "  'car' â†” 'vehicle': 0.8834\n",
      "    â†’ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (ê°•í•œ ê´€ë ¨ì„±)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‹¨ì–´ìŒë“¤\n",
    "word_pairs = [\n",
    "    ('artificial', 'intelligence'),\n",
    "    ('computer', 'technology'),\n",
    "    ('language', 'communication'),\n",
    "    ('learning', 'education'),\n",
    "    ('deep', 'shallow'),\n",
    "    ('king', 'queen'),\n",
    "    ('man', 'woman'),\n",
    "    ('car', 'vehicle')\n",
    "]\n",
    "\n",
    "def calculate_similarities(word_pairs):\n",
    "    \"\"\"ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "    print(\"ğŸ“ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°:\")\n",
    "    print(\"ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì´ ê¹Šì€ ë‹¨ì–´ë“¤ì…ë‹ˆë‹¤.\")\n",
    "    print()\n",
    "\n",
    "    print(\"ğŸ’¡ ë‹¨ì–´ ìŒë³„ ìœ ì‚¬ë„:\")\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in glove_model and word2 in glove_model:\n",
    "            similarity = glove_model.similarity(word1, word2)\n",
    "            print(f\"  '{word1}' â†” '{word2}': {similarity:.4f}\")\n",
    "            \n",
    "            # ìœ ì‚¬ë„ í•´ì„\n",
    "            if similarity > 0.7:\n",
    "                print(f\"    â†’ ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (ê°•í•œ ê´€ë ¨ì„±)\")\n",
    "            elif similarity > 0.5:\n",
    "                print(f\"    â†’ ë†’ì€ ìœ ì‚¬ë„ (ê´€ë ¨ì„± ìˆìŒ)\")\n",
    "            elif similarity > 0.3:\n",
    "                print(f\"    â†’ ì¤‘ê°„ ìœ ì‚¬ë„ (ì•½ê°„ ê´€ë ¨)\")\n",
    "            else:\n",
    "                print(f\"    â†’ ë‚®ì€ ìœ ì‚¬ë„ (ê´€ë ¨ì„± ì ìŒ)\")\n",
    "        else:\n",
    "            missing_words = [w for w in [word1, word2] if w not in glove_model]\n",
    "            print(f\"  '{word1}' â†” '{word2}': ê³„ì‚° ë¶ˆê°€ (ì–´íœ˜ì— ì—†ìŒ: {missing_words})\")\n",
    "        print()\n",
    "\n",
    "calculate_similarities(word_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1348e-6dbd-4b54-a186-e8e97f0dac31",
   "metadata": {},
   "source": [
    "ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚°ì´ë¼ëŠ” ê¸°ë³¸ ì›ë¦¬ë¥¼ ë‹¤ì–‘í•œ ì‹¤ì „ ë¬¸ì œì— ì‘ìš©\n",
    "- ê²€ìƒ‰ ì‹œìŠ¤í…œ - ì‚¬ìš©ìê°€ \"ë…¸íŠ¸ë¶\" ê²€ìƒ‰í•´ë„ \"laptop\" ë¬¸ì„œ ì°¾ì•„ì¤Œ\n",
    "- ì¶”ì²œ ì‹œìŠ¤í…œ - ë„·í”Œë¦­ìŠ¤, ìŠ¤í¬í‹°íŒŒì´ ê°™ì€ ê°œì¸í™” ì¶”ì²œ\n",
    "- ê°ì • ë¶„ì„ - ê³ ê° ë¦¬ë·°, SNS ëª¨ë‹ˆí„°ë§ ìë™í™”\n",
    "- ì±—ë´‡ - ë‹¤ì–‘í•œ í‘œí˜„ì„ ì´í•´í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ê³ ê° ì„œë¹„ìŠ¤\n",
    "- ì‚¬ê¸° íƒì§€ - í”¼ì‹± ë©”ì¼, í—ˆìœ„ ë¦¬ë·° ìë™ ê°ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31dbc2c1-0868-430e-b8a4-fbb588ad2f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°:\n",
      "ê° ë‹¨ì–´ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“ 'computer'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'computers': 0.9165\n",
      "  2. 'software': 0.8815\n",
      "  3. 'technology': 0.8526\n",
      "  4. 'electronic': 0.8126\n",
      "  5. 'internet': 0.8060\n",
      "\n",
      "ğŸ“ 'artificial'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'natural': 0.7425\n",
      "  2. 'tissue': 0.7081\n",
      "  3. 'synthetic': 0.7078\n",
      "  4. 'developed': 0.6942\n",
      "  5. 'therapeutic': 0.6857\n",
      "\n",
      "ğŸ“ 'language'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'languages': 0.8815\n",
      "  2. 'word': 0.8100\n",
      "  3. 'spoken': 0.8075\n",
      "  4. 'vocabulary': 0.7903\n",
      "  5. 'translation': 0.7879\n",
      "\n",
      "ğŸ“ 'learning'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'teaching': 0.8757\n",
      "  2. 'skills': 0.8351\n",
      "  3. 'experience': 0.8201\n",
      "  4. 'practical': 0.8190\n",
      "  5. 'knowledge': 0.8090\n",
      "\n",
      "ğŸ“ 'science'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  1. 'sciences': 0.8548\n",
      "  2. 'research': 0.8437\n",
      "  3. 'institute': 0.8386\n",
      "  4. 'studies': 0.8369\n",
      "  5. 'physics': 0.8314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_words = ['computer', 'artificial', 'language', 'learning', 'science']\n",
    "\n",
    "def find_similar_words(target_words):\n",
    "    \"\"\"ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°\"\"\"\n",
    "    print(\"ğŸ” ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°:\")\n",
    "    print(\"ê° ë‹¨ì–´ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.\")\n",
    "    print()\n",
    "    \n",
    "    for word in target_words:\n",
    "        if word in glove_model:\n",
    "            print(f\"ğŸ“ '{word}'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\")\n",
    "            try:\n",
    "                # ìƒìœ„ 5ê°œ ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°\n",
    "                similar_words = glove_model.most_similar(word, topn=5)\n",
    "                \n",
    "                for rank, (similar_word, score) in enumerate(similar_words, 1):\n",
    "                    print(f\"  {rank}. '{similar_word}': {score:.4f}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"âŒ '{word}'ëŠ” ì–´íœ˜ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            print()\n",
    "\n",
    "find_similar_words(target_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68444ae7-665b-4e3c-9f1b-3633fc7ee7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® ë‹¨ì–´ ìœ ì¶” (Word Analogies):\n",
      "GloVeì˜ ë†€ë¼ìš´ ëŠ¥ë ¥: ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í‘œí˜„!\n",
      "í˜•íƒœ: A : B = C : ?  (Aê°€ Bì— ëŒ€ì‘ë˜ëŠ” ê²ƒì²˜ëŸ¼, CëŠ” ë¬´ì—‡ì— ëŒ€ì‘ë ê¹Œ?)\n",
      "\n",
      "ğŸ”¬ ë‹¨ì–´ ìœ ì¶” ì‹¤í—˜:\n",
      "  ğŸ¯ 'king' - 'man' + 'queen' = ?\n",
      "    ê²°ê³¼ í›„ë³´:\n",
      "      1. 'coronation': 0.7995\n",
      "      2. 'hrh': 0.7570\n",
      "      3. 'throne': 0.7358\n",
      "    ğŸ’­ ê¸°ëŒ€í•œ ë‹µì€ 'woman'ì´ì—ˆì§€ë§Œ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤.\n",
      "\n",
      "  ğŸ¯ 'paris' - 'france' + 'london' = ?\n",
      "    ê²°ê³¼ í›„ë³´:\n",
      "      1. 'opened': 0.7355\n",
      "      2. 'at': 0.7293\n",
      "      3. 'hotel': 0.7116\n",
      "    ğŸ’­ ê¸°ëŒ€í•œ ë‹µì€ 'england'ì´ì—ˆì§€ë§Œ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤.\n",
      "\n",
      "  ğŸ¯ 'good' - 'better' + 'bad' = ?\n",
      "    ê²°ê³¼ í›„ë³´:\n",
      "      1. 'little': 0.8396\n",
      "      2. 'luck': 0.8365\n",
      "      3. 'thing': 0.8204\n",
      "    ğŸ’­ ê¸°ëŒ€í•œ ë‹µì€ 'worse'ì´ì—ˆì§€ë§Œ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤.\n",
      "\n",
      "  ğŸ¯ 'walking' - 'walked' + 'running' = ?\n",
      "    ê²°ê³¼ í›„ë³´:\n",
      "      1. 'turning': 0.7794\n",
      "      2. 'track': 0.7608\n",
      "      3. 'course': 0.7578\n",
      "    ğŸ’­ ê¸°ëŒ€í•œ ë‹µì€ 'ran'ì´ì—ˆì§€ë§Œ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ìœ ëª…í•œ ë‹¨ì–´ ìœ ì¶” ì˜ˆì œë“¤\n",
    "analogies = [\n",
    "    ('king', 'man', 'queen'),      # king - man + woman = queen ì˜ë„\n",
    "    ('paris', 'france', 'london'), # paris - france + england = london ì˜ë„  \n",
    "    ('good', 'better', 'bad'),     # good - better + worse = bad ì˜ë„\n",
    "    ('walking', 'walked', 'running'), # walking - walked + ran = running ì˜ë„\n",
    "]\n",
    "\n",
    "def demonstrate_word_analogies(analogies):\n",
    "    \"\"\"ë‹¨ì–´ ìœ ì¶” (Word Analogies) ë°ëª¨\"\"\"\n",
    "    print(\"ğŸ§® ë‹¨ì–´ ìœ ì¶” (Word Analogies):\")\n",
    "    print(\"GloVeì˜ ë†€ë¼ìš´ ëŠ¥ë ¥: ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í‘œí˜„!\")\n",
    "    print(\"í˜•íƒœ: A : B = C : ?  (Aê°€ Bì— ëŒ€ì‘ë˜ëŠ” ê²ƒì²˜ëŸ¼, CëŠ” ë¬´ì—‡ì— ëŒ€ì‘ë ê¹Œ?)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”¬ ë‹¨ì–´ ìœ ì¶” ì‹¤í—˜:\")\n",
    "    for word1, word2, word3 in analogies:\n",
    "        # ëª¨ë“  ë‹¨ì–´ê°€ ì–´íœ˜ì— ìˆëŠ”ì§€ í™•ì¸\n",
    "        if all(word in glove_model for word in [word1, word2, word3]):\n",
    "            try:\n",
    "                # word1 - word2 + word3 ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°\n",
    "                # positive=[word1, word3]: ë”í•˜ëŠ” ë²¡í„°ë“¤\n",
    "                # negative=[word2]: ë¹¼ëŠ” ë²¡í„°\n",
    "                result = glove_model.most_similar(\n",
    "                    positive=[word1, word3], \n",
    "                    negative=[word2], \n",
    "                    topn=3\n",
    "                )\n",
    "                \n",
    "                print(f\"  ğŸ¯ '{word1}' - '{word2}' + '{word3}' = ?\")\n",
    "                print(f\"    ê²°ê³¼ í›„ë³´:\")\n",
    "                for rank, (result_word, score) in enumerate(result, 1):\n",
    "                    print(f\"      {rank}. '{result_word}': {score:.4f}\")\n",
    "                \n",
    "                # ì²« ë²ˆì§¸ ê²°ê³¼ê°€ ì˜ë„í•œ ë‹µì¸ì§€ í™•ì¸\n",
    "                if result:\n",
    "                    best_answer = result[0][0]\n",
    "                    expected_answers = {\n",
    "                        ('king', 'man', 'queen'): 'woman',\n",
    "                        ('paris', 'france', 'london'): 'england', \n",
    "                        ('good', 'better', 'bad'): 'worse',\n",
    "                        ('walking', 'walked', 'running'): 'ran'\n",
    "                    }\n",
    "                    expected = expected_answers.get((word1, word2, word3))\n",
    "                    if expected and expected in [r[0] for r in result[:3]]:\n",
    "                        print(f\"    âœ… ê¸°ëŒ€í•œ ë‹µ '{expected}'ì´ ìƒìœ„ 3ê°œ ì•ˆì— ìˆìŠµë‹ˆë‹¤!\")\n",
    "                    else:\n",
    "                        print(f\"    ğŸ’­ ê¸°ëŒ€í•œ ë‹µì€ '{expected}'ì´ì—ˆì§€ë§Œ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ '{word1}' - '{word2}' + '{word3}': ê³„ì‚° ì‹¤íŒ¨ ({e})\")\n",
    "            print()\n",
    "        else:\n",
    "            missing = [w for w in [word1, word2, word3] if w not in glove_model]\n",
    "            print(f\"  âŒ '{word1}' - '{word2}' + '{word3}': ì–´íœ˜ì— ì—†ëŠ” ë‹¨ì–´ {missing}\")\n",
    "            print()\n",
    "\n",
    "demonstrate_word_analogies(analogies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d86f34-bebc-4f5e-b6af-a353ab875855",
   "metadata": {},
   "source": [
    "#### ì™„ë²½í•œ ë‹µì´ ì•ˆë‚˜ì˜¤ëŠ” ì´ìœ ?\n",
    "1. ëª¨ë¸ í¬ê¸°ì˜ í•œê³„\n",
    "- ìš°ë¦¬ê°€ ì‚¬ìš©í•œ glove-wiki-gigaword-50ì€ 50ì°¨ì›ì§œë¦¬ ì‘ì€ ëª¨ë¸\n",
    "- ìœ ëª…í•œ \"king - man + woman = queen\" ì˜ˆì œëŠ” ë³´í†µ 300ì°¨ì› ëª¨ë¸ì—ì„œ ì˜ ì‘ë™\n",
    "- ì°¨ì›ì´ ì ìœ¼ë©´ ë¯¸ë¬˜í•œ ì˜ë¯¸ ê´€ê³„ë¥¼ ì œëŒ€ë¡œ í¬ì°©í•˜ê¸° ì–´ë ¤ì›€\n",
    "\n",
    "2. ë²¡í„° ì—°ì‚°ì˜ í•œê³„\n",
    "- ë²¡í„° ê³µê°„ì—ì„œëŠ” ì—¬ëŸ¬ ì˜ë¯¸ ê´€ê³„ê°€ ë³µí•©ì ìœ¼ë¡œ ì–½í˜€ìˆìŒ\n",
    "- \"king - man + woman\"ì´ ì •í™•íˆ \"queen\"ë§Œì„ ê°€ë¦¬í‚¤ì§€ ì•ŠìŒ\n",
    "- ì™•ì‹¤, ê¶Œë ¥, ì„±ë³„ ë“± ì—¬ëŸ¬ ê°œë…ì´ ë™ì‹œì— í™œì„±í™”ë¨\n",
    "\n",
    "'king' - 'man' + 'queen' = ?<br>\n",
    "ê²°ê³¼: 'coronation', 'hrh', 'throne'<br>\n",
    "â†’ ì‹¤ì œë¡œëŠ” 'ì™•ê¶Œ', 'ì™•ì‹¤', 'ì™•ì¢Œ' ê´€ë ¨ ë‹¨ì–´ë“¤!<br>\n",
    "â†’ ì˜ë¯¸ì ìœ¼ë¡œëŠ” ë§ëŠ” ë‹µ! ë‹¨ì§€ ìš°ë¦¬ê°€ ì›í•œ ë‹µì´ ì•„ë‹ ë¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd07bd0f-43bb-4023-8ed4-9776b7d99e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Word2Vec vs GloVe ì„±ëŠ¥ ë¹„êµ:\n",
      "==================================================\n",
      "ğŸ“Š ê³µí†µ ì–´íœ˜: 47ê°œ ë‹¨ì–´\n",
      "\n",
      "ğŸ“ ìœ ì‚¬ë„ ë¹„êµ (ìƒìœ„ 6ê°œ ìŒ):\n",
      "ë‹¨ì–´ìŒ                  Word2Vec     GloVe        ì°¨ì´        \n",
      "------------------------------------------------------------\n",
      "models-learning      0.1707       0.4946       0.3239    \n",
      "models-semantic      0.2123       0.4127       0.2003    \n",
      "learning-semantic    -0.1680      0.4759       0.6439    \n",
      "learning-many        0.0347       0.6012       0.5665    \n",
      "semantic-many        0.0794       0.1504       0.0711    \n",
      "semantic-fascinating -0.0221      0.3300       0.3521    \n"
     ]
    }
   ],
   "source": [
    "def compare_with_word2vec(w2v_model, glove_model):\n",
    "    \"\"\"Word2Vecê³¼ GloVe ë¹„êµ\"\"\"\n",
    "    print(\"âš–ï¸ Word2Vec vs GloVe ì„±ëŠ¥ ë¹„êµ:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ê³µí†µ ë‹¨ì–´ë¡œ ë¹„êµ (ë‘˜ ë‹¤ ìˆëŠ” ë‹¨ì–´ ì°¾ê¸°)\n",
    "    w2v_vocab = set(w2v_model.wv.index_to_key)\n",
    "    glove_vocab = set(glove_model.index_to_key)\n",
    "    common_words = w2v_vocab.intersection(glove_vocab)\n",
    "    \n",
    "    if len(common_words) >= 2:\n",
    "        print(f\"ğŸ“Š ê³µí†µ ì–´íœ˜: {len(common_words)}ê°œ ë‹¨ì–´\")\n",
    "        \n",
    "        # ëª‡ ê°œ ë‹¨ì–´ ìŒì˜ ìœ ì‚¬ë„ ë¹„êµ\n",
    "        test_pairs = []\n",
    "        common_list = list(common_words)\n",
    "        for i in range(min(3, len(common_list))):\n",
    "            for j in range(i+1, min(i+3, len(common_list))):\n",
    "                test_pairs.append((common_list[i], common_list[j]))\n",
    "        \n",
    "        print(f\"\\nğŸ“ ìœ ì‚¬ë„ ë¹„êµ (ìƒìœ„ {len(test_pairs)}ê°œ ìŒ):\")\n",
    "        print(f\"{'ë‹¨ì–´ìŒ':<20} {'Word2Vec':<12} {'GloVe':<12} {'ì°¨ì´':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for word1, word2 in test_pairs:\n",
    "            try:\n",
    "                w2v_sim = w2v_model.wv.similarity(word1, word2)\n",
    "                glove_sim = glove_model.similarity(word1, word2)\n",
    "                diff = abs(w2v_sim - glove_sim)\n",
    "                \n",
    "                pair_name = f\"{word1}-{word2}\"\n",
    "                print(f\"{pair_name:<20} {w2v_sim:<12.4f} {glove_sim:<12.4f} {diff:<10.4f}\")\n",
    "            except:\n",
    "                print(f\"{word1}-{word2:<15} ê³„ì‚° ì˜¤ë¥˜\")\n",
    "    else:\n",
    "        print(\"Word2Vecê³¼ GloVe ê°„ ê³µí†µ ì–´íœ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "compare_with_word2vec(w2v_model, glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092e608-4455-44bd-be1c-fb2874fb2952",
   "metadata": {},
   "source": [
    "### 7. FastText (Fast Text Representation)\n",
    "FastText ê°œë…:\n",
    "- Facebookì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•\n",
    "- Word2Vecì˜ ì§„í™”ëœ ë²„ì „\n",
    "- í•µì‹¬ ì°¨ì´ì : í•˜ìœ„ ë‹¨ì–´(subword) ì •ë³´ë¥¼ í™œìš©\n",
    "- ë‹¨ì–´ë¥¼ ë¬¸ì n-gramìœ¼ë¡œ ë¶„í•´í•´ì„œ í•™ìŠµ\n",
    "\n",
    "FastTextì˜ í˜ì‹ ì  ì•„ì´ë””ì–´:\n",
    "- Word2Vec: \"apple\" â†’ í•˜ë‚˜ì˜ ë²¡í„°\n",
    "- FastText: \"apple\" â†’ \"<ap\", \"app\", \"ppl\", \"ple\", \"le>\" + \"apple\" ì „ì²´\n",
    "- ì¥ì : í›ˆë ¨ì— ì—†ë˜ ë‹¨ì–´ë„ í•˜ìœ„ ë‹¨ì–´ ì¡°í•©ìœ¼ë¡œ ë²¡í„° ìƒì„± ê°€ëŠ¥!\n",
    "\n",
    "í•˜ìœ„ ë‹¨ì–´(Subword) ì˜ˆì‹œ:\n",
    "- \"running\" â†’ \"<ru\", \"run\", \"unn\", \"nni\", \"nin\", \"ing\", \"ng>\"\n",
    "- \"unknown\" (ìƒˆ ë‹¨ì–´) â†’ \"<un\", \"unk\", \"nkn\", \"kno\", \"now\", \"own\", \"wn>\"\n",
    "- ë¹„ìŠ·í•œ ì² ì íŒ¨í„´ â†’ ë¹„ìŠ·í•œ ì˜ë¯¸ ì¶”ë¡ \n",
    "\n",
    "<br>\n",
    "\n",
    "FastText vs Word2Vec ë¹„êµ:\n",
    "<br>\n",
    "\n",
    "Word2Vecì˜ í•œê³„:\n",
    "OOV (Out-of-Vocabulary) ë¬¸ì œ:\n",
    "- í›ˆë ¨ ë°ì´í„°ì— ì—†ëŠ” ë‹¨ì–´ â†’ ë²¡í„° ìƒì„± ë¶ˆê°€\n",
    "- ì˜ˆ: 'smartphone' í•™ìŠµí–ˆì§€ë§Œ 'smartphones' ì—†ìœ¼ë©´ ì²˜ë¦¬ ëª»í•¨\n",
    "- ì˜¤íƒ€, ì‹ ì¡°ì–´, ì „ë¬¸ìš©ì–´ ì²˜ë¦¬ ì–´ë ¤ì›€\n",
    "\n",
    "<br>\n",
    "\n",
    "FastTextì˜ í•´ê²°ì±…:\n",
    "í•˜ìœ„ ë‹¨ì–´ ê¸°ë°˜ í•™ìŠµ:\n",
    "- ë‹¨ì–´ë¥¼ ì‘ì€ ì¡°ê°ë“¤ë¡œ ë¶„í•´\n",
    "- ì¡°ê°ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ìƒˆë¡œìš´ ë‹¨ì–´ ë²¡í„° ìƒì„±\n",
    "- ì˜ˆ: 'run' + 'ning' íŒ¨í„´ìœ¼ë¡œ 'running' ì´í•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae3a67ab-4c76-46b0-83aa-f607e4027c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” í•˜ìœ„ ë‹¨ì–´ ë¶„í•´ ê³¼ì • ì‹œì—°:\n",
      "----------------------------------------\n",
      "ğŸ“ ë‹¨ì–´ë³„ í•˜ìœ„ ë‹¨ì–´ ë¶„í•´ (n-gram=3~6):\n",
      "\n",
      "  'natural' ë¶„í•´:\n",
      "    ë§ˆì»¤ ì¶”ê°€: <natural>\n",
      "    í•˜ìœ„ ë‹¨ì–´ë“¤: ['<na', 'nat', 'atu', 'tur', 'ura', 'ral', 'al>', '<nat', 'natu', 'atur']...\n",
      "    ì´ 23ê°œ ì¡°ê°\n",
      "\n",
      "  'language' ë¶„í•´:\n",
      "    ë§ˆì»¤ ì¶”ê°€: <language>\n",
      "    í•˜ìœ„ ë‹¨ì–´ë“¤: ['<la', 'lan', 'ang', 'ngu', 'gua', 'uag', 'age', 'ge>', '<lan', 'lang']...\n",
      "    ì´ 27ê°œ ì¡°ê°\n",
      "\n",
      "  'processing' ë¶„í•´:\n",
      "    ë§ˆì»¤ ì¶”ê°€: <processing>\n",
      "    í•˜ìœ„ ë‹¨ì–´ë“¤: ['<pr', 'pro', 'roc', 'oce', 'ces', 'ess', 'ssi', 'sin', 'ing', 'ng>']...\n",
      "    ì´ 35ê°œ ì¡°ê°\n",
      "\n",
      "  'fascinating' ë¶„í•´:\n",
      "    ë§ˆì»¤ ì¶”ê°€: <fascinating>\n",
      "    í•˜ìœ„ ë‹¨ì–´ë“¤: ['<fa', 'fas', 'asc', 'sci', 'cin', 'ina', 'nat', 'ati', 'tin', 'ing']...\n",
      "    ì´ 39ê°œ ì¡°ê°\n",
      "\n",
      "  'field' ë¶„í•´:\n",
      "    ë§ˆì»¤ ì¶”ê°€: <field>\n",
      "    í•˜ìœ„ ë‹¨ì–´ë“¤: ['<fi', 'fie', 'iel', 'eld', 'ld>', '<fie', 'fiel', 'ield', 'eld>', '<fiel']...\n",
      "    ì´ 15ê°œ ì¡°ê°\n",
      "\n",
      "  'artificial' ë¶„í•´:\n",
      "    ë§ˆì»¤ ì¶”ê°€: <artificial>\n",
      "    í•˜ìœ„ ë‹¨ì–´ë“¤: ['<ar', 'art', 'rti', 'tif', 'ifi', 'fic', 'ici', 'cia', 'ial', 'al>']...\n",
      "    ì´ 35ê°œ ì¡°ê°\n",
      "\n",
      "  'intelligence' ë¶„í•´:\n",
      "    ë§ˆì»¤ ì¶”ê°€: <intelligence>\n",
      "    í•˜ìœ„ ë‹¨ì–´ë“¤: ['<in', 'int', 'nte', 'tel', 'ell', 'lli', 'lig', 'ige', 'gen', 'enc']...\n",
      "    ì´ 43ê°œ ì¡°ê°\n"
     ]
    }
   ],
   "source": [
    "def show_subword_process(example_words):\n",
    "    \"\"\"í•˜ìœ„ ë‹¨ì–´ ë¶„í•´ ê³¼ì • ì‹œì—°\"\"\"\n",
    "    print(f\"\\nğŸ” í•˜ìœ„ ë‹¨ì–´ ë¶„í•´ ê³¼ì • ì‹œì—°:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ì˜ˆì‹œ ë‹¨ì–´ë“¤  \n",
    "    print(\"ğŸ“ ë‹¨ì–´ë³„ í•˜ìœ„ ë‹¨ì–´ ë¶„í•´ (n-gram=3~6):\")\n",
    "    \n",
    "    for word in example_words:\n",
    "        print(f\"\\n  '{word}' ë¶„í•´:\")\n",
    "        \n",
    "        # ì‹œì‘/ë ë§ˆì»¤ ì¶”ê°€\n",
    "        word_with_markers = f\"<{word}>\"\n",
    "        print(f\"    ë§ˆì»¤ ì¶”ê°€: {word_with_markers}\")\n",
    "        \n",
    "        # 3-gramë¶€í„° 6-gramê¹Œì§€ ìƒì„±\n",
    "        subwords = []\n",
    "        \n",
    "        for n in range(3, min(7, len(word_with_markers) + 1)):\n",
    "            for i in range(len(word_with_markers) - n + 1):\n",
    "                subword = word_with_markers[i:i+n]\n",
    "                subwords.append(subword)\n",
    "        \n",
    "        # ì „ì²´ ë‹¨ì–´ë„ í¬í•¨\n",
    "        subwords.append(word)\n",
    "        \n",
    "        print(f\"    í•˜ìœ„ ë‹¨ì–´ë“¤: {subwords[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ\n",
    "        print(f\"    ì´ {len(subwords)}ê°œ ì¡°ê°\")\n",
    "\n",
    "show_subword_process(preprocessed_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95433668-4767-4f72-bc88-e6f5657d9547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ FastText ëª¨ë¸ í•™ìŠµ ì‹œì‘!\n",
      "==================================================\n",
      "ğŸ”„ FastText ëª¨ë¸ í•™ìŠµ ì¤‘...\n",
      "â±ï¸  Word2Vecë³´ë‹¤ ì•½ê°„ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (í•˜ìœ„ ë‹¨ì–´ ì²˜ë¦¬ ë•Œë¬¸)\n",
      "âœ… FastText ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š FastText ëª¨ë¸ ì •ë³´:\n",
      "  ì–´íœ˜ í¬ê¸°: 48ê°œ\n",
      "  ë²¡í„° ì°¨ì›: 100ì°¨ì›\n",
      "  í•™ìŠµëœ ë‹¨ì–´ë“¤: ['nlp', 'text', 'models', 'language', 'learning', 'natural', 'many', 'process', 'revolutionized', 'tasks', 'deep', 'data', 'classification', 'amounts', 'large', 'analyze', 'algorithms', 'results', 'sentiment', 'machine', 'intelligence', 'artificial', 'field', 'fascinating', 'processing', 'analysis', 'understanding', 'preprocessing', 'essential', 'step', 'stateoftheart', 'achieve', 'gpt', 'bert', 'modern', 'architecture', 'dominant', 'become', 'transformers', 'words', 'relationships', 'semantic', 'capture', 'embeddings', 'word', 'pipeline', 'common', 'applications']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nğŸš€ FastText ëª¨ë¸ í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\"*50)\n",
    "print(\"ğŸ”„ FastText ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "print(\"â±ï¸  Word2Vecë³´ë‹¤ ì•½ê°„ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (í•˜ìœ„ ë‹¨ì–´ ì²˜ë¦¬ ë•Œë¬¸)\")\n",
    "\n",
    "fasttext_model = FastText(\n",
    "    sentences=preprocessed_texts,  # í•™ìŠµ ë°ì´í„° (í† í°í™”ëœ ë¬¸ì¥ë“¤)\n",
    "    vector_size=100,              # ì„ë² ë”© ë²¡í„° ì°¨ì› ìˆ˜\n",
    "    window=5,                     # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°\n",
    "    min_count=1,                  # ìµœì†Œ ì¶œí˜„ ë¹ˆë„ (ë°ì´í„°ê°€ ì ì–´ì„œ 1ë¡œ ì„¤ì •)\n",
    "    workers=4,                    # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜\n",
    "    sg=0,                        # 0=CBOW, 1=Skip-gram\n",
    "    epochs=10                    # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜\n",
    ")\n",
    "\n",
    "print(\"âœ… FastText ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "print(f\"\\nğŸ“Š FastText ëª¨ë¸ ì •ë³´:\")\n",
    "print(f\"  ì–´íœ˜ í¬ê¸°: {len(fasttext_model.wv)}ê°œ\")\n",
    "print(f\"  ë²¡í„° ì°¨ì›: {fasttext_model.wv.vector_size}ì°¨ì›\")\n",
    "print(f\"  í•™ìŠµëœ ë‹¨ì–´ë“¤: {list(fasttext_model.wv.index_to_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "844d3551-844a-46bb-9a31-63a60dcd8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ê¸°ë³¸ ê¸°ëŠ¥ í™•ì¸:\n",
      "----------------------------------------\n",
      "ğŸ“ 'nlp' ë²¡í„°:\n",
      "  ì „ì²´ ì°¨ì›: 100ì°¨ì›\n",
      "  ì²˜ìŒ 10ì°¨ì›: [ 0.00229694 -0.00158578  0.00267106  0.00239871 -0.0020173   0.00127126\n",
      "  0.00130897  0.00026072 -0.00232229 -0.00022632]\n",
      "  ë²¡í„° í¬ê¸°: 0.0253\n",
      "\n",
      "ğŸ“ ë‹¨ì–´ ìœ ì‚¬ë„:\n",
      "  'nlp' â†” 'text': -0.0781\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_basic_functionality(fasttext_model):\n",
    "    \"\"\"ê¸°ë³¸ ê¸°ëŠ¥ ì‹œì—°\"\"\"\n",
    "    print(f\"\\nğŸ” ê¸°ë³¸ ê¸°ëŠ¥ í™•ì¸:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # í•™ìŠµëœ ë‹¨ì–´ì˜ ë²¡í„° í™•ì¸\n",
    "    vocab = list(fasttext_model.wv.index_to_key)\n",
    "    \n",
    "    if vocab:\n",
    "        test_word = vocab[0]\n",
    "        vector = fasttext_model.wv[test_word]\n",
    "        print(f\"ğŸ“ '{test_word}' ë²¡í„°:\")\n",
    "        print(f\"  ì „ì²´ ì°¨ì›: {len(vector)}ì°¨ì›\")\n",
    "        print(f\"  ì²˜ìŒ 10ì°¨ì›: {vector[:10]}\")\n",
    "        print(f\"  ë²¡í„° í¬ê¸°: {np.linalg.norm(vector):.4f}\")\n",
    "    \n",
    "    # ë‹¨ì–´ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    if len(vocab) >= 2:\n",
    "        word1, word2 = vocab[0], vocab[1]\n",
    "        try:\n",
    "            similarity = fasttext_model.wv.similarity(word1, word2)\n",
    "            print(f\"\\nğŸ“ ë‹¨ì–´ ìœ ì‚¬ë„:\")\n",
    "            print(f\"  '{word1}' â†” '{word2}': {similarity:.4f}\")\n",
    "        except:\n",
    "            print(f\"  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨\")\n",
    "\n",
    "demonstrate_basic_functionality(fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d5a68fc-6c48-43c9-80cc-e565a48eb422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ FastTextì˜ í•µì‹¬ ê¸°ëŠ¥: OOV ë‹¨ì–´ ì²˜ë¦¬\n",
      "==================================================\n",
      "ğŸ” OOV (Out-of-Vocabulary) ë¬¸ì œë€?\n",
      "  - í›ˆë ¨ ë°ì´í„°ì— ì—†ë˜ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ë§Œë‚¬ì„ ë•Œ\n",
      "  - Word2Vec: 'ë‹¨ì–´ë¥¼ ëª¨ë¥´ê² ë‹¤' â†’ ì˜¤ë¥˜\n",
      "  - FastText: 'í•˜ìœ„ ë‹¨ì–´ë¡œ ì¶”ë¡ í•´ë³¼ê²Œ' â†’ ë²¡í„° ìƒì„±!\n",
      "\n",
      "ğŸ“š í›ˆë ¨ëœ ì–´íœ˜: {'models', 'learning', 'semantic', 'many', 'fascinating', 'architecture', 'natural', 'language', 'classification', 'modern', 'artificial', 'essential', 'results', 'intelligence', 'relationships', 'understanding', 'become', 'step', 'embeddings', 'word', 'field', 'words', 'dominant', 'preprocessing', 'bert', 'process', 'processing', 'analyze', 'achieve', 'applications', 'transformers', 'machine', 'gpt', 'common', 'sentiment', 'deep', 'pipeline', 'stateoftheart', 'algorithms', 'amounts', 'capture', 'text', 'revolutionized', 'nlp', 'analysis', 'tasks', 'data', 'large'}\n",
      "\n",
      "ğŸ§ª OOV ë‹¨ì–´ í…ŒìŠ¤íŠ¸:\n",
      "í›ˆë ¨ ë°ì´í„°ì— ì—†ëŠ” ë‹¨ì–´ë“¤ë„ ë²¡í„° ìƒì„±ì´ ê°€ëŠ¥í•œì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "âœ… 'artificialintelligence' (OOV ë‹¨ì–´):\n",
      "  ë²¡í„° ìƒì„± ì„±ê³µ! ì°¨ì›: 100\n",
      "  ì²˜ìŒ 5ì°¨ì›: [ 0.0001186   0.00144737  0.00098309 -0.00015574  0.00026451]\n",
      "  ë²¡í„° í¬ê¸°: 0.0056\n",
      "  ê°€ì¥ ìœ ì‚¬í•œ í›ˆë ¨ ë‹¨ì–´: 'semantic' (-0.1166)\n",
      "\n",
      "âœ… 'machinelearning' (OOV ë‹¨ì–´):\n",
      "  ë²¡í„° ìƒì„± ì„±ê³µ! ì°¨ì›: 100\n",
      "  ì²˜ìŒ 5ì°¨ì›: [ 5.9420051e-04  2.0748317e-05 -1.9436714e-04 -8.5463328e-04\n",
      " -1.5398222e-03]\n",
      "  ë²¡í„° í¬ê¸°: 0.0077\n",
      "  ê°€ì¥ ìœ ì‚¬í•œ í›ˆë ¨ ë‹¨ì–´: 'learning' (0.5638)\n",
      "\n",
      "âœ… 'deeplearning' (OOV ë‹¨ì–´):\n",
      "  ë²¡í„° ìƒì„± ì„±ê³µ! ì°¨ì›: 100\n",
      "  ì²˜ìŒ 5ì°¨ì›: [ 1.2643914e-03  4.4824515e-05 -1.5857724e-03  5.1677309e-04\n",
      "  1.7969652e-04]\n",
      "  ë²¡í„° í¬ê¸°: 0.0087\n",
      "  ê°€ì¥ ìœ ì‚¬í•œ í›ˆë ¨ ë‹¨ì–´: 'learning' (0.6751)\n",
      "\n",
      "âœ… 'naturallanguage' (OOV ë‹¨ì–´):\n",
      "  ë²¡í„° ìƒì„± ì„±ê³µ! ì°¨ì›: 100\n",
      "  ì²˜ìŒ 5ì°¨ì›: [-4.3414647e-04 -1.6823537e-03  1.3413278e-03 -9.1364775e-05\n",
      " -2.1338665e-04]\n",
      "  ë²¡í„° í¬ê¸°: 0.0083\n",
      "  ê°€ì¥ ìœ ì‚¬í•œ í›ˆë ¨ ë‹¨ì–´: 'semantic' (-0.0011)\n",
      "\n",
      "âœ… 'smartphone' (OOV ë‹¨ì–´):\n",
      "  ë²¡í„° ìƒì„± ì„±ê³µ! ì°¨ì›: 100\n",
      "  ì²˜ìŒ 5ì°¨ì›: [ 0.00082182 -0.00043519  0.0006153   0.00166517  0.00084064]\n",
      "  ë²¡í„° í¬ê¸°: 0.0103\n",
      "  ê°€ì¥ ìœ ì‚¬í•œ í›ˆë ¨ ë‹¨ì–´: 'learning' (-0.0228)\n",
      "\n",
      "âœ… 'neuralnetwork' (OOV ë‹¨ì–´):\n",
      "  ë²¡í„° ìƒì„± ì„±ê³µ! ì°¨ì›: 100\n",
      "  ì²˜ìŒ 5ì°¨ì›: [-7.3950487e-04 -3.9964975e-06 -4.3781038e-04 -2.6377998e-04\n",
      " -5.9242075e-04]\n",
      "  ë²¡í„° í¬ê¸°: 0.0079\n",
      "  ê°€ì¥ ìœ ì‚¬í•œ í›ˆë ¨ ë‹¨ì–´: 'models' (-0.0185)\n"
     ]
    }
   ],
   "source": [
    "# OOV ë‹¨ì–´ë“¤ í…ŒìŠ¤íŠ¸\n",
    "oov_test_words = [\n",
    "    \"artificialintelligence\",  # 'artificial' + 'intelligence' í•©ì„±\n",
    "    \"machinelearning\",         # 'machine' + 'learning' í•©ì„±  \n",
    "    \"deeplearning\",           # 'deep' + 'learning' í•©ì„±\n",
    "    \"naturallanguage\",        # 'natural' + 'language' í•©ì„±\n",
    "    \"smartphone\",             # ì¼ë°˜ì ì¸ ì‹ ì¡°ì–´\n",
    "    \"neuralnetwork\"           # 'neural' + 'network' í•©ì„±\n",
    "]\n",
    "\n",
    "def demonstrate_oov_handling(oov_test_words):\n",
    "    \"\"\"OOV (ë¯¸ë“±ë¡ ë‹¨ì–´) ì²˜ë¦¬ ì‹œì—° - FastTextì˜ í•µì‹¬ ê¸°ëŠ¥!\"\"\"\n",
    "    print(f\"\\nğŸ¯ FastTextì˜ í•µì‹¬ ê¸°ëŠ¥: OOV ë‹¨ì–´ ì²˜ë¦¬\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"ğŸ” OOV (Out-of-Vocabulary) ë¬¸ì œë€?\")\n",
    "    print(\"  - í›ˆë ¨ ë°ì´í„°ì— ì—†ë˜ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ë§Œë‚¬ì„ ë•Œ\")\n",
    "    print(\"  - Word2Vec: 'ë‹¨ì–´ë¥¼ ëª¨ë¥´ê² ë‹¤' â†’ ì˜¤ë¥˜\")\n",
    "    print(\"  - FastText: 'í•˜ìœ„ ë‹¨ì–´ë¡œ ì¶”ë¡ í•´ë³¼ê²Œ' â†’ ë²¡í„° ìƒì„±!\")\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„°ì— ìˆëŠ” ë‹¨ì–´ í™•ì¸\n",
    "    vocab = set(fasttext_model.wv.index_to_key)\n",
    "    print(f\"\\nğŸ“š í›ˆë ¨ëœ ì–´íœ˜: {vocab}\")\n",
    "    print(f\"\\nğŸ§ª OOV ë‹¨ì–´ í…ŒìŠ¤íŠ¸:\")\n",
    "    print(\"í›ˆë ¨ ë°ì´í„°ì— ì—†ëŠ” ë‹¨ì–´ë“¤ë„ ë²¡í„° ìƒì„±ì´ ê°€ëŠ¥í•œì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    for oov_word in oov_test_words:\n",
    "        try:\n",
    "            # FastTextëŠ” í›ˆë ¨ì— ì—†ë˜ ë‹¨ì–´ë„ ë²¡í„° ìƒì„± ê°€ëŠ¥!\n",
    "            oov_vector = fasttext_model.wv[oov_word]\n",
    "            \n",
    "            print(f\"\\nâœ… '{oov_word}' (OOV ë‹¨ì–´):\")\n",
    "            print(f\"  ë²¡í„° ìƒì„± ì„±ê³µ! ì°¨ì›: {len(oov_vector)}\")\n",
    "            print(f\"  ì²˜ìŒ 5ì°¨ì›: {oov_vector[:5]}\")\n",
    "            print(f\"  ë²¡í„° í¬ê¸°: {np.linalg.norm(oov_vector):.4f}\")\n",
    "            \n",
    "            # í›ˆë ¨ëœ ë‹¨ì–´ë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "            if vocab:\n",
    "                similarities = []\n",
    "                for known_word in list(vocab)[:3]:  # ì²˜ìŒ 3ê°œ ë‹¨ì–´ì™€ ë¹„êµ\n",
    "                    try:\n",
    "                        sim = fasttext_model.wv.similarity(oov_word, known_word)\n",
    "                        similarities.append((known_word, sim))\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if similarities:\n",
    "                    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "                    print(f\"  ê°€ì¥ ìœ ì‚¬í•œ í›ˆë ¨ ë‹¨ì–´: '{similarities[0][0]}' ({similarities[0][1]:.4f})\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ '{oov_word}': ë²¡í„° ìƒì„± ì‹¤íŒ¨ - {e}\")\n",
    "\n",
    "demonstrate_oov_handling(oov_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45aa1c6a-b1ec-43a1-8f2f-a5ee4b3c6638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸ Word2Vec vs FastText: OOV ì²˜ë¦¬ ë¹„êµ\n",
      "==================================================\n",
      "ğŸ§ª í…ŒìŠ¤íŠ¸ ë‹¨ì–´: 'artificialintelligence'\n",
      "(í›ˆë ¨ ë°ì´í„°ì— ì—†ëŠ” í•©ì„±ì–´)\n",
      "\n",
      "ğŸ“Š Word2Vec ê²°ê³¼:\n",
      "  âŒ KeyError: ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "  â†’ Word2Vecì€ í›ˆë ¨ì— ì—†ë˜ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ\n",
      "\n",
      "ğŸ“Š FastText ê²°ê³¼:\n",
      "  âœ… ë²¡í„° ìƒì„± ì„±ê³µ (ì²˜ìŒ 5ì°¨ì›): [ 0.0001186   0.00144737  0.00098309 -0.00015574  0.00026451]\n",
      "  â†’ FastTextëŠ” í•˜ìœ„ ë‹¨ì–´ ì¡°í•©ìœ¼ë¡œ ë²¡í„° ìƒì„±!\n",
      "  ğŸ” ì¶”ë¡  ê³¼ì •:\n",
      "    'artificial' ë¶€ë¶„ì˜ í•˜ìœ„ ë‹¨ì–´ë“¤: <ar, art, rti, tif, ...\n",
      "    'intelligence' ë¶€ë¶„ì˜ í•˜ìœ„ ë‹¨ì–´ë“¤: int, nte, tel, ell, ...\n",
      "    â†’ ì´ëŸ° ì¡°ê°ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì „ì²´ ë‹¨ì–´ ì˜ë¯¸ ì¶”ë¡ \n"
     ]
    }
   ],
   "source": [
    "test_oov_word = \"artificialintelligence\"\n",
    "\n",
    "def compare_with_word2vec(test_oov_word):\n",
    "    \"\"\"Word2Vecê³¼ FastText OOV ì²˜ë¦¬ ë¹„êµ\"\"\"\n",
    "    print(f\"\\nâš–ï¸ Word2Vec vs FastText: OOV ì²˜ë¦¬ ë¹„êµ\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"ğŸ§ª í…ŒìŠ¤íŠ¸ ë‹¨ì–´: '{test_oov_word}'\")\n",
    "    print(\"(í›ˆë ¨ ë°ì´í„°ì— ì—†ëŠ” í•©ì„±ì–´)\")\n",
    "    \n",
    "    # Word2Vec í…ŒìŠ¤íŠ¸\n",
    "    if 'w2v_model' in globals():\n",
    "        print(f\"\\nğŸ“Š Word2Vec ê²°ê³¼:\")\n",
    "        try:\n",
    "            w2v_vector = w2v_model.wv[test_oov_word]\n",
    "            print(f\"  âœ… ë²¡í„° ìƒì„± ì„±ê³µ (ì²˜ìŒ 5ì°¨ì›): {w2v_vector[:5]}\")\n",
    "        except KeyError:\n",
    "            print(f\"  âŒ KeyError: ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            print(f\"  â†’ Word2Vecì€ í›ˆë ¨ì— ì—†ë˜ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“Š Word2Vec ëª¨ë¸ì´ ì—†ì–´ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # FastText í…ŒìŠ¤íŠ¸\n",
    "    print(f\"\\nğŸ“Š FastText ê²°ê³¼:\")\n",
    "    try:\n",
    "        ft_vector = fasttext_model.wv[test_oov_word]\n",
    "        print(f\"  âœ… ë²¡í„° ìƒì„± ì„±ê³µ (ì²˜ìŒ 5ì°¨ì›): {ft_vector[:5]}\")\n",
    "        print(f\"  â†’ FastTextëŠ” í•˜ìœ„ ë‹¨ì–´ ì¡°í•©ìœ¼ë¡œ ë²¡í„° ìƒì„±!\")\n",
    "        \n",
    "        # ì–´ë–¤ í•˜ìœ„ ë‹¨ì–´ë“¤ì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ ì¶”ë¡ \n",
    "        print(f\"  ğŸ” ì¶”ë¡  ê³¼ì •:\")\n",
    "        print(f\"    'artificial' ë¶€ë¶„ì˜ í•˜ìœ„ ë‹¨ì–´ë“¤: <ar, art, rti, tif, ...\")\n",
    "        print(f\"    'intelligence' ë¶€ë¶„ì˜ í•˜ìœ„ ë‹¨ì–´ë“¤: int, nte, tel, ell, ...\")\n",
    "        print(f\"    â†’ ì´ëŸ° ì¡°ê°ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì „ì²´ ë‹¨ì–´ ì˜ë¯¸ ì¶”ë¡ \")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "compare_with_word2vec(test_oov_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4987a84-a464-487c-9cf1-409833fff1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ FastText ì‹¤ìŠµ ìš”ì•½:\n",
      "==================================================\n",
      "ğŸ“ˆ í•™ìŠµ ê²°ê³¼:\n",
      "  ì–´íœ˜ í¬ê¸°: 48ê°œ ë‹¨ì–´\n",
      "  ë²¡í„° ì°¨ì›: 100ì°¨ì›\n",
      "  í•µì‹¬ íŠ¹ì§•: í•˜ìœ„ ë‹¨ì–´ ê¸°ë°˜ OOV ì²˜ë¦¬\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nğŸ’¡ FastText ì‹¤ìŠµ ìš”ì•½:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"ğŸ“ˆ í•™ìŠµ ê²°ê³¼:\")\n",
    "print(f\"  ì–´íœ˜ í¬ê¸°: {len(fasttext_model.wv)}ê°œ ë‹¨ì–´\")\n",
    "print(f\"  ë²¡í„° ì°¨ì›: {fasttext_model.wv.vector_size}ì°¨ì›\")\n",
    "print(f\"  í•µì‹¬ íŠ¹ì§•: í•˜ìœ„ ë‹¨ì–´ ê¸°ë°˜ OOV ì²˜ë¦¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d165f-109a-439c-aeb3-265f8f6a8700",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fce562-1aa5-4a4e-b16f-4d47a5d49ca8",
   "metadata": {},
   "source": [
    "### ìµœê·¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë°©ë²•: Transformers (BERT ë“±)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c516b-660e-4d6e-b8d4-48306a366583",
   "metadata": {},
   "source": [
    "### 8. í˜„ëŒ€ì ì¸ ë°©ë²•: Transformers (BERT)\n",
    "\n",
    "Transformers & BERT ì†Œê°œ:\n",
    "- 2017ë…„ \"Attention Is All You Need\" ë…¼ë¬¸ìœ¼ë¡œ ì‹œì‘ëœ í˜ëª…\n",
    "- BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- ê¸°ì¡´ ë°©ë²•ë“¤ê³¼ì˜ í•µì‹¬ ì°¨ì´ì : ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´\n",
    "\n",
    "ê¸°ì¡´ ë°©ë²•ë“¤ê³¼ì˜ ë¹„êµ:\n",
    "- Word2Vec/GloVe/FastText: ë‹¨ì–´ë³„ ê³ ì • ë²¡í„°\n",
    "â†’ \"bank\" ë‹¨ì–´ëŠ” í•­ìƒ ê°™ì€ ë²¡í„° (ì€í–‰? ê°•ë‘‘?)\n",
    "  \n",
    "- BERT: ë¬¸ë§¥ì— ë”°ë¼ ë™ì  ë²¡í„°  \n",
    "â†’ \"I went to the bank\" vs \"river bank\"ì—ì„œ ë‹¤ë¥¸ ë²¡í„°!\n",
    "\n",
    "BERTì˜ í˜ì‹ ì  íŠ¹ì§•:\n",
    "- 1. ì–‘ë°©í–¥ì„±: ì•ë’¤ ë¬¸ë§¥ì„ ëª¨ë‘ ê³ ë ¤\n",
    "- 2. ë¬¸ë§¥ ì˜ì¡´ì„±: ê°™ì€ ë‹¨ì–´ë„ ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¥¸ ì˜ë¯¸\n",
    "- 3. ì „ì´ í•™ìŠµ: ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ ë¯¸ë¦¬ í›ˆë ¨ â†’ íŠ¹ì • íƒœìŠ¤í¬ì— ì ìš©\n",
    "- 4. ë¬¸ì¥ ë ˆë²¨ ì´í•´: ë‹¨ì–´ê°€ ì•„ë‹Œ ì „ì²´ ë¬¸ì¥ì˜ ì˜ë¯¸ íŒŒì•…\n",
    "\n",
    "\n",
    "#### ì™œ Transformersê°€ NLP ì—­ì‚¬ë¥¼ ë°”ê¿¨ì„ê¹Œ?\n",
    "ì„±ëŠ¥ì˜ ë¹„ì•½ì  í–¥ìƒ:\n",
    "- ê¸°ì¡´ ëª¨ë¸: íŠ¹ì • íƒœìŠ¤í¬ë³„ë¡œ ë”°ë¡œ ì„¤ê³„\n",
    "- BERT: í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì—¬ëŸ¬ íƒœìŠ¤í¬ í•´ê²°\n",
    "- ê²°ê³¼: ê±°ì˜ ëª¨ë“  NLP ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê¸°ë¡ ê²½ì‹ \n",
    "\n",
    "ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì–¸ì–´ ì´í•´:\n",
    "- ë‹¨ì–´ í•˜ë‚˜í•˜ë‚˜ê°€ ì•„ë‹ˆë¼ ë¬¸ì¥ ì „ì²´ ë§¥ë½ íŒŒì•…\n",
    "- 'ì´ ë§ì´ ì´ ìƒí™©ì—ì„œ ë¬´ìŠ¨ ëœ»ì¼ê¹Œ?' ì¶”ë¡  ê°€ëŠ¥\n",
    "- ì•”ë¬µì  ì˜ë¯¸, ë°˜ì–´ë²•, ë¬¸ë§¥ ì˜ì¡´ì  ì˜ë¯¸ë„ ì–´ëŠ ì •ë„ ì´í•´\n",
    "\n",
    "ì „ì´ í•™ìŠµì˜ ìœ„ë ¥:\n",
    "- 1ë‹¨ê³„: ìˆ˜ì‹­ì–µ ê°œ ë¬¸ì¥ìœ¼ë¡œ ì–¸ì–´ ìì²´ë¥¼ í•™ìŠµ\n",
    "- 2ë‹¨ê³„: íŠ¹ì • íƒœìŠ¤í¬ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •\n",
    "- ê²°ê³¼: ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±\n",
    "\n",
    "#### BERT êµ¬ì¡° ê°œìš”\n",
    "í•µì‹¬ êµ¬ì„± ìš”ì†Œ:\n",
    "- 1. í† í¬ë‚˜ì´ì € (Tokenizer):\n",
    "     - ë¬¸ì¥ì„ BERTê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í† í°ìœ¼ë¡œ ë³€í™˜\n",
    "     - ì˜ˆ: 'Hello world' â†’ ['[CLS]', 'Hello', 'world', '[SEP]']\n",
    "- 2. ì„ë² ë”© ë ˆì´ì–´:\n",
    "     - í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜ (ë‹¨ì–´ + ìœ„ì¹˜ + ë¬¸ì¥ ì •ë³´)\n",
    "- 3. íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ë“¤:\n",
    "     - ì—¬ëŸ¬ ì¸µì˜ attention ë©”ì»¤ë‹ˆì¦˜\n",
    "     - ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ë“¤ê³¼ 'ëŒ€í™”'í•˜ë©° ì˜ë¯¸ íŒŒì•…\n",
    "- 4. ì¶œë ¥:\n",
    "     - [CLS]: ì „ì²´ ë¬¸ì¥ì˜ ì˜ë¯¸ (ë¶„ë¥˜ íƒœìŠ¤í¬ìš©)\n",
    "     - ê° í† í°ë³„: ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë‹¨ì–´ë³„ ë²¡í„°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "411d4229-a6cc-4c41-ba1e-ba863ade096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6cd13-a99a-4497-97ef-db3b1448a44b",
   "metadata": {},
   "source": [
    "ëª¨ë¸ ì„ íƒ: DistilBERT\n",
    "\n",
    "BERT ëª¨ë¸ í¬ê¸° ë¹„êµ:\n",
    "- BERT-base: 110M íŒŒë¼ë¯¸í„°, 768ì°¨ì›, 12ì¸µ\n",
    "- BERT-large: 340M íŒŒë¼ë¯¸í„°, 1024ì°¨ì›, 24ì¸µ\n",
    "- DistilBERT: 66M íŒŒë¼ë¯¸í„°, 768ì°¨ì›, 6ì¸µ â† ìš°ë¦¬ê°€ ì‚¬ìš©\n",
    "\n",
    "- DistilBERT ì„ íƒ ì´ìœ :\n",
    "  - í¬ê¸°: BERT-baseì˜ 60% í¬ê¸°\n",
    "  - ì†ë„: ì•½ 60% ë¹ ë¦„\n",
    "  - ì„±ëŠ¥: BERT-baseì˜ 97% ì„±ëŠ¥ ìœ ì§€\n",
    "  - ë©”ëª¨ë¦¬: ì¼ë°˜ PCì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥\n",
    "<br>\n",
    "  â†’ í•™ìŠµìš©ìœ¼ë¡œ ìµœì !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "651cb4fa-93ad-475b-b2e3-fc8a113b59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ BERT ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "â±ï¸  ì²˜ìŒ ì‹¤í–‰ ì‹œ ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œ (ì‹œê°„ ì†Œìš”)\n",
      "ğŸ“¥ ì‚¬ìš© ëª¨ë¸: distilbert-base-uncased\n",
      "  - distilbert: ê²½ëŸ‰í™”ëœ BERT\n",
      "  - base: ê¸°ë³¸ í¬ê¸° (large ëŒ€ë¹„)\n",
      "  - uncased: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨\n",
      "\n",
      "1ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\n",
      "âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\n",
      "\n",
      "2ï¸âƒ£ BERT ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "âœ… BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š ëª¨ë¸ ì •ë³´:\n",
      "  ëª¨ë¸ ì´ë¦„: distilbert-base-uncased\n",
      "  íŒŒë¼ë¯¸í„° ìˆ˜: ì•½ 66Mê°œ\n",
      "  ì¶œë ¥ ì°¨ì›: 768ì°¨ì›\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ”„ BERT ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "print(\"â±ï¸  ì²˜ìŒ ì‹¤í–‰ ì‹œ ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œ (ì‹œê°„ ì†Œìš”)\")\n",
    "\n",
    "# ëª¨ë¸ ì´ë¦„ ì§€ì •\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "print(f\"ğŸ“¥ ì‚¬ìš© ëª¨ë¸: {model_name}\")\n",
    "print(\"  - distilbert: ê²½ëŸ‰í™”ëœ BERT\")\n",
    "print(\"  - base: ê¸°ë³¸ í¬ê¸° (large ëŒ€ë¹„)\")\n",
    "print(\"  - uncased: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"\\n1ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "print(\"\\n2ï¸âƒ£ BERT ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(\"âœ… BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ ì •ë³´:\")\n",
    "print(f\"  ëª¨ë¸ ì´ë¦„: {model_name}\")\n",
    "print(f\"  íŒŒë¼ë¯¸í„° ìˆ˜: ì•½ 66Mê°œ\")\n",
    "print(f\"  ì¶œë ¥ ì°¨ì›: 768ì°¨ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a167c9cb-04c5-4950-bd50-63e5b8afbf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” í† í°í™” ê³¼ì • ì‹œì—°\n",
      "------------------------------\n",
      "ì›ë³¸ ë¬¸ì¥: 'Natural language processing is amazing!'\n",
      "í† í°í™” ê²°ê³¼: ['natural', 'language', 'processing', 'is', 'amazing', '!']\n",
      "í† í° ID: [3019, 2653, 6364, 2003, 6429, 999]\n",
      "íŠ¹ìˆ˜ í† í° í¬í•¨: tensor([[ 101, 3019, 2653, 6364, 2003, 6429,  999,  102]])\n",
      "ë””ì½”ë”© ê²°ê³¼: '[CLS] natural language processing is amazing! [SEP]'\n",
      "\n",
      "ğŸ’¡ íŠ¹ìˆ˜ í† í° ì„¤ëª…:\n",
      "  [CLS]: ë¬¸ì¥ ì‹œì‘ (Classification í† í°)\n",
      "  [SEP]: ë¬¸ì¥ ë (Separator í† í°)\n",
      "  [PAD]: ê¸¸ì´ ë§ì¶¤ìš© íŒ¨ë”©\n",
      "  [UNK]: ëª¨ë¥´ëŠ” ë‹¨ì–´\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Natural language processing is amazing!\"\n",
    "\n",
    "def demonstrate_tokenization(example_text):\n",
    "    \"\"\"í† í°í™” ê³¼ì • ì‹œì—°\"\"\"\n",
    "    print(\"\\nğŸ” í† í°í™” ê³¼ì • ì‹œì—°\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(f\"ì›ë³¸ ë¬¸ì¥: '{example_text}'\")\n",
    "    \n",
    "    # í† í°í™” ìˆ˜í–‰\n",
    "    tokens = tokenizer.tokenize(example_text)\n",
    "    print(f\"í† í°í™” ê²°ê³¼: {tokens}\")\n",
    "    \n",
    "    # í† í° IDë¡œ ë³€í™˜\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"í† í° ID: {token_ids}\")\n",
    "    \n",
    "    # íŠ¹ìˆ˜ í† í° ì¶”ê°€ëœ ë²„ì „\n",
    "    encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "    print(f\"íŠ¹ìˆ˜ í† í° í¬í•¨: {encoded['input_ids']}\")\n",
    "    \n",
    "    # ë””ì½”ë”© (ë³µì›)\n",
    "    decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "    print(f\"ë””ì½”ë”© ê²°ê³¼: '{decoded}'\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ íŠ¹ìˆ˜ í† í° ì„¤ëª…:\")\n",
    "    print(\"  [CLS]: ë¬¸ì¥ ì‹œì‘ (Classification í† í°)\")\n",
    "    print(\"  [SEP]: ë¬¸ì¥ ë (Separator í† í°)\")\n",
    "    print(\"  [PAD]: ê¸¸ì´ ë§ì¶¤ìš© íŒ¨ë”©\")\n",
    "    print(\"  [UNK]: ëª¨ë¥´ëŠ” ë‹¨ì–´\")\n",
    "\n",
    "demonstrate_tokenization(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceeec23b-d977-4f15-ae27-12b45791882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬í•  ë¬¸ì¥ë“¤:\n",
      "  1. Natural language processing is a fascinating field of artificial intelligence.\n",
      "  2. Machine learning algorithms can process and analyze large amounts of text data.\n",
      "  3. Deep learning models have revolutionized natural language understanding.\n"
     ]
    }
   ],
   "source": [
    "test_sentences = sample_texts[:3]\n",
    "print(\"ì²˜ë¦¬í•  ë¬¸ì¥ë“¤:\")\n",
    "for i, sent in enumerate(test_sentences, 1):\n",
    "    print(f\"  {i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11fc3180-018b-46be-aeba-b19966d9b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ ì‹¤ì œ BERT ì„ë² ë”© ì¶”ì¶œ ì‹œì‘\n",
      "==================================================\n",
      "\n",
      "ğŸ¯ BERT ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜ ë™ì‘:\n",
      "  1ë‹¨ê³„: ê° ë¬¸ì¥ì„ í† í°í™”\n",
      "  2ë‹¨ê³„: í† í°ë“¤ì„ BERT ëª¨ë¸ì— ì…ë ¥\n",
      "  3ë‹¨ê³„: [CLS] í† í°ì˜ ì¶œë ¥ì„ ë¬¸ì¥ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©\n",
      "\n",
      "ğŸ“ 3ê°œ ë¬¸ì¥ ì²˜ë¦¬ ì¤‘...\n",
      "  ì²˜ë¦¬ ì¤‘: ë¬¸ì¥ 1/3\n",
      "    ì›ë³¸: 'Natural language processing is a fascinating field of artificial intelligence.'\n",
      "    í† í° ìˆ˜: 13ê°œ\n",
      "    ì„ë² ë”© ì°¨ì›: torch.Size([768])\n",
      "  ì²˜ë¦¬ ì¤‘: ë¬¸ì¥ 2/3\n",
      "    ì›ë³¸: 'Machine learning algorithms can process and analyze large amounts of text data.'\n",
      "    í† í° ìˆ˜: 15ê°œ\n",
      "    ì„ë² ë”© ì°¨ì›: torch.Size([768])\n",
      "  ì²˜ë¦¬ ì¤‘: ë¬¸ì¥ 3/3\n",
      "    ì›ë³¸: 'Deep learning models have revolutionized natural language understanding.'\n",
      "    í† í° ìˆ˜: 12ê°œ\n",
      "    ì„ë² ë”© ì°¨ì›: torch.Size([768])\n",
      "\n",
      "ğŸ“Š BERT ì„ë² ë”© ê²°ê³¼:\n",
      "  ì„ë² ë”© ë°°ì—´ í¬ê¸°: (3, 768)\n",
      "  ë¬¸ì¥ ìˆ˜: 3ê°œ\n",
      "  ê° ì„ë² ë”© ì°¨ì›: 768ì°¨ì›\n",
      "\n",
      "ğŸ” ì²« ë²ˆì§¸ ë¬¸ì¥ ì„ë² ë”© ìƒì„¸:\n",
      "  ì „ì²´ ì°¨ì›: 768\n",
      "  ì²˜ìŒ 10ì°¨ì›: [-0.2655637  -0.12168896 -0.39667323 -0.0114592  -0.04621242 -0.12133733\n",
      "  0.127688    0.3928875  -0.1328064  -0.50623184]\n",
      "  ë²¡í„° í¬ê¸°: 13.2355\n",
      "  ìµœëŒ€ê°’: 3.2072\n",
      "  ìµœì†Œê°’: -7.4128\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(texts, tokenizer, model):\n",
    "    \"\"\"\n",
    "    BERT ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        texts: ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        tokenizer: BERT í† í¬ë‚˜ì´ì €\n",
    "        model: BERT ëª¨ë¸\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: ê° ë¬¸ì¥ì˜ ì„ë² ë”© ë²¡í„°ë“¤\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ¯ BERT ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜ ë™ì‘:\")\n",
    "    print(\"  1ë‹¨ê³„: ê° ë¬¸ì¥ì„ í† í°í™”\")\n",
    "    print(\"  2ë‹¨ê³„: í† í°ë“¤ì„ BERT ëª¨ë¸ì— ì…ë ¥\")\n",
    "    print(\"  3ë‹¨ê³„: [CLS] í† í°ì˜ ì¶œë ¥ì„ ë¬¸ì¥ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©\")\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # gradient ê³„ì‚° ë¹„í™œì„±í™” (ì¶”ë¡  ëª¨ë“œ)\n",
    "    with torch.no_grad():\n",
    "        print(f\"\\nğŸ“ {len(texts)}ê°œ ë¬¸ì¥ ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            print(f\"  ì²˜ë¦¬ ì¤‘: ë¬¸ì¥ {i+1}/{len(texts)}\")\n",
    "            print(f\"    ì›ë³¸: '{text}'\")\n",
    "            \n",
    "            # 1ë‹¨ê³„: í† í°í™” ë° í…ì„œ ë³€í™˜\n",
    "            inputs = tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\",     # PyTorch í…ì„œë¡œ ë°˜í™˜\n",
    "                padding=True,            # ê¸¸ì´ ë§ì¶¤\n",
    "                truncation=True,         # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ì‹œ ìë¦„\n",
    "                max_length=512          # BERT ìµœëŒ€ ì…ë ¥ ê¸¸ì´\n",
    "            )\n",
    "            \n",
    "            print(f\"    í† í° ìˆ˜: {inputs['input_ids'].shape[1]}ê°œ\")\n",
    "            \n",
    "            # 2ë‹¨ê³„: BERT ëª¨ë¸ ì‹¤í–‰\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # 3ë‹¨ê³„: [CLS] í† í°ì˜ ì„ë² ë”© ì¶”ì¶œ\n",
    "            # last_hidden_state: [ë°°ì¹˜í¬ê¸°, ì‹œí€€ìŠ¤ê¸¸ì´, ì°¨ì›ìˆ˜]\n",
    "            # [0]: ì²« ë²ˆì§¸ (ìœ ì¼í•œ) ë¬¸ì¥\n",
    "            # [0]: ì²« ë²ˆì§¸ í† í° ([CLS])\n",
    "            cls_embedding = outputs.last_hidden_state[0][0]\n",
    "            \n",
    "            print(f\"    ì„ë² ë”© ì°¨ì›: {cls_embedding.shape}\")\n",
    "            \n",
    "            # numpyë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥\n",
    "            embeddings.append(cls_embedding.numpy())\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# ì‹¤ì œ ì„ë² ë”© ì¶”ì¶œ (ì²« 3ê°œ ë¬¸ì¥ë§Œ - ì‹œê°„ ì ˆì•½)\n",
    "print(f\"\\nğŸš€ ì‹¤ì œ BERT ì„ë² ë”© ì¶”ì¶œ ì‹œì‘\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "bert_embeddings = get_bert_embeddings(test_sentences, tokenizer, model)\n",
    "\n",
    "print(f\"\\nğŸ“Š BERT ì„ë² ë”© ê²°ê³¼:\")\n",
    "print(f\"  ì„ë² ë”© ë°°ì—´ í¬ê¸°: {bert_embeddings.shape}\")\n",
    "print(f\"  ë¬¸ì¥ ìˆ˜: {bert_embeddings.shape[0]}ê°œ\")\n",
    "print(f\"  ê° ì„ë² ë”© ì°¨ì›: {bert_embeddings.shape[1]}ì°¨ì›\")\n",
    "\n",
    "print(f\"\\nğŸ” ì²« ë²ˆì§¸ ë¬¸ì¥ ì„ë² ë”© ìƒì„¸:\")\n",
    "first_embedding = bert_embeddings[0]\n",
    "print(f\"  ì „ì²´ ì°¨ì›: {len(first_embedding)}\")\n",
    "print(f\"  ì²˜ìŒ 10ì°¨ì›: {first_embedding[:10]}\")\n",
    "print(f\"  ë²¡í„° í¬ê¸°: {np.linalg.norm(first_embedding):.4f}\")\n",
    "print(f\"  ìµœëŒ€ê°’: {np.max(first_embedding):.4f}\")\n",
    "print(f\"  ìµœì†Œê°’: {np.min(first_embedding):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c082371-3858-41c1-a53e-7350b5dacb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ BERT ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„\n",
      "----------------------------------------\n",
      "ğŸ’¡ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í•´ì„:\n",
      "  1.0: ì™„ì „íˆ ë™ì¼í•œ ë°©í–¥ (ë§¤ìš° ìœ ì‚¬)\n",
      "  0.0: ì§êµ (ê´€ë ¨ ì—†ìŒ)\n",
      "  -1.0: ì™„ì „íˆ ë°˜ëŒ€ ë°©í–¥ (ì •ë°˜ëŒ€)\n",
      "\n",
      "ğŸ“Š 3ê°œ ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\n",
      "     ë¬¸ì¥ 1  ë¬¸ì¥ 2  ë¬¸ì¥ 3  \n",
      "ë¬¸ì¥ 1  1.000   0.951   0.958  \n",
      "ë¬¸ì¥ 2  0.951   1.000   0.951  \n",
      "ë¬¸ì¥ 3  0.958   0.951   1.000  \n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        a, b: ë‘ ë²¡í„°\n",
    "        \n",
    "    Returns:\n",
    "        float: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (-1 ~ 1)\n",
    "    \"\"\"\n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = AÂ·B / (|A| Ã— |B|)\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "# ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ë¶„ì„\n",
    "print(f\"\\nğŸ“ BERT ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"ğŸ’¡ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í•´ì„:\")\n",
    "print(\"  1.0: ì™„ì „íˆ ë™ì¼í•œ ë°©í–¥ (ë§¤ìš° ìœ ì‚¬)\")\n",
    "print(\"  0.0: ì§êµ (ê´€ë ¨ ì—†ìŒ)\")\n",
    "print(\"  -1.0: ì™„ì „íˆ ë°˜ëŒ€ ë°©í–¥ (ì •ë°˜ëŒ€)\")\n",
    "\n",
    "# ëª¨ë“  ë¬¸ì¥ ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "n_sentences = len(bert_embeddings)\n",
    "\n",
    "print(f\"\\nğŸ“Š {n_sentences}ê°œ ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\")\n",
    "print(\"     \", end=\"\")\n",
    "for j in range(n_sentences):\n",
    "    print(f\"ë¬¸ì¥{j+1:2d}\", end=\"  \")\n",
    "print()\n",
    "\n",
    "for i in range(n_sentences):\n",
    "    print(f\"ë¬¸ì¥{i+1:2d}\", end=\" \")\n",
    "    for j in range(n_sentences):\n",
    "        if i == j:\n",
    "            sim = 1.0  # ìê¸° ìì‹ ê³¼ëŠ” ì™„ì „ ìœ ì‚¬\n",
    "        else:\n",
    "            sim = cosine_similarity(bert_embeddings[i], bert_embeddings[j])\n",
    "        print(f\"{sim:6.3f}\", end=\"  \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3766253a-c807-46e6-a8ac-dbed40d0c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ† ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ ìŒ:\n",
      "  ë¬¸ì¥ 1: 'Natural language processing is a fascinating field of artificial intelligence.'\n",
      "  ë¬¸ì¥ 3: 'Deep learning models have revolutionized natural language understanding.'\n",
      "  ìœ ì‚¬ë„: 0.9581\n",
      "\n",
      "ğŸ”» ê°€ì¥ ë‹¤ë¥¸ ë¬¸ì¥ ìŒ:\n",
      "  ë¬¸ì¥ 2: 'Machine learning algorithms can process and analyze large amounts of text data.'\n",
      "  ë¬¸ì¥ 3: 'Deep learning models have revolutionized natural language understanding.'\n",
      "  ìœ ì‚¬ë„: 0.9507\n"
     ]
    }
   ],
   "source": [
    "# ê°€ì¥ ìœ ì‚¬í•œ/ë‹¤ë¥¸ ë¬¸ì¥ ìŒ ì°¾ê¸°\n",
    "max_sim = -2.0\n",
    "min_sim = 2.0\n",
    "max_pair = None\n",
    "min_pair = None\n",
    "\n",
    "for i in range(n_sentences):\n",
    "    for j in range(i+1, n_sentences):\n",
    "        sim = cosine_similarity(bert_embeddings[i], bert_embeddings[j])\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            max_pair = (i, j)\n",
    "        if sim < min_sim:\n",
    "            min_sim = sim\n",
    "            min_pair = (i, j)\n",
    "\n",
    "if max_pair:\n",
    "    print(f\"\\nğŸ† ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ ìŒ:\")\n",
    "    print(f\"  ë¬¸ì¥ {max_pair[0]+1}: '{test_sentences[max_pair[0]]}'\")\n",
    "    print(f\"  ë¬¸ì¥ {max_pair[1]+1}: '{test_sentences[max_pair[1]]}'\")\n",
    "    print(f\"  ìœ ì‚¬ë„: {max_sim:.4f}\")\n",
    "\n",
    "if min_pair:\n",
    "    print(f\"\\nğŸ”» ê°€ì¥ ë‹¤ë¥¸ ë¬¸ì¥ ìŒ:\")\n",
    "    print(f\"  ë¬¸ì¥ {min_pair[0]+1}: '{test_sentences[min_pair[0]]}'\")\n",
    "    print(f\"  ë¬¸ì¥ {min_pair[1]+1}: '{test_sentences[min_pair[1]]}'\")\n",
    "    print(f\"  ìœ ì‚¬ë„: {min_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9f753-7fc3-4145-8de3-046013e80499",
   "metadata": {},
   "source": [
    "#### BERT vs ì „í†µì  ë°©ë²•ë“¤ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "346d8c7c-5f86-4751-a16b-1ac68c3d884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì„ë² ë”© ì°¨ì› ë¹„êµ:\n",
      "  TF-IDF: 48ì°¨ì› (í¬ì†Œ)\n",
      "  Word2Vec: 100ì°¨ì› (ë°€ì§‘)\n",
      "  FastText: 100ì°¨ì› (ë°€ì§‘)\n",
      "  BERT: 768ì°¨ì› (ë°€ì§‘, ë¬¸ë§¥ì )\n",
      "\n",
      "ğŸ¯ ê° ë°©ë²•ì˜ íŠ¹ì§•:\n",
      "  TF-IDF:\n",
      "    âœ… ë¹ ë¦„, í•´ì„ ìš©ì´\n",
      "    âŒ ë‹¨ì–´ ìˆœì„œ ë¬´ì‹œ, ì˜ë¯¸ ê´€ê³„ ë¶€ì¡±\n",
      "  Word2Vec/FastText:\n",
      "    âœ… ì˜ë¯¸ì  ìœ ì‚¬ì„±, íš¨ìœ¨ì \n",
      "    âŒ ë¬¸ë§¥ ì˜ì¡´ì„± ë¶€ì¡±\n",
      "  BERT:\n",
      "    âœ… ë¬¸ë§¥ ì´í•´, ì–‘ë°©í–¥ì„±, ë†’ì€ ì„±ëŠ¥\n",
      "    âŒ ê³„ì‚° ë¹„ìš© ë†’ìŒ, ë³µì¡í•¨\n",
      "\n",
      "ğŸ’¼ ì‹¤ì œ ì‚¬ìš© ê°€ì´ë“œ:\n",
      "  ğŸš€ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…: TF-IDF\n",
      "  ğŸ¯ ì¼ë°˜ì  NLP: Word2Vec/FastText\n",
      "  ğŸ† ìµœê³  ì„±ëŠ¥ í•„ìš”: BERT/Transformers\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š ì„ë² ë”© ì°¨ì› ë¹„êµ:\")\n",
    "print(f\"  TF-IDF: {len(tfidf_vectorizer.vocabulary_)}ì°¨ì› (í¬ì†Œ)\")\n",
    "if 'w2v_model' in globals():\n",
    "    print(f\"  Word2Vec: {w2v_model.wv.vector_size}ì°¨ì› (ë°€ì§‘)\")\n",
    "if 'fasttext_model' in globals():\n",
    "    print(f\"  FastText: {fasttext_model.wv.vector_size}ì°¨ì› (ë°€ì§‘)\")\n",
    "print(f\"  BERT: {bert_embeddings.shape[1]}ì°¨ì› (ë°€ì§‘, ë¬¸ë§¥ì )\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ê° ë°©ë²•ì˜ íŠ¹ì§•:\")\n",
    "print(\"  TF-IDF:\")\n",
    "print(\"    âœ… ë¹ ë¦„, í•´ì„ ìš©ì´\")\n",
    "print(\"    âŒ ë‹¨ì–´ ìˆœì„œ ë¬´ì‹œ, ì˜ë¯¸ ê´€ê³„ ë¶€ì¡±\")\n",
    "\n",
    "print(\"  Word2Vec/FastText:\")\n",
    "print(\"    âœ… ì˜ë¯¸ì  ìœ ì‚¬ì„±, íš¨ìœ¨ì \")\n",
    "print(\"    âŒ ë¬¸ë§¥ ì˜ì¡´ì„± ë¶€ì¡±\")\n",
    "\n",
    "print(\"  BERT:\")\n",
    "print(\"    âœ… ë¬¸ë§¥ ì´í•´, ì–‘ë°©í–¥ì„±, ë†’ì€ ì„±ëŠ¥\")\n",
    "print(\"    âŒ ê³„ì‚° ë¹„ìš© ë†’ìŒ, ë³µì¡í•¨\")\n",
    "\n",
    "print(f\"\\nğŸ’¼ ì‹¤ì œ ì‚¬ìš© ê°€ì´ë“œ:\")\n",
    "print(\"  ğŸš€ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…: TF-IDF\")\n",
    "print(\"  ğŸ¯ ì¼ë°˜ì  NLP: Word2Vec/FastText\")\n",
    "print(\"  ğŸ† ìµœê³  ì„±ëŠ¥ í•„ìš”: BERT/Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f76137-4b07-4a54-bf21-7a7515eed3f7",
   "metadata": {},
   "source": [
    "#### Transformers ì‹¤ìŠµ ìš”ì•½\n",
    "\n",
    "âœ… BERT ì‹¤ìŠµ ì„±ê³µ!\n",
    "- ì²˜ë¦¬ëœ ë¬¸ì¥ ìˆ˜: 3ê°œ\n",
    "- ì„ë² ë”© ì°¨ì›: 768ì°¨ì›\n",
    "- í•µì‹¬ íŠ¹ì§•: ë¬¸ë§¥ ì˜ì¡´ì  ì„ë² ë”©\n",
    "\n",
    "ğŸ¯ Transformersì˜ í˜ì‹ :\n",
    "  1. ë¬¸ë§¥ ì´í•´ â†’ ê°™ì€ ë‹¨ì–´ë„ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ ì˜ë¯¸\n",
    "  2. ì–‘ë°©í–¥ì„± â†’ ì•ë’¤ ë¬¸ë§¥ ëª¨ë‘ ê³ ë ¤\n",
    "  3. ì „ì´ í•™ìŠµ â†’ ëŒ€ìš©ëŸ‰ ì‚¬ì „ í›ˆë ¨ + íƒœìŠ¤í¬ë³„ ë¯¸ì„¸ ì¡°ì •\n",
    "  4. ë²”ìš©ì„± â†’ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì—¬ëŸ¬ NLP íƒœìŠ¤í¬ í•´ê²°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de907d-d6f3-4301-882e-813d117e55f2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f613c3-dc9f-4cac-804d-5eca8c3d2361",
   "metadata": {},
   "source": [
    "### **ë°©ë²•ë³„ íŠ¹ì§• ìš”ì•½**\n",
    "<br>\n",
    "BoW:<br>\n",
    "- ì¥ì : ê°„ë‹¨, ë¹ ë¦„, í•´ì„ ê°€ëŠ¥<br>\n",
    "- ë‹¨ì : í¬ì†Œ ë²¡í„°, ë‹¨ì–´ ìˆœì„œ ë¬´ì‹œ, ì˜ë¯¸ ì •ë³´ ë¶€ì¡±<br>\n",
    "- ì‚¬ìš©ì²˜: ë¬¸ì„œ ë¶„ë¥˜, í…ìŠ¤íŠ¸ ê²€ìƒ‰<br>\n",
    "<br>\n",
    "N-gram:<br>\n",
    "- ì¥ì : ì§€ì—­ì  ë‹¨ì–´ ìˆœì„œ ê³ ë ¤, BoW í™•ì¥<br>\n",
    "- ë‹¨ì : ì°¨ì› í­ë°œ, í¬ì†Œì„± ì¦ê°€<br>\n",
    "- ì‚¬ìš©ì²˜: ì–¸ì–´ ëª¨ë¸ë§, êµ¬ë¬¸ íŒ¨í„´ ë¶„ì„<br>\n",
    "<br>\n",
    "TF-IDF:<br>\n",
    "- ì¥ì : ë‹¨ì–´ ì¤‘ìš”ë„ ë°˜ì˜, ì¼ë°˜ì ì¸ ë‹¨ì–´ ì–µì œ<br>\n",
    "- ë‹¨ì : ì—¬ì „íˆ í¬ì†Œ ë²¡í„°, ì˜ë¯¸ ì •ë³´ ì œí•œ<br>\n",
    "- ì‚¬ìš©ì²˜: ì •ë³´ ê²€ìƒ‰, ë¬¸ì„œ ìœ ì‚¬ë„<br>\n",
    "<br>\n",
    "Word2Vec:<br>\n",
    "- ì¥ì : ë°€ì§‘ ë²¡í„°, ì˜ë¯¸ì  ìœ ì‚¬ë„ í¬ì°©<br>\n",
    "- ë‹¨ì : OOV ë¬¸ì œ, ì •ì  ì„ë² ë”©<br>\n",
    "- ì‚¬ìš©ì²˜: ë‹¨ì–´ ìœ ì‚¬ë„, ì˜ë¯¸ì  ì¶”ë¡ <br>\n",
    "<br>\n",
    "GloVe:<br>\n",
    "- ì¥ì : ì „ì—­ í†µê³„ ì •ë³´ í™œìš©, ì¢‹ì€ ì„±ëŠ¥<br>\n",
    "- ë‹¨ì : OOV ë¬¸ì œ, ì •ì  ì„ë² ë”©<br>\n",
    "- ì‚¬ìš©ì²˜: ë‹¨ì–´ ì„ë² ë”©, ì „ì´ í•™ìŠµ<br>\n",
    "<br>\n",
    "FastText:<br>\n",
    "- ì¥ì : OOV ì²˜ë¦¬ ê°€ëŠ¥, í•˜ìœ„ ë‹¨ì–´ ì •ë³´ í™œìš©<br>\n",
    "- ë‹¨ì : ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ<br>\n",
    "- ì‚¬ìš©ì²˜: í˜•íƒœí•™ì´ ë³µì¡í•œ ì–¸ì–´, OOVê°€ ë§ì€ ìƒí™©<br>\n",
    "<br>\n",
    "Transformers:<br>\n",
    "- ì¥ì : ë¬¸ë§¥ì  ì„ë² ë”©, ìµœê³  ì„±ëŠ¥, ë‹¤ì–‘í•œ ì‘ì—… ê°€ëŠ¥<br>\n",
    "- ë‹¨ì : ê³„ì‚° ë¹„ìš© ë†’ìŒ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ<br>\n",
    "- ì‚¬ìš©ì²˜: ìµœì‹  NLP ì‘ì—… ì „ë°˜<br>\n",
    "<br>\n",
    "ê° ë°©ë²•ì€ ìƒí™©ì— ë”°ë¼ ì¥ë‹¨ì ì´ ìˆìœ¼ë¯€ë¡œ, ë°ì´í„° íŠ¹ì„±ê³¼ ì‘ì—… ëª©í‘œì— ë§ê²Œ ì„ íƒí•˜ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Course (Python 3.9)",
   "language": "python",
   "name": "ai_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
