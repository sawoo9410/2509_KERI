{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdbb873-3379-4fd2-a0c4-0c29c29c24bc",
   "metadata": {},
   "source": [
    "### Created on 2025\n",
    "### @author: S.W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed542d40-336c-4b76-8d91-dd90b7df9a18",
   "metadata": {},
   "source": [
    "## 텍스트 전처리 및 벡터화 실습 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf59346-1810-4737-9b0a-fc1e64b26233",
   "metadata": {},
   "source": [
    "### 라이브리리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54064f3-5a8e-49c1-835b-4c7501aa4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2482f406-b095-43ec-9128-f55509254d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 텍스트 전처리 및 벡터화 실습 ===\n",
      "\n",
      "샘플 텍스트:\n",
      "1. Natural language processing is a fascinating field of artificial intelligence.\n",
      "2. Machine learning algorithms can process and analyze large amounts of text data.\n",
      "3. Deep learning models have revolutionized natural language understanding.\n",
      "4. Text preprocessing is an essential step in NLP pipeline.\n",
      "5. Word embeddings capture semantic relationships between words.\n",
      "6. Transformers have become the dominant architecture in modern NLP.\n",
      "7. BERT and GPT models achieve state-of-the-art results in many tasks.\n",
      "8. Text classification and sentiment analysis are common NLP applications.\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 설치 명령어 (주석 제거 후 실행)\n",
    "\"\"\"\n",
    "pip install nltk scikit-learn gensim transformers torch\n",
    "python -m nltk.downloader punkt stopwords\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# 샘플 데이터\n",
    "sample_texts = [\n",
    "    \"Natural language processing is a fascinating field of artificial intelligence.\",\n",
    "    \"Machine learning algorithms can process and analyze large amounts of text data.\",\n",
    "    \"Deep learning models have revolutionized natural language understanding.\",\n",
    "    \"Text preprocessing is an essential step in NLP pipeline.\",\n",
    "    \"Word embeddings capture semantic relationships between words.\",\n",
    "    \"Transformers have become the dominant architecture in modern NLP.\",\n",
    "    \"BERT and GPT models achieve state-of-the-art results in many tasks.\",\n",
    "    \"Text classification and sentiment analysis are common NLP applications.\"\n",
    "]\n",
    "\n",
    "print(\"=== 텍스트 전처리 및 벡터화 실습 ===\\n\")\n",
    "print(\"샘플 텍스트:\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67616e0-461e-4ef0-94e5-fbd6849dfbb7",
   "metadata": {},
   "source": [
    "### 1. 기본 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb334b63-60d4-43c6-a70b-b84c4acb5937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "1. 기본 텍스트 전처리\n",
      "==================================================\n",
      "전처리 과정을 단계별로 확인해보겠습니다:\n",
      "\n",
      "📝 문서 1 전처리:\n",
      "원본: Natural language processing is a fascinating field of artificial intelligence.\n",
      "    1단계 - 소문자 변환: natural language processing is a fascinating field of artificial intelligence.\n",
      "    2단계 - 구두점 제거: natural language processing is a fascinating field of artificial intelligence\n",
      "    3단계 - 토큰화: ['natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence']\n",
      "    불용어 예시: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    제거된 불용어: 'is'\n",
      "    제거된 불용어: 'a'\n",
      "    제거된 불용어: 'of'\n",
      "    4단계 - 불용어 제거 결과: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "✅ 최종 결과: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "토큰 개수: 7개\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📝 문서 2 전처리:\n",
      "원본: Machine learning algorithms can process and analyze large amounts of text data.\n",
      "    1단계 - 소문자 변환: machine learning algorithms can process and analyze large amounts of text data.\n",
      "    2단계 - 구두점 제거: machine learning algorithms can process and analyze large amounts of text data\n",
      "    3단계 - 토큰화: ['machine', 'learning', 'algorithms', 'can', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'text', 'data']\n",
      "    불용어 예시: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    제거된 불용어: 'can'\n",
      "    제거된 불용어: 'and'\n",
      "    제거된 불용어: 'of'\n",
      "    4단계 - 불용어 제거 결과: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "✅ 최종 결과: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "토큰 개수: 9개\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📝 문서 3 전처리:\n",
      "원본: Deep learning models have revolutionized natural language understanding.\n",
      "    1단계 - 소문자 변환: deep learning models have revolutionized natural language understanding.\n",
      "    2단계 - 구두점 제거: deep learning models have revolutionized natural language understanding\n",
      "    3단계 - 토큰화: ['deep', 'learning', 'models', 'have', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "    불용어 예시: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    제거된 불용어: 'have'\n",
      "    4단계 - 불용어 제거 결과: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "✅ 최종 결과: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "토큰 개수: 7개\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📝 문서 4 전처리:\n",
      "원본: Text preprocessing is an essential step in NLP pipeline.\n",
      "    1단계 - 소문자 변환: text preprocessing is an essential step in nlp pipeline.\n",
      "    2단계 - 구두점 제거: text preprocessing is an essential step in nlp pipeline\n",
      "    3단계 - 토큰화: ['text', 'preprocessing', 'is', 'an', 'essential', 'step', 'in', 'nlp', 'pipeline']\n",
      "    불용어 예시: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    제거된 불용어: 'is'\n",
      "    제거된 불용어: 'an'\n",
      "    제거된 불용어: 'in'\n",
      "    4단계 - 불용어 제거 결과: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "✅ 최종 결과: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "토큰 개수: 6개\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📝 문서 5 전처리:\n",
      "원본: Word embeddings capture semantic relationships between words.\n",
      "    1단계 - 소문자 변환: word embeddings capture semantic relationships between words.\n",
      "    2단계 - 구두점 제거: word embeddings capture semantic relationships between words\n",
      "    3단계 - 토큰화: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'between', 'words']\n",
      "    불용어 예시: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    제거된 불용어: 'between'\n",
      "    4단계 - 불용어 제거 결과: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "✅ 최종 결과: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "토큰 개수: 6개\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📝 문서 6 전처리:\n",
      "원본: Transformers have become the dominant architecture in modern NLP.\n",
      "    1단계 - 소문자 변환: transformers have become the dominant architecture in modern nlp.\n",
      "    2단계 - 구두점 제거: transformers have become the dominant architecture in modern nlp\n",
      "    3단계 - 토큰화: ['transformers', 'have', 'become', 'the', 'dominant', 'architecture', 'in', 'modern', 'nlp']\n",
      "    불용어 예시: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    제거된 불용어: 'have'\n",
      "    제거된 불용어: 'the'\n",
      "    제거된 불용어: 'in'\n",
      "    4단계 - 불용어 제거 결과: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "✅ 최종 결과: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "토큰 개수: 6개\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📝 문서 7 전처리:\n",
      "원본: BERT and GPT models achieve state-of-the-art results in many tasks.\n",
      "    1단계 - 소문자 변환: bert and gpt models achieve state-of-the-art results in many tasks.\n",
      "    2단계 - 구두점 제거: bert and gpt models achieve stateoftheart results in many tasks\n",
      "    3단계 - 토큰화: ['bert', 'and', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'in', 'many', 'tasks']\n",
      "    불용어 예시: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    제거된 불용어: 'and'\n",
      "    제거된 불용어: 'in'\n",
      "    4단계 - 불용어 제거 결과: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "✅ 최종 결과: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "토큰 개수: 8개\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📝 문서 8 전처리:\n",
      "원본: Text classification and sentiment analysis are common NLP applications.\n",
      "    1단계 - 소문자 변환: text classification and sentiment analysis are common nlp applications.\n",
      "    2단계 - 구두점 제거: text classification and sentiment analysis are common nlp applications\n",
      "    3단계 - 토큰화: ['text', 'classification', 'and', 'sentiment', 'analysis', 'are', 'common', 'nlp', 'applications']\n",
      "    불용어 예시: ['when', 'his', 'had', 'wasn', 'don', 'will', \"he's\", 'o', 'where', 'other']...\n",
      "    제거된 불용어: 'and'\n",
      "    제거된 불용어: 'are'\n",
      "    4단계 - 불용어 제거 결과: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "✅ 최종 결과: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "토큰 개수: 7개\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def basic_preprocessing(text):\n",
    "    \"\"\"\n",
    "    기본적인 텍스트 전처리 함수\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "    \n",
    "    Returns:\n",
    "        list: 전처리된 토큰 리스트\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1단계: 소문자 변환 (대소문자 통일로 일관성 확보)\n",
    "    # 예: \"Natural\" -> \"natural\"\n",
    "    text = text.lower()\n",
    "    print(f\"    1단계 - 소문자 변환: {text}\")\n",
    "    \n",
    "    # 2단계: 구두점 및 특수문자 제거\n",
    "    # 정규표현식 [^\\w\\s]: 문자, 숫자, 공백이 아닌 모든 것을 제거\n",
    "    # 예: \"natural!\" -> \"natural\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    print(f\"    2단계 - 구두점 제거: {text}\")\n",
    "    \n",
    "    # 3단계: 토큰화 (문장을 개별 단어로 분리)\n",
    "    # word_tokenize: NLTK의 토큰화 함수 사용\n",
    "    tokens = word_tokenize(text)\n",
    "    print(f\"    3단계 - 토큰화: {tokens}\")\n",
    "    \n",
    "    # 4단계: 불용어 제거 (의미가 적은 일반적인 단어 제거)\n",
    "    # 불용어 예: the, is, a, an, and, or, but 등\n",
    "    try:\n",
    "        # NLTK 영어 불용어 사전 로드\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        print(f\"    불용어 예시: {list(stop_words)[:10]}...\")\n",
    "        \n",
    "        # 불용어가 아닌 토큰만 유지\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                filtered_tokens.append(token)\n",
    "            else:\n",
    "                print(f\"    제거된 불용어: '{token}'\")\n",
    "        \n",
    "        tokens = filtered_tokens\n",
    "        \n",
    "    except LookupError:\n",
    "        # NLTK 불용어 데이터가 없는 경우 대안 처리\n",
    "        print(\"    NLTK stopwords not downloaded. Using basic preprocessing.\")\n",
    "        print(\"    대안: 길이가 2 이하인 단어 제거\")\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    print(f\"    4단계 - 불용어 제거 결과: {tokens}\")\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"1. 기본 텍스트 전처리\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 각 샘플 텍스트에 대해 전처리 수행\n",
    "preprocessed_texts = []  # 전처리된 결과를 저장할 리스트\n",
    "\n",
    "print(\"전처리 과정을 단계별로 확인해보겠습니다:\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"📝 문서 {i+1} 전처리:\")\n",
    "    print(f\"원본: {text}\")\n",
    "    \n",
    "    # 전처리 함수 호출 (각 단계별 진행상황이 출력됨)\n",
    "    tokens = basic_preprocessing(text)\n",
    "    \n",
    "    # 결과를 리스트에 저장\n",
    "    preprocessed_texts.append(tokens)\n",
    "    \n",
    "    print(f\"✅ 최종 결과: {tokens}\")\n",
    "    print(f\"토큰 개수: {len(tokens)}개\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b650271-02c5-4b0a-b975-44f9532bc139",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944eee61-8a1b-4468-be1e-1e947ffe6338",
   "metadata": {},
   "source": [
    "### 2. Bag of Words (BoW)\n",
    "\n",
    "Bag of Words (BoW) 개념:\n",
    "- 문서를 단어의 집합(가방)으로 표현\n",
    "- 단어의 순서는 무시하고, 출현 빈도만 고려\n",
    "- 각 문서를 고정된 크기의 벡터로 변환\n",
    "- 벡터의 각 차원은 특정 단어의 출현 횟수를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bccf4627-9a00-4c09-a8a6-547b8fde557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2. Bag of Words (BoW)\n",
      "==================================================\n",
      "======================================================================\n",
      " 1단계: 전체 문서에서 고유 단어 수집\n",
      "  문서 1의 단어들: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  문서 2의 단어들: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "  문서 3의 단어들: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "  문서 4의 단어들: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "  문서 5의 단어들: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "  문서 6의 단어들: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "  문서 7의 단어들: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "  문서 8의 단어들: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "\n",
      "📖 어휘 사전 생성 완료!\n",
      "  - 총 고유 단어 수: 48개\n",
      "  - 어휘 사전 (처음 15개): ['achieve', 'algorithms', 'amounts', 'analysis', 'analyze', 'applications', 'architecture', 'artificial', 'become', 'bert', 'capture', 'classification', 'common', 'data', 'deep']\n",
      "  - ... (나머지 33개)\n",
      "\n",
      " 2단계: 각 문서를 BoW 벡터로 변환\n",
      "\n",
      "  📄 문서 1 처리 중...\n",
      "    토큰: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "    출현한 단어와 빈도: {'artificial': 1, 'fascinating': 1, 'field': 1, 'intelligence': 1, 'language': 1, 'natural': 1, 'processing': 1}\n",
      "    BoW 벡터 길이: 48\n",
      "    0이 아닌 값의 개수: 7\n",
      "\n",
      "  📄 문서 2 처리 중...\n",
      "    토큰: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "    출현한 단어와 빈도: {'algorithms': 1, 'amounts': 1, 'analyze': 1, 'data': 1, 'large': 1, 'learning': 1, 'machine': 1, 'process': 1, 'text': 1}\n",
      "    BoW 벡터 길이: 48\n",
      "    0이 아닌 값의 개수: 9\n",
      "\n",
      "  📄 문서 3 처리 중...\n",
      "    토큰: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "    출현한 단어와 빈도: {'deep': 1, 'language': 1, 'learning': 1, 'models': 1, 'natural': 1, 'revolutionized': 1, 'understanding': 1}\n",
      "    BoW 벡터 길이: 48\n",
      "    0이 아닌 값의 개수: 7\n",
      "\n",
      "  📄 문서 4 처리 중...\n",
      "    토큰: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "    출현한 단어와 빈도: {'essential': 1, 'nlp': 1, 'pipeline': 1, 'preprocessing': 1, 'step': 1, 'text': 1}\n",
      "    BoW 벡터 길이: 48\n",
      "    0이 아닌 값의 개수: 6\n",
      "\n",
      "  📄 문서 5 처리 중...\n",
      "    토큰: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "    출현한 단어와 빈도: {'capture': 1, 'embeddings': 1, 'relationships': 1, 'semantic': 1, 'word': 1, 'words': 1}\n",
      "    BoW 벡터 길이: 48\n",
      "    0이 아닌 값의 개수: 6\n",
      "\n",
      "  📄 문서 6 처리 중...\n",
      "    토큰: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "    출현한 단어와 빈도: {'architecture': 1, 'become': 1, 'dominant': 1, 'modern': 1, 'nlp': 1, 'transformers': 1}\n",
      "    BoW 벡터 길이: 48\n",
      "    0이 아닌 값의 개수: 6\n",
      "\n",
      "  📄 문서 7 처리 중...\n",
      "    토큰: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "    출현한 단어와 빈도: {'achieve': 1, 'bert': 1, 'gpt': 1, 'many': 1, 'models': 1, 'results': 1, 'stateoftheart': 1, 'tasks': 1}\n",
      "    BoW 벡터 길이: 48\n",
      "    0이 아닌 값의 개수: 8\n",
      "\n",
      "  📄 문서 8 처리 중...\n",
      "    토큰: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "    출현한 단어와 빈도: {'analysis': 1, 'applications': 1, 'classification': 1, 'common': 1, 'nlp': 1, 'sentiment': 1, 'text': 1}\n",
      "    BoW 벡터 길이: 48\n",
      "    0이 아닌 값의 개수: 7\n",
      "\n",
      " BoW 행렬 생성 완료!\n",
      " 행렬 크기: (8, 48) (문서 수 × 어휘 크기)\n",
      " 첫 번째 문서의 BoW 벡터 (처음 10차원): [0 0 0 0 0 0 0 1 0 0]\n",
      " 희소성(Sparsity): 85.4% (대부분의 값이 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2. Bag of Words (BoW)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def create_bow_manual(texts):\n",
    "    \"\"\"\n",
    "    수동으로 BoW 벡터 생성하기\n",
    "    \n",
    "    Args:\n",
    "        texts (list): 전처리된 토큰 리스트들\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (BoW 행렬, 어휘 사전)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" 1단계: 전체 문서에서 고유 단어 수집\")\n",
    "    # 모든 문서의 모든 단어를 하나의 집합으로 수집\n",
    "    all_words = set()\n",
    "    for i, tokens in enumerate(texts):\n",
    "        print(f\"  문서 {i+1}의 단어들: {tokens}\")\n",
    "        all_words.update(tokens)  # 집합에 단어들 추가\n",
    "    \n",
    "    # 어휘 사전 생성 (알파벳 순으로 정렬)\n",
    "    vocab = sorted(list(all_words))\n",
    "    print(f\"\\n📖 어휘 사전 생성 완료!\")\n",
    "    print(f\"  - 총 고유 단어 수: {len(vocab)}개\")\n",
    "    print(f\"  - 어휘 사전 (처음 15개): {vocab[:15]}\")\n",
    "    if len(vocab) > 15:\n",
    "        print(f\"  - ... (나머지 {len(vocab)-15}개)\")\n",
    "    \n",
    "    print(f\"\\n 2단계: 각 문서를 BoW 벡터로 변환\")\n",
    "    # 각 문서에 대해 BoW 벡터 생성\n",
    "    bow_vectors = []\n",
    "    \n",
    "    for doc_idx, tokens in enumerate(texts):\n",
    "        print(f\"\\n  📄 문서 {doc_idx+1} 처리 중...\")\n",
    "        print(f\"    토큰: {tokens}\")\n",
    "        \n",
    "        # 각 어휘에 대해 해당 문서에서의 출현 횟수 계산\n",
    "        vector = []\n",
    "        word_counts = {}\n",
    "        \n",
    "        for word in vocab:\n",
    "            count = tokens.count(word)  # 해당 단어의 출현 횟수\n",
    "            vector.append(count)\n",
    "            if count > 0:  # 출현한 단어만 기록\n",
    "                word_counts[word] = count\n",
    "        \n",
    "        bow_vectors.append(vector)\n",
    "        \n",
    "        # 출현한 단어들만 출력\n",
    "        print(f\"    출현한 단어와 빈도: {word_counts}\")\n",
    "        print(f\"    BoW 벡터 길이: {len(vector)}\")\n",
    "        print(f\"    0이 아닌 값의 개수: {sum(1 for x in vector if x > 0)}\")\n",
    "    \n",
    "    return np.array(bow_vectors), vocab\n",
    "\n",
    "print(\"=\" * 70)\n",
    "bow_manual, vocab_manual = create_bow_manual(preprocessed_texts)\n",
    "\n",
    "print(f\"\\n BoW 행렬 생성 완료!\")\n",
    "print(f\" 행렬 크기: {bow_manual.shape} (문서 수 × 어휘 크기)\")\n",
    "print(f\" 첫 번째 문서의 BoW 벡터 (처음 10차원): {bow_manual[0][:10]}\")\n",
    "\n",
    "# 희소성 계산\n",
    "total_elements = bow_manual.shape[0] * bow_manual.shape[1]\n",
    "zero_elements = np.sum(bow_manual == 0)\n",
    "sparsity = (zero_elements / total_elements) * 100\n",
    "print(f\" 희소성(Sparsity): {sparsity:.1f}% (대부분의 값이 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c328a6-1ceb-4a58-b074-e645e564f285",
   "metadata": {},
   "source": [
    "**실제 프로젝트에서는 Scikit-learn의 CountVectorizer를 주로 사용합니다.<br>\n",
    "더 효율적이고 다양한 옵션을 제공합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751bf00a-9392-43b7-b197-e149b5312df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "📚 Scikit-learn을 사용한 BoW\n",
      "==================================================\n",
      "🔧 Scikit-learn BoW 설정:\n",
      "  - 어휘 크기: 48\n",
      "  - 행렬 크기: (8, 48)\n",
      "  - 행렬 타입: <class 'scipy.sparse._csr.csr_matrix'> (메모리 효율적인 희소 행렬)\n",
      "\n",
      "🔍 어휘 사전 비교:\n",
      "  - 수동 구현: 48개 단어\n",
      "  - Scikit-learn: 48개 단어\n",
      "  - 동일한 결과: True\n",
      "\n",
      "📊 결과 비교:\n",
      "  - 수동 구현 첫 문서: [0 0 0 0 0]...\n",
      "  - Scikit-learn 첫 문서: [0 0 0 0 0]...\n",
      "  - 결과 일치: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📚 Scikit-learn을 사용한 BoW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scikit-learn CountVectorizer 사용\n",
    "vectorizer_bow = CountVectorizer(\n",
    "    tokenizer=lambda x: x,  # 이미 토큰화된 데이터 사용\n",
    "    lowercase=False         # 이미 소문자 변환 완료\n",
    ")\n",
    "\n",
    "bow_sklearn = vectorizer_bow.fit_transform(preprocessed_texts)\n",
    "\n",
    "print(f\"🔧 Scikit-learn BoW 설정:\")\n",
    "print(f\"  - 어휘 크기: {len(vectorizer_bow.vocabulary_)}\")\n",
    "print(f\"  - 행렬 크기: {bow_sklearn.shape}\")\n",
    "print(f\"  - 행렬 타입: {type(bow_sklearn)} (메모리 효율적인 희소 행렬)\")\n",
    "\n",
    "# 어휘 사전 비교\n",
    "sklearn_vocab = sorted(vectorizer_bow.vocabulary_.keys())\n",
    "print(f\"\\n🔍 어휘 사전 비교:\")\n",
    "print(f\"  - 수동 구현: {len(vocab_manual)}개 단어\")\n",
    "print(f\"  - Scikit-learn: {len(sklearn_vocab)}개 단어\")\n",
    "print(f\"  - 동일한 결과: {vocab_manual == sklearn_vocab}\")\n",
    "\n",
    "# 희소 행렬을 밀집 행렬로 변환하여 비교\n",
    "bow_sklearn_dense = bow_sklearn.toarray()\n",
    "print(f\"\\n📊 결과 비교:\")\n",
    "print(f\"  - 수동 구현 첫 문서: {bow_manual[0][:5]}...\")\n",
    "print(f\"  - Scikit-learn 첫 문서: {bow_sklearn_dense[0][:5]}...\")\n",
    "print(f\"  - 결과 일치: {np.array_equal(bow_manual, bow_sklearn_dense)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc43427-3b36-46be-b25a-4c50464c4ff4",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6362984-9b9e-4ebd-9b8b-89e5f78d1501",
   "metadata": {},
   "source": [
    "### 3. N-gram\n",
    "N-gram 개념:\n",
    "- N개의 연속된 단어를 하나의 단위로 취급\n",
    "- 단어 간의 순서와 맥락 정보를 어느 정도 보존\n",
    "- 1-gram(unigram): 개별 단어\n",
    "- 2-gram(bigram): 두 개의 연속된 단어\n",
    "- 3-gram(trigram): 세 개의 연속된 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c2445b-d97b-480a-b0bf-656d164b5336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "3. N-gram\n",
      "==================================================\n",
      "\n",
      "📚 N-gram 개념:\n",
      "- N개의 연속된 단어를 하나의 단위로 취급\n",
      "- 단어 간의 순서와 맥락 정보를 어느 정도 보존\n",
      "- 1-gram(unigram): 개별 단어\n",
      "- 2-gram(bigram): 두 개의 연속된 단어\n",
      "- 3-gram(trigram): 세 개의 연속된 단어\n",
      "\n",
      "🎯 샘플 문서 분석:\n",
      "원본 문서: Natural language processing is a fascinating field of artificial intelligence.\n",
      "전처리된 토큰: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "\n",
      "========================================\n",
      "📝 1-gram (Unigram) 생성\n",
      "========================================\n",
      "🔍 1-gram 생성 과정:\n",
      "  입력 토큰: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  토큰 개수: 7\n",
      "  위치 0: ['natural'] → 'natural'\n",
      "  위치 1: ['language'] → 'language'\n",
      "  위치 2: ['processing'] → 'processing'\n",
      "  위치 3: ['fascinating'] → 'fascinating'\n",
      "  위치 4: ['field'] → 'field'\n",
      "  위치 5: ['artificial'] → 'artificial'\n",
      "  위치 6: ['intelligence'] → 'intelligence'\n",
      "  생성된 1-gram 개수: 7\n",
      "\n",
      "========================================\n",
      "📝 2-gram (Bigram) 생성\n",
      "========================================\n",
      "🔍 2-gram 생성 과정:\n",
      "  입력 토큰: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  토큰 개수: 7\n",
      "  위치 0: ['natural', 'language'] → 'natural language'\n",
      "  위치 1: ['language', 'processing'] → 'language processing'\n",
      "  위치 2: ['processing', 'fascinating'] → 'processing fascinating'\n",
      "  위치 3: ['fascinating', 'field'] → 'fascinating field'\n",
      "  위치 4: ['field', 'artificial'] → 'field artificial'\n",
      "  위치 5: ['artificial', 'intelligence'] → 'artificial intelligence'\n",
      "  생성된 2-gram 개수: 6\n",
      "\n",
      "========================================\n",
      "📝 3-gram (Trigram) 생성\n",
      "========================================\n",
      "🔍 3-gram 생성 과정:\n",
      "  입력 토큰: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  토큰 개수: 7\n",
      "  위치 0: ['natural', 'language', 'processing'] → 'natural language processing'\n",
      "  위치 1: ['language', 'processing', 'fascinating'] → 'language processing fascinating'\n",
      "  위치 2: ['processing', 'fascinating', 'field'] → 'processing fascinating field'\n",
      "  위치 3: ['fascinating', 'field', 'artificial'] → 'fascinating field artificial'\n",
      "  위치 4: ['field', 'artificial', 'intelligence'] → 'field artificial intelligence'\n",
      "  생성된 3-gram 개수: 5\n",
      "\n",
      "📊 N-gram 분석 결과:\n",
      "  원본 토큰 수: 7\n",
      "  1-gram 수: 7 (= 토큰 수)\n",
      "  2-gram 수: 6 (= 토큰 수 - 1)\n",
      "  3-gram 수: 5 (= 토큰 수 - 2)\n",
      "\n",
      "🔍 생성된 N-gram 예시:\n",
      "  1-gram: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  2-gram: ['natural language', 'language processing', 'processing fascinating', 'fascinating field', 'field artificial', 'artificial intelligence']\n",
      "  3-gram: ['natural language processing', 'language processing fascinating', 'processing fascinating field', 'fascinating field artificial', 'field artificial intelligence']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3. N-gram\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "📚 N-gram 개념:\n",
    "- N개의 연속된 단어를 하나의 단위로 취급\n",
    "- 단어 간의 순서와 맥락 정보를 어느 정도 보존\n",
    "- 1-gram(unigram): 개별 단어\n",
    "- 2-gram(bigram): 두 개의 연속된 단어\n",
    "- 3-gram(trigram): 세 개의 연속된 단어\n",
    "\"\"\")\n",
    "\n",
    "def create_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    N-gram 생성 함수\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): 토큰 리스트\n",
    "        n (int): N-gram의 N 값\n",
    "    \n",
    "    Returns:\n",
    "        list: N-gram 리스트\n",
    "    \"\"\"\n",
    "    print(f\"🔍 {n}-gram 생성 과정:\")\n",
    "    print(f\"  입력 토큰: {tokens}\")\n",
    "    print(f\"  토큰 개수: {len(tokens)}\")\n",
    "    \n",
    "    ngrams = []\n",
    "    \n",
    "    # 슬라이딩 윈도우 방식으로 N-gram 생성\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        # i번째부터 i+n번째까지의 토큰들을 결합\n",
    "        ngram_tokens = tokens[i:i+n]\n",
    "        ngram = ' '.join(ngram_tokens)\n",
    "        ngrams.append(ngram)\n",
    "        print(f\"  위치 {i}: {ngram_tokens} → '{ngram}'\")\n",
    "    \n",
    "    print(f\"  생성된 {n}-gram 개수: {len(ngrams)}\")\n",
    "    return ngrams\n",
    "\n",
    "# 첫 번째 문서로 N-gram 예시 생성\n",
    "sample_tokens = preprocessed_texts[0]\n",
    "print(f\"🎯 샘플 문서 분석:\")\n",
    "print(f\"원본 문서: {sample_texts[0]}\")\n",
    "print(f\"전처리된 토큰: {sample_tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"📝 1-gram (Unigram) 생성\")\n",
    "print(\"=\"*40)\n",
    "unigrams = create_ngrams(sample_tokens, 1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"📝 2-gram (Bigram) 생성\")\n",
    "print(\"=\"*40)\n",
    "bigrams = create_ngrams(sample_tokens, 2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"📝 3-gram (Trigram) 생성\")\n",
    "print(\"=\"*40)\n",
    "trigrams = create_ngrams(sample_tokens, 3)\n",
    "\n",
    "# N-gram 특성 분석\n",
    "print(f\"\\n📊 N-gram 분석 결과:\")\n",
    "print(f\"  원본 토큰 수: {len(sample_tokens)}\")\n",
    "print(f\"  1-gram 수: {len(unigrams)} (= 토큰 수)\")\n",
    "print(f\"  2-gram 수: {len(bigrams)} (= 토큰 수 - 1)\")\n",
    "print(f\"  3-gram 수: {len(trigrams)} (= 토큰 수 - 2)\")\n",
    "\n",
    "print(f\"\\n🔍 생성된 N-gram 예시:\")\n",
    "print(f\"  1-gram: {unigrams}\")\n",
    "print(f\"  2-gram: {bigrams}\")\n",
    "print(f\"  3-gram: {trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a83681-1087-4bb8-a05f-71fbfe99f030",
   "metadata": {},
   "source": [
    "#### Scikit-learn을 사용한 N-gram\n",
    "N-gram 벡터화 설정:\n",
    "- ngram_range=(1, 3): 1-gram부터 3-gram까지 모두 포함\n",
    "- max_features=50: 최대 50개의 특성만 선택 (빈도가 높은 순)\n",
    "<br>\n",
    "\n",
    "**실제 프로젝트에서는 Scikit-learn의 CountVectorizer에서 ngram_range 매개변수를 사용하여 N-gram을 생성합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71864f3-f736-418f-901e-46f18734d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 N-gram 행렬 정보:\n",
      "  - 행렬 크기: (8, 50)\n",
      "  - 문서 수: 8\n",
      "  - N-gram 특성 수: 50\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ngram = CountVectorizer(\n",
    "    ngram_range=(1, 3),    # 1-gram부터 3-gram까지\n",
    "    max_features=50,       # 최대 특성 수 제한 (차원 폭발 방지)\n",
    "    stop_words='english'   # 영어 불용어 자동 제거\n",
    ")\n",
    "\n",
    "# 원본 텍스트에서 직접 N-gram 생성\n",
    "ngram_matrix = vectorizer_ngram.fit_transform(sample_texts)\n",
    "\n",
    "print(f\"\\n📊 N-gram 행렬 정보:\")\n",
    "print(f\"  - 행렬 크기: {ngram_matrix.shape}\")\n",
    "print(f\"  - 문서 수: {ngram_matrix.shape[0]}\")\n",
    "print(f\"  - N-gram 특성 수: {ngram_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfbddfa4-7d7a-413a-a341-a2a522a6daca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 N-gram 특성 예시 (처음 20개):\n",
      "   1. 'achieve' (1-gram)\n",
      "   2. 'language' (1-gram)\n",
      "   3. 'learning' (1-gram)\n",
      "   4. 'learning algorithms process' (3-gram)\n",
      "   5. 'learning models' (2-gram)\n",
      "   6. 'learning models revolutionized' (3-gram)\n",
      "   7. 'machine' (1-gram)\n",
      "   8. 'machine learning' (2-gram)\n",
      "   9. 'machine learning algorithms' (3-gram)\n",
      "  10. 'models' (1-gram)\n",
      "  11. 'models achieve' (2-gram)\n",
      "  12. 'models achieve state' (3-gram)\n",
      "  13. 'models revolutionized' (2-gram)\n",
      "  14. 'models revolutionized natural' (3-gram)\n",
      "  15. 'modern' (1-gram)\n",
      "  16. 'modern nlp' (2-gram)\n",
      "  17. 'natural' (1-gram)\n",
      "  18. 'natural language' (2-gram)\n",
      "  19. 'natural language processing' (3-gram)\n",
      "  20. 'natural language understanding' (3-gram)\n",
      "  ... (나머지 30개)\n"
     ]
    }
   ],
   "source": [
    "# 생성된 N-gram 특성 확인\n",
    "feature_names = vectorizer_ngram.get_feature_names_out()\n",
    "print(f\"생성된 N-gram 특성 예시 (처음 20개):\")\n",
    "for i, feature in enumerate(feature_names[:20]):\n",
    "    feature_type = \"1-gram\" if ' ' not in feature else f\"{len(feature.split())}-gram\"\n",
    "    print(f\"  {i+1:2d}. '{feature}' ({feature_type})\")\n",
    "\n",
    "if len(feature_names) > 20:\n",
    "    print(f\"  ... (나머지 {len(feature_names)-20}개)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c883ea4-70d1-4461-b641-d632779f4c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 문서에서 출현한 N-gram:\n",
      "원본: Natural language processing is a fascinating field of artificial intelligence.\n",
      "  'language' (1-gram): 1회\n",
      "  'natural' (1-gram): 1회\n",
      "  'natural language' (2-gram): 1회\n",
      "  'natural language processing' (3-gram): 1회\n",
      "  'processing' (1-gram): 1회\n",
      "  'processing fascinating' (2-gram): 1회\n",
      "  'processing fascinating field' (3-gram): 1회\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 문서의 N-gram 분석\n",
    "first_doc_vector = ngram_matrix[0].toarray().flatten()\n",
    "non_zero_indices = np.where(first_doc_vector > 0)[0]\n",
    "\n",
    "print(f\"첫 번째 문서에서 출현한 N-gram:\")\n",
    "print(f\"원본: {sample_texts[0]}\")\n",
    "for idx in non_zero_indices[:10]:  # 처음 10개만 출력\n",
    "    feature = feature_names[idx]\n",
    "    count = first_doc_vector[idx]\n",
    "    feature_type = \"1-gram\" if ' ' not in feature else f\"{len(feature.split())}-gram\"\n",
    "    print(f\"  '{feature}' ({feature_type}): {count}회\")\n",
    "\n",
    "if len(non_zero_indices) > 10:\n",
    "    print(f\"  ... (나머지 {len(non_zero_indices)-10}개)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f84d5-b207-477e-8710-2cd44ed11a60",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84825214-7f5f-4d4a-8d23-735617528540",
   "metadata": {},
   "source": [
    "### 4. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "TF-IDF 개념:\n",
    "- TF (Term Frequency): 특정 문서에서 단어의 출현 빈도\n",
    "- IDF (Inverse Document Frequency): 전체 문서에서 단어의 희귀성\n",
    "- TF-IDF = TF × IDF\n",
    "- 목적: 자주 나오지만 특별한 의미가 없는 단어의 가중치를 낮춤\n",
    "- 문서의 핵심 내용을 나타내는 단어에 높은 가중치 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ee4f6d-07e1-48cd-b0d1-1b2f980fd068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 TF-IDF 계산 예시 (첫 3개 문서)\n",
      "원본 문서들:\n",
      "  문서 1:\n",
      "    원본: Natural language processing is a fascinating field of artificial intelligence.\n",
      "    토큰: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  문서 2:\n",
      "    원본: Machine learning algorithms can process and analyze large amounts of text data.\n",
      "    토큰: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "  문서 3:\n",
      "    원본: Deep learning models have revolutionized natural language understanding.\n",
      "    토큰: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n"
     ]
    }
   ],
   "source": [
    "# 첫 3개 문서로 예시 실행 (출력을 간결하게 하기 위해)\n",
    "sample_docs = preprocessed_texts[:3]\n",
    "print(\"\\n🎯 TF-IDF 계산 예시 (첫 3개 문서)\")\n",
    "print(\"원본 문서들:\")\n",
    "for i in range(3):\n",
    "    print(f\"  문서 {i+1}:\")\n",
    "    print(f\"    원본: {sample_texts[i]}\")\n",
    "    print(f\"    토큰: {sample_docs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7234d31e-97f8-456c-a2ba-e9207e2ef000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "4. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "==================================================\n",
      "🔍 TF-IDF 계산 시작\n",
      "==================================================\n",
      "\n",
      "📊 IDF 계산 과정:\n",
      "  전체 문서 수: 3\n",
      "  전체 고유 단어 수: 20\n",
      "  'algorithms': log(3/1) = 1.0986\n",
      "  'amounts': log(3/1) = 1.0986\n",
      "  'analyze': log(3/1) = 1.0986\n",
      "  'artificial': log(3/1) = 1.0986\n",
      "  'data': log(3/1) = 1.0986\n",
      "  'deep': log(3/1) = 1.0986\n",
      "  'fascinating': log(3/1) = 1.0986\n",
      "  'field': log(3/1) = 1.0986\n",
      "  'intelligence': log(3/1) = 1.0986\n",
      "  'language': log(3/2) = 0.4055\n",
      "  'large': log(3/1) = 1.0986\n",
      "  'learning': log(3/2) = 0.4055\n",
      "  'machine': log(3/1) = 1.0986\n",
      "  'models': log(3/1) = 1.0986\n",
      "  'natural': log(3/2) = 0.4055\n",
      "  'process': log(3/1) = 1.0986\n",
      "  'processing': log(3/1) = 1.0986\n",
      "  'revolutionized': log(3/1) = 1.0986\n",
      "  'text': log(3/1) = 1.0986\n",
      "  'understanding': log(3/1) = 1.0986\n",
      "\n",
      "📋 IDF 값 요약:\n",
      "  'algorithms': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'amounts': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'analyze': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'artificial': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'data': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'deep': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'fascinating': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'field': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'intelligence': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "  'large': 1.0986 (희귀한 단어일수록 높은 값)\n",
      "\n",
      "📄 문서 1 TF-IDF 계산:\n",
      "  토큰: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "📊 TF 계산 과정:\n",
      "  입력 토큰: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "  총 단어 수: 7\n",
      "  단어별 출현 횟수: {'natural': 1, 'language': 1, 'processing': 1, 'fascinating': 1, 'field': 1, 'artificial': 1, 'intelligence': 1}\n",
      "  'natural': 1/7 = 0.1429\n",
      "  'language': 1/7 = 0.1429\n",
      "  'processing': 1/7 = 0.1429\n",
      "  'fascinating': 1/7 = 0.1429\n",
      "  'field': 1/7 = 0.1429\n",
      "  'artificial': 1/7 = 0.1429\n",
      "  'intelligence': 1/7 = 0.1429\n",
      "\n",
      "  TF-IDF = TF × IDF 계산:\n",
      "    'natural': 0.1429 × 0.4055 = 0.0579\n",
      "    'language': 0.1429 × 0.4055 = 0.0579\n",
      "    'processing': 0.1429 × 1.0986 = 0.1569\n",
      "    'fascinating': 0.1429 × 1.0986 = 0.1569\n",
      "    'field': 0.1429 × 1.0986 = 0.1569\n",
      "    'artificial': 0.1429 × 1.0986 = 0.1569\n",
      "    'intelligence': 0.1429 × 1.0986 = 0.1569\n",
      "  💡 문서 1의 핵심 단어 (TF-IDF 기준):\n",
      "    'processing': 0.1569\n",
      "    'fascinating': 0.1569\n",
      "    'field': 0.1569\n",
      "\n",
      "📄 문서 2 TF-IDF 계산:\n",
      "  토큰: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "📊 TF 계산 과정:\n",
      "  입력 토큰: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "  총 단어 수: 9\n",
      "  단어별 출현 횟수: {'machine': 1, 'learning': 1, 'algorithms': 1, 'process': 1, 'analyze': 1, 'large': 1, 'amounts': 1, 'text': 1, 'data': 1}\n",
      "  'machine': 1/9 = 0.1111\n",
      "  'learning': 1/9 = 0.1111\n",
      "  'algorithms': 1/9 = 0.1111\n",
      "  'process': 1/9 = 0.1111\n",
      "  'analyze': 1/9 = 0.1111\n",
      "  'large': 1/9 = 0.1111\n",
      "  'amounts': 1/9 = 0.1111\n",
      "  'text': 1/9 = 0.1111\n",
      "  'data': 1/9 = 0.1111\n",
      "\n",
      "  TF-IDF = TF × IDF 계산:\n",
      "    'machine': 0.1111 × 1.0986 = 0.1221\n",
      "    'learning': 0.1111 × 0.4055 = 0.0451\n",
      "    'algorithms': 0.1111 × 1.0986 = 0.1221\n",
      "    'process': 0.1111 × 1.0986 = 0.1221\n",
      "    'analyze': 0.1111 × 1.0986 = 0.1221\n",
      "    'large': 0.1111 × 1.0986 = 0.1221\n",
      "    'amounts': 0.1111 × 1.0986 = 0.1221\n",
      "    'text': 0.1111 × 1.0986 = 0.1221\n",
      "    'data': 0.1111 × 1.0986 = 0.1221\n",
      "  💡 문서 2의 핵심 단어 (TF-IDF 기준):\n",
      "    'machine': 0.1221\n",
      "    'algorithms': 0.1221\n",
      "    'process': 0.1221\n",
      "\n",
      "📄 문서 3 TF-IDF 계산:\n",
      "  토큰: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "📊 TF 계산 과정:\n",
      "  입력 토큰: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "  총 단어 수: 7\n",
      "  단어별 출현 횟수: {'deep': 1, 'learning': 1, 'models': 1, 'revolutionized': 1, 'natural': 1, 'language': 1, 'understanding': 1}\n",
      "  'deep': 1/7 = 0.1429\n",
      "  'learning': 1/7 = 0.1429\n",
      "  'models': 1/7 = 0.1429\n",
      "  'revolutionized': 1/7 = 0.1429\n",
      "  'natural': 1/7 = 0.1429\n",
      "  'language': 1/7 = 0.1429\n",
      "  'understanding': 1/7 = 0.1429\n",
      "\n",
      "  TF-IDF = TF × IDF 계산:\n",
      "    'deep': 0.1429 × 1.0986 = 0.1569\n",
      "    'learning': 0.1429 × 0.4055 = 0.0579\n",
      "    'models': 0.1429 × 1.0986 = 0.1569\n",
      "    'revolutionized': 0.1429 × 1.0986 = 0.1569\n",
      "    'natural': 0.1429 × 0.4055 = 0.0579\n",
      "    'language': 0.1429 × 0.4055 = 0.0579\n",
      "    'understanding': 0.1429 × 1.0986 = 0.1569\n",
      "  💡 문서 3의 핵심 단어 (TF-IDF 기준):\n",
      "    'deep': 0.1569\n",
      "    'models': 0.1569\n",
      "    'revolutionized': 0.1569\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"4. TF-IDF (Term Frequency-Inverse Document Frequency)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def calculate_tf(tokens):\n",
    "    \"\"\"\n",
    "    TF (Term Frequency) 계산하기\n",
    "    TF = (특정 단어의 출현 횟수) / (문서의 총 단어 수)\n",
    "    \"\"\"\n",
    "    print(f\"📊 TF 계산 과정:\")\n",
    "    print(f\"  입력 토큰: {tokens}\")\n",
    "    \n",
    "    tf_dict = {}\n",
    "    total_count = len(tokens)\n",
    "    print(f\"  총 단어 수: {total_count}\")\n",
    "    \n",
    "    # 각 단어의 출현 횟수 계산\n",
    "    word_counts = Counter(tokens)\n",
    "    print(f\"  단어별 출현 횟수: {dict(word_counts)}\")\n",
    "    \n",
    "    # TF 값 계산 \n",
    "    for token, count in word_counts.items():\n",
    "        tf_value = count / total_count\n",
    "        tf_dict[token] = tf_value\n",
    "        print(f\"  '{token}': {count}/{total_count} = {tf_value:.4f}\")\n",
    "    \n",
    "    return tf_dict\n",
    "\n",
    "def calculate_idf(documents):\n",
    "    \"\"\"\n",
    "    IDF (Inverse Document Frequency) 계산하기\n",
    "    IDF = log(전체 문서 수 / 해당 단어가 포함된 문서 수)\n",
    "    \"\"\"\n",
    "    print(f\"\\n📊 IDF 계산 과정:\")\n",
    "    \n",
    "    idf_dict = {}\n",
    "    total_documents = len(documents)\n",
    "    print(f\"  전체 문서 수: {total_documents}\")\n",
    "    \n",
    "    # 모든 고유 단어 수집\n",
    "    all_words = set()\n",
    "    for doc in documents:\n",
    "        all_words.update(doc)\n",
    "    \n",
    "    print(f\"  전체 고유 단어 수: {len(all_words)}\")\n",
    "    \n",
    "    # 각 단어에 대해 IDF 계산\n",
    "    for word in sorted(all_words):\n",
    "        # 해당 단어가 포함된 문서 수 계산\n",
    "        containing_docs = 0\n",
    "        for doc in documents:\n",
    "            if word in doc:\n",
    "                containing_docs += 1\n",
    "        \n",
    "        # IDF 계산\n",
    "        idf_value = math.log(total_documents / containing_docs)\n",
    "        idf_dict[word] = idf_value\n",
    "        \n",
    "        print(f\"  '{word}': log({total_documents}/{containing_docs}) = {idf_value:.4f}\")\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "def calculate_tfidf_manual(documents):\n",
    "    \"\"\"\n",
    "    TF-IDF 수동으로 계산하기\n",
    "    \"\"\"\n",
    "    print(\"🔍 TF-IDF 계산 시작\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1단계: IDF 계산 (전체 문서에 대해)\n",
    "    idf = calculate_idf(documents)\n",
    "    \n",
    "    print(f\"\\n📋 IDF 값 요약:\")\n",
    "    sorted_idf = sorted(idf.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_idf[:10]:\n",
    "        print(f\"  '{word}': {score:.4f} (희귀한 단어일수록 높은 값)\")\n",
    "    \n",
    "    # 2단계: 각 문서별 TF-IDF 계산\n",
    "    tfidf_documents = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        print(f\"\\n📄 문서 {doc_idx+1} TF-IDF 계산:\")\n",
    "        print(f\"  토큰: {doc}\")\n",
    "        \n",
    "        # TF 계산\n",
    "        tf = calculate_tf(doc)\n",
    "        \n",
    "        # TF-IDF 계산\n",
    "        tfidf_doc = {}\n",
    "        print(f\"\\n  TF-IDF = TF × IDF 계산:\")\n",
    "        for word, tf_val in tf.items():\n",
    "            tfidf_val = tf_val * idf[word]\n",
    "            tfidf_doc[word] = tfidf_val\n",
    "            print(f\"    '{word}': {tf_val:.4f} × {idf[word]:.4f} = {tfidf_val:.4f}\")\n",
    "        \n",
    "        tfidf_documents.append(tfidf_doc)\n",
    "        \n",
    "        # 해당 문서의 중요 단어 출력\n",
    "        sorted_tfidf = sorted(tfidf_doc.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"  💡 문서 {doc_idx+1}의 핵심 단어 (TF-IDF 기준):\")\n",
    "        for word, score in sorted_tfidf[:3]:\n",
    "            print(f\"    '{word}': {score:.4f}\")\n",
    "    \n",
    "    return tfidf_documents, idf\n",
    "\n",
    "tfidf_manual, idf_scores = calculate_tfidf_manual(sample_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d248ccb-0547-4423-aa8e-dc4a92666c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "📊 TF-IDF 결과 분석\n",
      "==================================================\n",
      "\n",
      "📄 문서 1 분석:\n",
      "  원본: Natural language processing is a fascinating field of artificial intelligence.\n",
      "  TF-IDF 상위 단어:\n",
      "    1. 'processing': 0.1569\n",
      "    2. 'fascinating': 0.1569\n",
      "    3. 'field': 0.1569\n",
      "    4. 'artificial': 0.1569\n",
      "    5. 'intelligence': 0.1569\n",
      "    6. 'natural': 0.0579\n",
      "    7. 'language': 0.0579\n",
      "\n",
      "📄 문서 2 분석:\n",
      "  원본: Machine learning algorithms can process and analyze large amounts of text data.\n",
      "  TF-IDF 상위 단어:\n",
      "    1. 'machine': 0.1221\n",
      "    2. 'algorithms': 0.1221\n",
      "    3. 'process': 0.1221\n",
      "    4. 'analyze': 0.1221\n",
      "    5. 'large': 0.1221\n",
      "    6. 'amounts': 0.1221\n",
      "    7. 'text': 0.1221\n",
      "    8. 'data': 0.1221\n",
      "    9. 'learning': 0.0451\n",
      "\n",
      "📄 문서 3 분석:\n",
      "  원본: Deep learning models have revolutionized natural language understanding.\n",
      "  TF-IDF 상위 단어:\n",
      "    1. 'deep': 0.1569\n",
      "    2. 'models': 0.1569\n",
      "    3. 'revolutionized': 0.1569\n",
      "    4. 'understanding': 0.1569\n",
      "    5. 'learning': 0.0579\n",
      "    6. 'natural': 0.0579\n",
      "    7. 'language': 0.0579\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 TF-IDF 결과 분석\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 각 문서별 상위 단어 분석\n",
    "for doc_idx in range(len(tfidf_manual)):\n",
    "    print(f\"\\n📄 문서 {doc_idx+1} 분석:\")\n",
    "    print(f\"  원본: {sample_texts[doc_idx]}\")\n",
    "    \n",
    "    tfidf_doc = tfidf_manual[doc_idx]\n",
    "    sorted_tfidf = sorted(tfidf_doc.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"  TF-IDF 상위 단어:\")\n",
    "    for rank, (word, score) in enumerate(sorted_tfidf, 1):\n",
    "        print(f\"    {rank}. '{word}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9df0b-44ef-47fa-acd0-21d2daad495a",
   "metadata": {},
   "source": [
    "#### Scikit-learn을 사용한 TF-IDF\n",
    "\n",
    "**실제 프로젝트에서는 Scikit-learn의 TfidfVectorizer를 사용합니다.\n",
    "더 효율적이고 다양한 옵션을 제공합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c294b67-1ee3-42f7-8d63-2acb6979672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 문자열들:\n",
      "  1. natural language processing fascinating field artificial intelligence\n",
      "  2. machine learning algorithms process analyze large amounts text data\n",
      "  3. deep learning models revolutionized natural language understanding\n",
      "  4. text preprocessing essential step nlp pipeline\n",
      "  5. word embeddings capture semantic relationships words\n",
      "  6. transformers become dominant architecture modern nlp\n",
      "  7. bert gpt models achieve stateoftheart results many tasks\n",
      "  8. text classification sentiment analysis common nlp applications\n"
     ]
    }
   ],
   "source": [
    "# 전처리된 토큰을 다시 문자열로 변환 (Scikit-learn이 문자열을 받기 때문)\n",
    "preprocessed_strings = []\n",
    "for tokens in preprocessed_texts:\n",
    "    text_string = ' '.join(tokens)\n",
    "    preprocessed_strings.append(text_string)\n",
    "\n",
    "print(\"전처리된 문자열들:\")\n",
    "for i, text in enumerate(preprocessed_strings):\n",
    "    print(f\"  {i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d40ad4d0-85fb-43ab-81b1-196590ad70c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Scikit-learn TF-IDF 결과:\n",
      "  - 행렬 크기: (8, 48) (문서 수 × 단어 수)\n",
      "  - 어휘 크기: 48개\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn TfidfVectorizer 사용\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,          # 이미 소문자 변환 완료\n",
    "    stop_words=None,          # 이미 불용어 제거 완료\n",
    "    norm='l2',               # L2 정규화 \n",
    "    use_idf=True,            # IDF 사용\n",
    "    smooth_idf=True,         # 안정적인 계산을 위한 스무딩\n",
    "    sublinear_tf=False       # 기본 TF 사용\n",
    ")\n",
    "\n",
    "tfidf_sklearn = tfidf_vectorizer.fit_transform(preprocessed_strings)\n",
    "\n",
    "print(f\"\\n🔧 Scikit-learn TF-IDF 결과:\")\n",
    "print(f\"  - 행렬 크기: {tfidf_sklearn.shape} (문서 수 × 단어 수)\")\n",
    "print(f\"  - 어휘 크기: {len(tfidf_vectorizer.vocabulary_)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "641c278f-07fb-42ff-8302-9b80119957ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📖 전체 어휘:\n",
      "  0: 'achieve'\n",
      "  1: 'algorithms'\n",
      "  2: 'amounts'\n",
      "  3: 'analysis'\n",
      "  4: 'analyze'\n",
      "  5: 'applications'\n",
      "  6: 'architecture'\n",
      "  7: 'artificial'\n",
      "  8: 'become'\n",
      "  9: 'bert'\n",
      "  10: 'capture'\n",
      "  11: 'classification'\n",
      "  12: 'common'\n",
      "  13: 'data'\n",
      "  14: 'deep'\n",
      "  15: 'dominant'\n",
      "  16: 'embeddings'\n",
      "  17: 'essential'\n",
      "  18: 'fascinating'\n",
      "  19: 'field'\n",
      "  20: 'gpt'\n",
      "  21: 'intelligence'\n",
      "  22: 'language'\n",
      "  23: 'large'\n",
      "  24: 'learning'\n",
      "  25: 'machine'\n",
      "  26: 'many'\n",
      "  27: 'models'\n",
      "  28: 'modern'\n",
      "  29: 'natural'\n",
      "  30: 'nlp'\n",
      "  31: 'pipeline'\n",
      "  32: 'preprocessing'\n",
      "  33: 'process'\n",
      "  34: 'processing'\n",
      "  35: 'relationships'\n",
      "  36: 'results'\n",
      "  37: 'revolutionized'\n",
      "  38: 'semantic'\n",
      "  39: 'sentiment'\n",
      "  40: 'stateoftheart'\n",
      "  41: 'step'\n",
      "  42: 'tasks'\n",
      "  43: 'text'\n",
      "  44: 'transformers'\n",
      "  45: 'understanding'\n",
      "  46: 'word'\n",
      "  47: 'words'\n"
     ]
    }
   ],
   "source": [
    "# 어휘 확인\n",
    "print(f\"\\n📖 전체 어휘:\")\n",
    "vocabulary = tfidf_vectorizer.vocabulary_\n",
    "sorted_vocab = sorted(vocabulary.items(), key=lambda x: x[1])\n",
    "for word, idx in sorted_vocab:\n",
    "    print(f\"  {idx}: '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fddd4b62-dd23-43b0-884e-697cc972da0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Scikit-learn IDF 값:\n",
      "  'achieve': 2.5041\n",
      "  'algorithms': 2.5041\n",
      "  'amounts': 2.5041\n",
      "  'analysis': 2.5041\n",
      "  'analyze': 2.5041\n",
      "  'applications': 2.5041\n",
      "  'architecture': 2.5041\n",
      "  'artificial': 2.5041\n",
      "  'become': 2.5041\n",
      "  'bert': 2.5041\n",
      "  'capture': 2.5041\n",
      "  'classification': 2.5041\n",
      "  'common': 2.5041\n",
      "  'data': 2.5041\n",
      "  'deep': 2.5041\n",
      "  'dominant': 2.5041\n",
      "  'embeddings': 2.5041\n",
      "  'essential': 2.5041\n",
      "  'fascinating': 2.5041\n",
      "  'field': 2.5041\n",
      "  'gpt': 2.5041\n",
      "  'intelligence': 2.5041\n",
      "  'language': 2.0986\n",
      "  'large': 2.5041\n",
      "  'learning': 2.0986\n",
      "  'machine': 2.5041\n",
      "  'many': 2.5041\n",
      "  'models': 2.0986\n",
      "  'modern': 2.5041\n",
      "  'natural': 2.0986\n",
      "  'nlp': 1.8109\n",
      "  'pipeline': 2.5041\n",
      "  'preprocessing': 2.5041\n",
      "  'process': 2.5041\n",
      "  'processing': 2.5041\n",
      "  'relationships': 2.5041\n",
      "  'results': 2.5041\n",
      "  'revolutionized': 2.5041\n",
      "  'semantic': 2.5041\n",
      "  'sentiment': 2.5041\n",
      "  'stateoftheart': 2.5041\n",
      "  'step': 2.5041\n",
      "  'tasks': 2.5041\n",
      "  'text': 1.8109\n",
      "  'transformers': 2.5041\n",
      "  'understanding': 2.5041\n",
      "  'word': 2.5041\n",
      "  'words': 2.5041\n",
      "\n",
      "📈 각 문서의 TF-IDF 값:\n",
      "\n",
      "문서 1:\n",
      "  원본: Natural language processing is a fascinating field of artificial intelligence.\n",
      "  1. 'artificial': 0.3951\n",
      "  2. 'fascinating': 0.3951\n",
      "  3. 'field': 0.3951\n",
      "  4. 'intelligence': 0.3951\n",
      "  5. 'processing': 0.3951\n",
      "  6. 'language': 0.3312\n",
      "  7. 'natural': 0.3312\n",
      "\n",
      "문서 2:\n",
      "  원본: Machine learning algorithms can process and analyze large amounts of text data.\n",
      "  1. 'algorithms': 0.3487\n",
      "  2. 'amounts': 0.3487\n",
      "  3. 'analyze': 0.3487\n",
      "  4. 'data': 0.3487\n",
      "  5. 'large': 0.3487\n",
      "  6. 'machine': 0.3487\n",
      "  7. 'process': 0.3487\n",
      "  8. 'learning': 0.2922\n",
      "  9. 'text': 0.2522\n",
      "\n",
      "문서 3:\n",
      "  원본: Deep learning models have revolutionized natural language understanding.\n",
      "  1. 'deep': 0.4149\n",
      "  2. 'revolutionized': 0.4149\n",
      "  3. 'understanding': 0.4149\n",
      "  4. 'language': 0.3477\n",
      "  5. 'learning': 0.3477\n",
      "  6. 'models': 0.3477\n",
      "  7. 'natural': 0.3477\n"
     ]
    }
   ],
   "source": [
    "# IDF 값 확인\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "\n",
    "print(f\"\\n📊 Scikit-learn IDF 값:\")\n",
    "for i, word in enumerate(feature_names):\n",
    "    print(f\"  '{word}': {idf_values[i]:.4f}\")\n",
    "\n",
    "# 각 문서의 TF-IDF 값 확인 (첫 3개 문서)\n",
    "print(f\"\\n📈 각 문서의 TF-IDF 값:\")\n",
    "for doc_idx in range(min(3, tfidf_sklearn.shape[0])):\n",
    "    doc_tfidf = tfidf_sklearn[doc_idx].toarray().flatten()\n",
    "    \n",
    "    print(f\"\\n문서 {doc_idx+1}:\")\n",
    "    print(f\"  원본: {sample_texts[doc_idx]}\")\n",
    "    \n",
    "    # 0이 아닌 값들만 출력\n",
    "    word_scores = []\n",
    "    for i, score in enumerate(doc_tfidf):\n",
    "        if score > 0:\n",
    "            word = feature_names[i]\n",
    "            word_scores.append((word, score))\n",
    "    \n",
    "    # 점수 순으로 정렬\n",
    "    word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for rank, (word, score) in enumerate(word_scores, 1):\n",
    "        print(f\"  {rank}. '{word}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b19181-cbf6-4cc9-92e4-a4d8cd16e22e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750ed43-4772-4aa3-9ba1-c84bcafacd00",
   "metadata": {},
   "source": [
    "### 5. Word2Vec (Word to Vector)\n",
    "Word2Vec 개념:\n",
    "- 단어를 고정 크기의 실수 벡터로 변환하는 기술\n",
    "- 비슷한 의미의 단어들이 벡터 공간에서 가까운 위치에 배치됨\n",
    "- TF-IDF와 달리 단어 간 의미적 관계를 학습함\n",
    "- 두 가지 모델: CBOW (주변 단어로 중심 단어 예측), Skip-gram (중심 단어로 주변 단어 예측)\n",
    "\n",
    "Word2Vec의 핵심 아이디어:\n",
    "- \"비슷한 맥락에서 나타나는 단어들은 비슷한 의미를 가진다\"\n",
    "- 예: \"강아지는 귀엽다\", \"고양이는 귀엽다\" → '강아지'와 '고양이'는 유사한 벡터를 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b596f4-8637-49b9-8bfb-33e21f0055b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Word2Vec 학습에 사용할 데이터 확인:\n",
      "전처리된 문서들:\n",
      "  문서 1: ['natural', 'language', 'processing', 'fascinating', 'field', 'artificial', 'intelligence']\n",
      "    토큰 수: 7개\n",
      "  문서 2: ['machine', 'learning', 'algorithms', 'process', 'analyze', 'large', 'amounts', 'text', 'data']\n",
      "    토큰 수: 9개\n",
      "  문서 3: ['deep', 'learning', 'models', 'revolutionized', 'natural', 'language', 'understanding']\n",
      "    토큰 수: 7개\n",
      "  문서 4: ['text', 'preprocessing', 'essential', 'step', 'nlp', 'pipeline']\n",
      "    토큰 수: 6개\n",
      "  문서 5: ['word', 'embeddings', 'capture', 'semantic', 'relationships', 'words']\n",
      "    토큰 수: 6개\n",
      "  문서 6: ['transformers', 'become', 'dominant', 'architecture', 'modern', 'nlp']\n",
      "    토큰 수: 6개\n",
      "  문서 7: ['bert', 'gpt', 'models', 'achieve', 'stateoftheart', 'results', 'many', 'tasks']\n",
      "    토큰 수: 8개\n",
      "  문서 8: ['text', 'classification', 'sentiment', 'analysis', 'common', 'nlp', 'applications']\n",
      "    토큰 수: 7개\n",
      "\n",
      "📊 전체 어휘 통계:\n",
      "  총 토큰 수: 56개\n",
      "  고유 단어 수: 48개\n",
      "  가장 빈번한 단어 5개:\n",
      "    'text': 3번\n",
      "    'nlp': 3번\n",
      "    'natural': 2번\n",
      "    'language': 2번\n",
      "    'learning': 2번\n"
     ]
    }
   ],
   "source": [
    "def show_preprocessing_result(preprocessed_texts):\n",
    "    \"\"\"전처리 결과를 다시 한번 확인\"\"\"\n",
    "    print(\"🔍 Word2Vec 학습에 사용할 데이터 확인:\")\n",
    "    print(\"전처리된 문서들:\")\n",
    "    for i, tokens in enumerate(preprocessed_texts):\n",
    "        print(f\"  문서 {i+1}: {tokens}\")\n",
    "        print(f\"    토큰 수: {len(tokens)}개\")\n",
    "    \n",
    "    # 전체 어휘 통계\n",
    "    all_words = []\n",
    "    for tokens in preprocessed_texts:\n",
    "        all_words.extend(tokens)\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    print(f\"\\n📊 전체 어휘 통계:\")\n",
    "    print(f\"  총 토큰 수: {len(all_words)}개\")\n",
    "    print(f\"  고유 단어 수: {len(word_counts)}개\")\n",
    "    print(f\"  가장 빈번한 단어 5개:\")\n",
    "    for word, count in word_counts.most_common(5):\n",
    "        print(f\"    '{word}': {count}번\")\n",
    "\n",
    "show_preprocessing_result(preprocessed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee8c10c-820f-461b-bef3-8e5a7f48f03f",
   "metadata": {},
   "source": [
    "Word2Vec 주요 파라미터 설명:\n",
    "- vector_size=100   : 각 단어를 100차원 벡터로 표현\n",
    "- window=5         : 중심 단어 기준 앞뒤 5개 단어까지 고려\n",
    "- min_count=1      : 최소 1번 이상 나타난 단어만 학습 (데이터가 적어서 1로 설정)\n",
    "- workers=4        : 4개 프로세스로 병렬 학습\n",
    "- sg=0            : CBOW 모델 사용 (0=CBOW, 1=Skip-gram)\n",
    "- epochs=10       : 전체 데이터를 10번 반복 학습\n",
    "<br>\n",
    "\n",
    "CBOW vs Skip-gram:\n",
    "<br>\n",
    "\n",
    "CBOW (Continuous Bag of Words):\n",
    "- 주변 단어들로 중심 단어를 예측\n",
    "- 예: '자연어 [?] 는 흥미롭다' → '처리'를 예측\n",
    "- 빠른 학습, 빈번한 단어에 좋음\n",
    "\n",
    "<br>\n",
    "\n",
    "Skip-gram:\n",
    "- 중심 단어로 주변 단어들을 예측\n",
    "- 예: '처리' → '자연어', '는', '흥미롭다' 예측\n",
    "- 정확한 벡터, 희귀 단어에 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a961f09c-6db1-4945-95fc-fc88fcf6d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Word2Vec 모델 학습 중...\n",
      "✅ Word2Vec 모델 학습 완료!\n",
      "\n",
      "📊 Word2Vec 모델 정보:\n",
      "  어휘 크기: 48개\n",
      "  벡터 차원: 100차원\n",
      "  학습된 단어들: ['nlp', 'text', 'models', 'language', 'learning', 'natural', 'many', 'process', 'revolutionized', 'tasks', 'deep', 'data', 'classification', 'amounts', 'large', 'analyze', 'algorithms', 'results', 'sentiment', 'machine', 'intelligence', 'artificial', 'field', 'fascinating', 'processing', 'analysis', 'understanding', 'preprocessing', 'essential', 'step', 'stateoftheart', 'achieve', 'gpt', 'bert', 'modern', 'architecture', 'dominant', 'become', 'transformers', 'words', 'relationships', 'semantic', 'capture', 'embeddings', 'word', 'pipeline', 'common', 'applications']\n",
      "\n",
      "🔍 단어 벡터 예시:\n",
      "  'nlp' 벡터:\n",
      "    전체 차원: 100차원\n",
      "    처음 10차원: [-0.00053441  0.0002342   0.00509681  0.00901455 -0.00930748 -0.00712235\n",
      "  0.00645791  0.0089834  -0.00501605 -0.00377127]\n",
      "    벡터 크기(norm): 0.0566\n",
      "\n",
      "  'text' 벡터:\n",
      "    전체 차원: 100차원\n",
      "    처음 10차원: [-0.00861682  0.00366286  0.00518951  0.00575312  0.00745169 -0.00618391\n",
      "  0.00111267  0.00605572 -0.00285139 -0.00618342]\n",
      "    벡터 크기(norm): 0.0580\n",
      "\n",
      "  'models' 벡터:\n",
      "    전체 차원: 100차원\n",
      "    처음 10차원: [ 7.9089732e-05  3.0848815e-03 -6.8244315e-03 -1.3741386e-03\n",
      "  7.6535218e-03  7.3492429e-03 -3.6750515e-03  2.6406990e-03\n",
      " -8.3153769e-03  6.1992793e-03]\n",
      "    벡터 크기(norm): 0.0569\n",
      "\n",
      "  'language' 벡터:\n",
      "    전체 차원: 100차원\n",
      "    처음 10차원: [-0.00825263  0.0093145  -0.00020367 -0.00196852  0.00460115 -0.00409642\n",
      "  0.00274841  0.00694611  0.00605521 -0.0075093 ]\n",
      "    벡터 크기(norm): 0.0580\n",
      "\n",
      "  'learning' 벡터:\n",
      "    전체 차원: 100차원\n",
      "    처음 10차원: [-0.00713939  0.00123792 -0.00717642 -0.00223109  0.00370713  0.00582198\n",
      "  0.00120219  0.00211002 -0.00411433  0.00722092]\n",
      "    벡터 크기(norm): 0.0578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec 모델 학습\n",
    "print(\"🔄 Word2Vec 모델 학습 중...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=preprocessed_texts,  # 학습 데이터 (토큰화된 문장들)\n",
    "    vector_size=100,              # 임베딩 벡터 차원 수\n",
    "    window=5,                     # 컨텍스트 윈도우 크기\n",
    "    min_count=1,                  # 최소 출현 빈도 (데이터가 적어서 1로 설정)\n",
    "    workers=4,                    # 병렬 처리 워커 수\n",
    "    sg=0,                        # 0=CBOW, 1=Skip-gram\n",
    "    epochs=10                    # 학습 반복 횟수\n",
    ")\n",
    "\n",
    "print(\"✅ Word2Vec 모델 학습 완료!\")\n",
    "\n",
    "print(f\"\\n📊 Word2Vec 모델 정보:\")\n",
    "print(f\"  어휘 크기: {len(w2v_model.wv)}개\")\n",
    "print(f\"  벡터 차원: {w2v_model.wv.vector_size}차원\")\n",
    "print(f\"  학습된 단어들: {list(w2v_model.wv.index_to_key)}\")\n",
    "\n",
    "print(f\"\\n🔍 단어 벡터 예시:\")\n",
    "# 학습된 모든 단어의 벡터 확인\n",
    "for i, word in enumerate(w2v_model.wv.index_to_key[:5]):  # 처음 5개 단어만\n",
    "    vector = w2v_model.wv[word]\n",
    "    print(f\"  '{word}' 벡터:\")\n",
    "    print(f\"    전체 차원: {len(vector)}차원\")\n",
    "    print(f\"    처음 10차원: {vector[:10]}\")\n",
    "    print(f\"    벡터 크기(norm): {np.linalg.norm(vector):.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfe66ac0-b44f-44d5-bb8e-de2f4ed59735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📐 단어 간 유사도 계산:\n",
      "유사도 = 코사인 유사도 (두 벡터 간 각도로 측정)\n",
      "  1.0에 가까울수록 유사, 0에 가까울수록 관련 없음\n",
      "\n",
      "💡 단어 쌍별 유사도:\n",
      "  'natural' ↔ 'nlp': -0.0597\n",
      "  'natural' ↔ 'text': 0.0095\n",
      "  'natural' ↔ 'models': 0.0649\n",
      "  'natural' ↔ 'language': 0.1319\n",
      "  'natural' ↔ 'learning': 0.1391\n",
      "  'natural' ↔ 'many': 0.0192\n",
      "  'natural' ↔ 'process': -0.0578\n",
      "  'natural' ↔ 'revolutionized': 0.0610\n"
     ]
    }
   ],
   "source": [
    "def calculate_word_similarity(w2v_model):\n",
    "    \"\"\"단어 간 유사도 계산 및 설명\"\"\"\n",
    "    print(\"📐 단어 간 유사도 계산:\")\n",
    "    print(\"유사도 = 코사인 유사도 (두 벡터 간 각도로 측정)\")\n",
    "    print(\"  1.0에 가까울수록 유사, 0에 가까울수록 관련 없음\")\n",
    "    print()\n",
    "    \n",
    "    # 학습된 단어들 중에서 유사도 계산\n",
    "    vocab = list(w2v_model.wv.index_to_key)\n",
    "    \n",
    "    if len(vocab) >= 2:\n",
    "        print(\"💡 단어 쌍별 유사도:\")\n",
    "        \n",
    "        # 몇 가지 단어 쌍의 유사도 계산\n",
    "        word_pairs = []\n",
    "        \n",
    "        # 'natural'과 다른 단어들 비교\n",
    "        if 'natural' in vocab:\n",
    "            for word in vocab:\n",
    "                if word != 'natural':\n",
    "                    word_pairs.append(('natural', word))\n",
    "        \n",
    "        # 'learning'과 다른 단어들 비교  \n",
    "        if 'learning' in vocab:\n",
    "            for word in vocab:\n",
    "                if word != 'learning' and ('learning', word) not in word_pairs:\n",
    "                    word_pairs.append(('learning', word))\n",
    "        \n",
    "        # 처음 몇 개만 보여주기\n",
    "        for word1, word2 in word_pairs[:8]:\n",
    "            try:\n",
    "                similarity = w2v_model.wv.similarity(word1, word2)\n",
    "                print(f\"  '{word1}' ↔ '{word2}': {similarity:.4f}\")\n",
    "            except KeyError as e:\n",
    "                print(f\"  '{word1}' ↔ '{word2}': 계산 불가 ({e})\")\n",
    "    else:\n",
    "        print(\"유사도 계산을 위한 충분한 어휘가 없습니다.\")\n",
    "\n",
    "calculate_word_similarity(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e09d64fc-b5b9-45a9-b049-eaa54b0077ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 유사 단어 찾기:\n",
      "각 단어와 가장 유사한 단어들을 찾아보겠습니다.\n",
      "\n",
      "📝 'learning'와 유사한 단어들:\n",
      "  1. 'amounts': 0.2534\n",
      "  2. 'understanding': 0.2008\n",
      "  3. 'transformers': 0.1953\n",
      "\n",
      "📝 'natural'와 유사한 단어들:\n",
      "  1. 'data': 0.1671\n",
      "  2. 'stateoftheart': 0.1632\n",
      "  3. 'learning': 0.1391\n",
      "\n",
      "📝 'language'와 유사한 단어들:\n",
      "  1. 'large': 0.1783\n",
      "  2. 'dominant': 0.1638\n",
      "  3. 'pipeline': 0.1496\n",
      "\n",
      "📝 'processing'와 유사한 단어들:\n",
      "  1. 'semantic': 0.1924\n",
      "  2. 'capture': 0.1566\n",
      "  3. 'deep': 0.1020\n",
      "\n",
      "📝 'data'와 유사한 단어들:\n",
      "  1. 'stateoftheart': 0.1819\n",
      "  2. 'models': 0.1729\n",
      "  3. 'natural': 0.1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_similar_words(w2v_model):\n",
    "    \"\"\"유사 단어 찾기\"\"\"\n",
    "    print(f\"\\n🔎 유사 단어 찾기:\")\n",
    "    print(\"각 단어와 가장 유사한 단어들을 찾아보겠습니다.\")\n",
    "    print()\n",
    "    \n",
    "    vocab = list(w2v_model.wv.index_to_key)\n",
    "    \n",
    "    # 몇 개 단어에 대해 유사 단어 찾기\n",
    "    target_words = ['learning', 'natural', 'language', 'processing', 'data']\n",
    "    \n",
    "    for target_word in target_words:\n",
    "        if target_word in vocab:\n",
    "            print(f\"📝 '{target_word}'와 유사한 단어들:\")\n",
    "            try:\n",
    "                # 가장 유사한 상위 3개 단어 찾기\n",
    "                similar_words = w2v_model.wv.most_similar(target_word, topn=3)\n",
    "                \n",
    "                if similar_words:\n",
    "                    for rank, (word, score) in enumerate(similar_words, 1):\n",
    "                        print(f\"  {rank}. '{word}': {score:.4f}\")\n",
    "                else:\n",
    "                    print(\"  유사한 단어를 찾을 수 없습니다.\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  오류: {e}\")\n",
    "            print()\n",
    "\n",
    "find_similar_words(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81cc93d1-6ade-4d7e-ab94-35cd6d75f7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 벡터 연산 (단어 관계 학습):\n",
      "Word2Vec의 놀라운 특성: 단어 간 관계를 벡터 연산으로 표현 가능!\n",
      "예: king - man + woman = queen (왕 - 남자 + 여자 = 여왕)\n",
      "\n",
      "🔬 우리 데이터로 벡터 연산 실험:\n",
      "  'natural' - 'language' + 'processing' ≈ 'semantic' (0.2005)\n",
      "  'machine' - 'learning' + 'data' ≈ 'process' (0.2366)\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_vector_arithmetic(w2v_model):\n",
    "    \"\"\"벡터 연산 데모\"\"\"\n",
    "    print(\"🧮 벡터 연산 (단어 관계 학습):\")\n",
    "    print(\"Word2Vec의 놀라운 특성: 단어 간 관계를 벡터 연산으로 표현 가능!\")\n",
    "    print(\"예: king - man + woman = queen (왕 - 남자 + 여자 = 여왕)\")\n",
    "    print()\n",
    "    \n",
    "    vocab = list(w2v_model.wv.index_to_key)\n",
    "    \n",
    "    # 우리 데이터에서 가능한 벡터 연산 시도\n",
    "    if len(vocab) >= 3:\n",
    "        print(\"🔬 우리 데이터로 벡터 연산 실험:\")\n",
    "        \n",
    "        # 몇 가지 조합 시도\n",
    "        combinations = [\n",
    "            ('natural', 'language', 'processing'),\n",
    "            ('machine', 'learning', 'data'),\n",
    "            ('deep', 'learning', 'model')\n",
    "        ]\n",
    "        \n",
    "        for word1, word2, word3 in combinations:\n",
    "            if all(word in vocab for word in [word1, word2, word3]):\n",
    "                try:\n",
    "                    # word1 - word2 + word3와 가장 유사한 단어 찾기\n",
    "                    result = w2v_model.wv.most_similar(\n",
    "                        positive=[word1, word3], \n",
    "                        negative=[word2], \n",
    "                        topn=1\n",
    "                    )\n",
    "                    if result:\n",
    "                        result_word, score = result[0]\n",
    "                        print(f\"  '{word1}' - '{word2}' + '{word3}' ≈ '{result_word}' ({score:.4f})\")\n",
    "                except:\n",
    "                    print(f\"  '{word1}' - '{word2}' + '{word3}': 계산 실패\")\n",
    "    \n",
    "    if not any(all(word in vocab for word in combo) for combo in combinations):\n",
    "        print(\"벡터 연산을 위한 충분한 관련 단어들이 없습니다.\")\n",
    "        print(\"더 많은 데이터로 학습하면 더 흥미로운 결과를 볼 수 있습니다!\")\n",
    "\n",
    "demonstrate_vector_arithmetic(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a95ccd-fd96-4f32-982b-928e9244dc0c",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7377f79-1ef9-42d8-85c6-e2f0cd2563e9",
   "metadata": {},
   "source": [
    "### 6. GloVe (사전 훈련된 모델 사용)\n",
    "\n",
    "GloVe 개념:\n",
    "- Stanford에서 개발한 단어 임베딩 기법\n",
    "- Word2Vec과 달리 전역적(Global) 통계 정보를 활용\n",
    "- 단어-단어 동시출현 행렬(Co-occurrence Matrix)을 기반으로 학습\n",
    "- Word2Vec의 예측 기반 방법과 전통적인 통계 방법을 결합\n",
    "\n",
    "GloVe의 핵심 아이디어:\n",
    "- \"단어의 의미는 전체 말뭉치에서의 동시출현 패턴으로 결정된다\"\n",
    "- 예: '얼음'과 '증기' 모두 '물'과 자주 나타나지만, '고체'는 '얼음'과 더 관련있음\n",
    "- 이러한 비율 정보를 벡터로 학습\n",
    "\n",
    "Word2Vec vs GloVe:\n",
    "- Word2Vec: 지역적 문맥 윈도우 내에서 예측 학습\n",
    "- GloVe: 전체 말뭉치의 통계적 정보를 직접 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cde18072-45eb-4f1a-8cd5-5df969cac5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GloVe 사전 훈련된 모델 로드 시작...\n",
      "⏱️  첫 실행 시 인터넷에서 다운로드하므로 시간이 걸릴 수 있습니다.\n",
      "📦 모델 크기: 약 50MB (glove-wiki-gigaword-50)\n",
      "\n",
      "📥 Gensim API를 통해 GloVe 모델 다운로드 중...\n",
      "✅ GloVe 모델 로드 성공!\n"
     ]
    }
   ],
   "source": [
    "print(\"🔄 GloVe 사전 훈련된 모델 로드 시작...\")\n",
    "print(\"⏱️  첫 실행 시 인터넷에서 다운로드하므로 시간이 걸릴 수 있습니다.\")\n",
    "print(\"📦 모델 크기: 약 50MB (glove-wiki-gigaword-50)\")\n",
    "\n",
    "# 작은 GloVe 모델 로드 (50차원)\n",
    "print(\"\\n📥 Gensim API를 통해 GloVe 모델 다운로드 중...\")\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "print(\"✅ GloVe 모델 로드 성공!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea2fbbe1-df5f-4a76-8f64-2b73897d190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 GloVe 모델 정보:\n",
      "  어휘 크기: 400,000개 단어\n",
      "  벡터 차원: 50차원\n",
      "  학습 데이터: Wikipedia + Gigaword (60억 토큰)\n",
      "\n",
      "📖 어휘 예시 (처음 20개):\n",
      "   1. 'the'\n",
      "   2. ','\n",
      "   3. '.'\n",
      "   4. 'of'\n",
      "   5. 'to'\n",
      "   6. 'and'\n",
      "   7. 'in'\n",
      "   8. 'a'\n",
      "   9. '\"'\n",
      "  10. ''s'\n",
      "  11. 'for'\n",
      "  12. '-'\n",
      "  13. 'that'\n",
      "  14. 'on'\n",
      "  15. 'is'\n",
      "  16. 'was'\n",
      "  17. 'said'\n",
      "  18. 'with'\n",
      "  19. 'he'\n",
      "  20. 'as'\n"
     ]
    }
   ],
   "source": [
    "# 모델 기본 정보 출력\n",
    "print(f\"\\n📊 GloVe 모델 정보:\")\n",
    "print(f\"  어휘 크기: {len(glove_model):,}개 단어\")\n",
    "print(f\"  벡터 차원: 50차원\")\n",
    "print(f\"  학습 데이터: Wikipedia + Gigaword (60억 토큰)\")\n",
    "\n",
    "# 어휘 예시 확인\n",
    "print(f\"\\n📖 어휘 예시 (처음 20개):\")\n",
    "vocab_sample = list(glove_model.index_to_key[:20])\n",
    "for i, word in enumerate(vocab_sample):\n",
    "    print(f\"  {i+1:2d}. '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bcf3b48-c9c5-4e25-bdb9-8f33e4484035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 단어 벡터 예시:\n",
      "  📝 'natural' 벡터:\n",
      "    전체 차원: 50차원\n",
      "    처음 10차원: [ 0.44265  0.84765 -0.4598   0.67993  0.13841  0.39456 -0.17343 -0.64055\n",
      "  0.86439  0.81624]\n",
      "    벡터 크기: 5.0903\n",
      "    최대값: 3.3077, 최소값: -0.9165\n",
      "\n",
      "  📝 'language' 벡터:\n",
      "    전체 차원: 50차원\n",
      "    처음 10차원: [-0.5799    -0.1101    -1.1557    -0.0029906 -0.20613    0.45289\n",
      " -0.16671   -1.0382    -0.99241    0.39884  ]\n",
      "    벡터 크기: 6.0993\n",
      "    최대값: 3.7163, 최소값: -1.3878\n",
      "\n",
      "  📝 'processing' 벡터:\n",
      "    전체 차원: 50차원\n",
      "    처음 10차원: [ 1.6092e-01 -9.0221e-01  1.5797e-01  1.1776e+00 -6.2201e-04 -1.9004e-02\n",
      " -1.5081e-01 -5.8863e-01  1.5128e+00  4.2868e-01]\n",
      "    벡터 크기: 5.3926\n",
      "    최대값: 3.2094, 최소값: -0.9775\n",
      "\n",
      "  📝 'fascinating' 벡터:\n",
      "    전체 차원: 50차원\n",
      "    처음 10차원: [ 0.90512   0.63951  -0.94111   0.33322   1.0375   -0.060236  0.043731\n",
      " -0.26376   0.074989  0.8521  ]\n",
      "    벡터 크기: 4.6380\n",
      "    최대값: 1.6140, 최소값: -1.6849\n",
      "\n",
      "  📝 'field' 벡터:\n",
      "    전체 차원: 50차원\n",
      "    처음 10차원: [-0.49284  0.3731   0.15565  0.70044  0.77405 -0.48151 -1.2244  -0.02163\n",
      "  0.63856 -0.49535]\n",
      "    벡터 크기: 5.1761\n",
      "    최대값: 2.9194, 최소값: -1.7308\n",
      "\n",
      "  📝 'artificial' 벡터:\n",
      "    전체 차원: 50차원\n",
      "    처음 10차원: [ 0.71561  0.70368 -0.65484  0.49875 -0.85243  1.1059  -0.1036  -0.88284\n",
      "  0.80456  0.71413]\n",
      "    벡터 크기: 4.5111\n",
      "    최대값: 2.3077, 최소값: -0.8828\n",
      "\n",
      "  📝 'intelligence' 벡터:\n",
      "    전체 차원: 50차원\n",
      "    처음 10차원: [ 0.8782   -0.45171   0.96737   0.040347  0.76235  -0.63825   0.18944\n",
      " -0.26633   0.58874  -0.93608 ]\n",
      "    벡터 크기: 5.7777\n",
      "    최대값: 2.4132, 최소값: -2.3939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_word_vectors(preprocessed_texts):\n",
    "    \"\"\"단어 벡터 예시\"\"\"\n",
    "    print(f\"\\n🔍 단어 벡터 예시:\")\n",
    "\n",
    "    for word in preprocessed_texts:\n",
    "        if word in glove_model:\n",
    "            vector = glove_model[word]\n",
    "            print(f\"  📝 '{word}' 벡터:\")\n",
    "            print(f\"    전체 차원: {len(vector)}차원\")\n",
    "            print(f\"    처음 10차원: {vector[:10]}\")\n",
    "            print(f\"    벡터 크기: {np.linalg.norm(vector):.4f}\")\n",
    "            print(f\"    최대값: {np.max(vector):.4f}, 최소값: {np.min(vector):.4f}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"  ❌ '{word}'는 어휘에 없습니다.\")\n",
    "\n",
    "demonstrate_word_vectors(preprocessed_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3f18926-0f1c-4b7b-8807-628c3029121c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📐 단어 간 유사도 계산:\n",
      "유사도가 높을수록 의미적으로 관련이 깊은 단어들입니다.\n",
      "\n",
      "💡 단어 쌍별 유사도:\n",
      "  'artificial' ↔ 'intelligence': 0.1664\n",
      "    → 낮은 유사도 (관련성 적음)\n",
      "\n",
      "  'computer' ↔ 'technology': 0.8526\n",
      "    → 매우 높은 유사도 (강한 관련성)\n",
      "\n",
      "  'language' ↔ 'communication': 0.5878\n",
      "    → 높은 유사도 (관련성 있음)\n",
      "\n",
      "  'learning' ↔ 'education': 0.7518\n",
      "    → 매우 높은 유사도 (강한 관련성)\n",
      "\n",
      "  'deep' ↔ 'shallow': 0.7442\n",
      "    → 매우 높은 유사도 (강한 관련성)\n",
      "\n",
      "  'king' ↔ 'queen': 0.7839\n",
      "    → 매우 높은 유사도 (강한 관련성)\n",
      "\n",
      "  'man' ↔ 'woman': 0.8860\n",
      "    → 매우 높은 유사도 (강한 관련성)\n",
      "\n",
      "  'car' ↔ 'vehicle': 0.8834\n",
      "    → 매우 높은 유사도 (강한 관련성)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 의미적으로 관련된 단어쌍들\n",
    "word_pairs = [\n",
    "    ('artificial', 'intelligence'),\n",
    "    ('computer', 'technology'),\n",
    "    ('language', 'communication'),\n",
    "    ('learning', 'education'),\n",
    "    ('deep', 'shallow'),\n",
    "    ('king', 'queen'),\n",
    "    ('man', 'woman'),\n",
    "    ('car', 'vehicle')\n",
    "]\n",
    "\n",
    "def calculate_similarities(word_pairs):\n",
    "    \"\"\"단어 간 유사도 계산\"\"\"\n",
    "    print(\"📐 단어 간 유사도 계산:\")\n",
    "    print(\"유사도가 높을수록 의미적으로 관련이 깊은 단어들입니다.\")\n",
    "    print()\n",
    "\n",
    "    print(\"💡 단어 쌍별 유사도:\")\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in glove_model and word2 in glove_model:\n",
    "            similarity = glove_model.similarity(word1, word2)\n",
    "            print(f\"  '{word1}' ↔ '{word2}': {similarity:.4f}\")\n",
    "            \n",
    "            # 유사도 해석\n",
    "            if similarity > 0.7:\n",
    "                print(f\"    → 매우 높은 유사도 (강한 관련성)\")\n",
    "            elif similarity > 0.5:\n",
    "                print(f\"    → 높은 유사도 (관련성 있음)\")\n",
    "            elif similarity > 0.3:\n",
    "                print(f\"    → 중간 유사도 (약간 관련)\")\n",
    "            else:\n",
    "                print(f\"    → 낮은 유사도 (관련성 적음)\")\n",
    "        else:\n",
    "            missing_words = [w for w in [word1, word2] if w not in glove_model]\n",
    "            print(f\"  '{word1}' ↔ '{word2}': 계산 불가 (어휘에 없음: {missing_words})\")\n",
    "        print()\n",
    "\n",
    "calculate_similarities(word_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1348e-6dbd-4b54-a186-e8e97f0dac31",
   "metadata": {},
   "source": [
    "벡터 간 유사도 계산이라는 기본 원리를 다양한 실전 문제에 응용\n",
    "- 검색 시스템 - 사용자가 \"노트북\" 검색해도 \"laptop\" 문서 찾아줌\n",
    "- 추천 시스템 - 넷플릭스, 스포티파이 같은 개인화 추천\n",
    "- 감정 분석 - 고객 리뷰, SNS 모니터링 자동화\n",
    "- 챗봇 - 다양한 표현을 이해하는 스마트 고객 서비스\n",
    "- 사기 탐지 - 피싱 메일, 허위 리뷰 자동 감지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31dbc2c1-0868-430e-b8a4-fbb588ad2f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 유사 단어 찾기:\n",
      "각 단어와 의미적으로 가장 가까운 단어들을 찾아보겠습니다.\n",
      "\n",
      "📝 'computer'와 가장 유사한 단어들:\n",
      "  1. 'computers': 0.9165\n",
      "  2. 'software': 0.8815\n",
      "  3. 'technology': 0.8526\n",
      "  4. 'electronic': 0.8126\n",
      "  5. 'internet': 0.8060\n",
      "\n",
      "📝 'artificial'와 가장 유사한 단어들:\n",
      "  1. 'natural': 0.7425\n",
      "  2. 'tissue': 0.7081\n",
      "  3. 'synthetic': 0.7078\n",
      "  4. 'developed': 0.6942\n",
      "  5. 'therapeutic': 0.6857\n",
      "\n",
      "📝 'language'와 가장 유사한 단어들:\n",
      "  1. 'languages': 0.8815\n",
      "  2. 'word': 0.8100\n",
      "  3. 'spoken': 0.8075\n",
      "  4. 'vocabulary': 0.7903\n",
      "  5. 'translation': 0.7879\n",
      "\n",
      "📝 'learning'와 가장 유사한 단어들:\n",
      "  1. 'teaching': 0.8757\n",
      "  2. 'skills': 0.8351\n",
      "  3. 'experience': 0.8201\n",
      "  4. 'practical': 0.8190\n",
      "  5. 'knowledge': 0.8090\n",
      "\n",
      "📝 'science'와 가장 유사한 단어들:\n",
      "  1. 'sciences': 0.8548\n",
      "  2. 'research': 0.8437\n",
      "  3. 'institute': 0.8386\n",
      "  4. 'studies': 0.8369\n",
      "  5. 'physics': 0.8314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_words = ['computer', 'artificial', 'language', 'learning', 'science']\n",
    "\n",
    "def find_similar_words(target_words):\n",
    "    \"\"\"유사 단어 찾기\"\"\"\n",
    "    print(\"🔎 유사 단어 찾기:\")\n",
    "    print(\"각 단어와 의미적으로 가장 가까운 단어들을 찾아보겠습니다.\")\n",
    "    print()\n",
    "    \n",
    "    for word in target_words:\n",
    "        if word in glove_model:\n",
    "            print(f\"📝 '{word}'와 가장 유사한 단어들:\")\n",
    "            try:\n",
    "                # 상위 5개 유사 단어 찾기\n",
    "                similar_words = glove_model.most_similar(word, topn=5)\n",
    "                \n",
    "                for rank, (similar_word, score) in enumerate(similar_words, 1):\n",
    "                    print(f\"  {rank}. '{similar_word}': {score:.4f}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  오류 발생: {e}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"❌ '{word}'는 어휘에 없습니다.\")\n",
    "            print()\n",
    "\n",
    "find_similar_words(target_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68444ae7-665b-4e3c-9f1b-3633fc7ee7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 단어 유추 (Word Analogies):\n",
      "GloVe의 놀라운 능력: 단어 간 관계를 벡터 연산으로 표현!\n",
      "형태: A : B = C : ?  (A가 B에 대응되는 것처럼, C는 무엇에 대응될까?)\n",
      "\n",
      "🔬 단어 유추 실험:\n",
      "  🎯 'king' - 'man' + 'queen' = ?\n",
      "    결과 후보:\n",
      "      1. 'coronation': 0.7995\n",
      "      2. 'hrh': 0.7570\n",
      "      3. 'throne': 0.7358\n",
      "    💭 기대한 답은 'woman'이었지만 다른 결과가 나왔습니다.\n",
      "\n",
      "  🎯 'paris' - 'france' + 'london' = ?\n",
      "    결과 후보:\n",
      "      1. 'opened': 0.7355\n",
      "      2. 'at': 0.7293\n",
      "      3. 'hotel': 0.7116\n",
      "    💭 기대한 답은 'england'이었지만 다른 결과가 나왔습니다.\n",
      "\n",
      "  🎯 'good' - 'better' + 'bad' = ?\n",
      "    결과 후보:\n",
      "      1. 'little': 0.8396\n",
      "      2. 'luck': 0.8365\n",
      "      3. 'thing': 0.8204\n",
      "    💭 기대한 답은 'worse'이었지만 다른 결과가 나왔습니다.\n",
      "\n",
      "  🎯 'walking' - 'walked' + 'running' = ?\n",
      "    결과 후보:\n",
      "      1. 'turning': 0.7794\n",
      "      2. 'track': 0.7608\n",
      "      3. 'course': 0.7578\n",
      "    💭 기대한 답은 'ran'이었지만 다른 결과가 나왔습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 유명한 단어 유추 예제들\n",
    "analogies = [\n",
    "    ('king', 'man', 'queen'),      # king - man + woman = queen 의도\n",
    "    ('paris', 'france', 'london'), # paris - france + england = london 의도  \n",
    "    ('good', 'better', 'bad'),     # good - better + worse = bad 의도\n",
    "    ('walking', 'walked', 'running'), # walking - walked + ran = running 의도\n",
    "]\n",
    "\n",
    "def demonstrate_word_analogies(analogies):\n",
    "    \"\"\"단어 유추 (Word Analogies) 데모\"\"\"\n",
    "    print(\"🧮 단어 유추 (Word Analogies):\")\n",
    "    print(\"GloVe의 놀라운 능력: 단어 간 관계를 벡터 연산으로 표현!\")\n",
    "    print(\"형태: A : B = C : ?  (A가 B에 대응되는 것처럼, C는 무엇에 대응될까?)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔬 단어 유추 실험:\")\n",
    "    for word1, word2, word3 in analogies:\n",
    "        # 모든 단어가 어휘에 있는지 확인\n",
    "        if all(word in glove_model for word in [word1, word2, word3]):\n",
    "            try:\n",
    "                # word1 - word2 + word3 와 가장 유사한 단어 찾기\n",
    "                # positive=[word1, word3]: 더하는 벡터들\n",
    "                # negative=[word2]: 빼는 벡터\n",
    "                result = glove_model.most_similar(\n",
    "                    positive=[word1, word3], \n",
    "                    negative=[word2], \n",
    "                    topn=3\n",
    "                )\n",
    "                \n",
    "                print(f\"  🎯 '{word1}' - '{word2}' + '{word3}' = ?\")\n",
    "                print(f\"    결과 후보:\")\n",
    "                for rank, (result_word, score) in enumerate(result, 1):\n",
    "                    print(f\"      {rank}. '{result_word}': {score:.4f}\")\n",
    "                \n",
    "                # 첫 번째 결과가 의도한 답인지 확인\n",
    "                if result:\n",
    "                    best_answer = result[0][0]\n",
    "                    expected_answers = {\n",
    "                        ('king', 'man', 'queen'): 'woman',\n",
    "                        ('paris', 'france', 'london'): 'england', \n",
    "                        ('good', 'better', 'bad'): 'worse',\n",
    "                        ('walking', 'walked', 'running'): 'ran'\n",
    "                    }\n",
    "                    expected = expected_answers.get((word1, word2, word3))\n",
    "                    if expected and expected in [r[0] for r in result[:3]]:\n",
    "                        print(f\"    ✅ 기대한 답 '{expected}'이 상위 3개 안에 있습니다!\")\n",
    "                    else:\n",
    "                        print(f\"    💭 기대한 답은 '{expected}'이었지만 다른 결과가 나왔습니다.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ '{word1}' - '{word2}' + '{word3}': 계산 실패 ({e})\")\n",
    "            print()\n",
    "        else:\n",
    "            missing = [w for w in [word1, word2, word3] if w not in glove_model]\n",
    "            print(f\"  ❌ '{word1}' - '{word2}' + '{word3}': 어휘에 없는 단어 {missing}\")\n",
    "            print()\n",
    "\n",
    "demonstrate_word_analogies(analogies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d86f34-bebc-4f5e-b6af-a353ab875855",
   "metadata": {},
   "source": [
    "#### 완벽한 답이 안나오는 이유?\n",
    "1. 모델 크기의 한계\n",
    "- 우리가 사용한 glove-wiki-gigaword-50은 50차원짜리 작은 모델\n",
    "- 유명한 \"king - man + woman = queen\" 예제는 보통 300차원 모델에서 잘 작동\n",
    "- 차원이 적으면 미묘한 의미 관계를 제대로 포착하기 어려움\n",
    "\n",
    "2. 벡터 연산의 한계\n",
    "- 벡터 공간에서는 여러 의미 관계가 복합적으로 얽혀있음\n",
    "- \"king - man + woman\"이 정확히 \"queen\"만을 가리키지 않음\n",
    "- 왕실, 권력, 성별 등 여러 개념이 동시에 활성화됨\n",
    "\n",
    "'king' - 'man' + 'queen' = ?<br>\n",
    "결과: 'coronation', 'hrh', 'throne'<br>\n",
    "→ 실제로는 '왕권', '왕실', '왕좌' 관련 단어들!<br>\n",
    "→ 의미적으로는 맞는 답! 단지 우리가 원한 답이 아닐 뿐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd07bd0f-43bb-4023-8ed4-9776b7d99e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖️ Word2Vec vs GloVe 성능 비교:\n",
      "==================================================\n",
      "📊 공통 어휘: 47개 단어\n",
      "\n",
      "📐 유사도 비교 (상위 6개 쌍):\n",
      "단어쌍                  Word2Vec     GloVe        차이        \n",
      "------------------------------------------------------------\n",
      "models-learning      0.1707       0.4946       0.3239    \n",
      "models-semantic      0.2123       0.4127       0.2003    \n",
      "learning-semantic    -0.1680      0.4759       0.6439    \n",
      "learning-many        0.0347       0.6012       0.5665    \n",
      "semantic-many        0.0794       0.1504       0.0711    \n",
      "semantic-fascinating -0.0221      0.3300       0.3521    \n"
     ]
    }
   ],
   "source": [
    "def compare_with_word2vec(w2v_model, glove_model):\n",
    "    \"\"\"Word2Vec과 GloVe 비교\"\"\"\n",
    "    print(\"⚖️ Word2Vec vs GloVe 성능 비교:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 공통 단어로 비교 (둘 다 있는 단어 찾기)\n",
    "    w2v_vocab = set(w2v_model.wv.index_to_key)\n",
    "    glove_vocab = set(glove_model.index_to_key)\n",
    "    common_words = w2v_vocab.intersection(glove_vocab)\n",
    "    \n",
    "    if len(common_words) >= 2:\n",
    "        print(f\"📊 공통 어휘: {len(common_words)}개 단어\")\n",
    "        \n",
    "        # 몇 개 단어 쌍의 유사도 비교\n",
    "        test_pairs = []\n",
    "        common_list = list(common_words)\n",
    "        for i in range(min(3, len(common_list))):\n",
    "            for j in range(i+1, min(i+3, len(common_list))):\n",
    "                test_pairs.append((common_list[i], common_list[j]))\n",
    "        \n",
    "        print(f\"\\n📐 유사도 비교 (상위 {len(test_pairs)}개 쌍):\")\n",
    "        print(f\"{'단어쌍':<20} {'Word2Vec':<12} {'GloVe':<12} {'차이':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for word1, word2 in test_pairs:\n",
    "            try:\n",
    "                w2v_sim = w2v_model.wv.similarity(word1, word2)\n",
    "                glove_sim = glove_model.similarity(word1, word2)\n",
    "                diff = abs(w2v_sim - glove_sim)\n",
    "                \n",
    "                pair_name = f\"{word1}-{word2}\"\n",
    "                print(f\"{pair_name:<20} {w2v_sim:<12.4f} {glove_sim:<12.4f} {diff:<10.4f}\")\n",
    "            except:\n",
    "                print(f\"{word1}-{word2:<15} 계산 오류\")\n",
    "    else:\n",
    "        print(\"Word2Vec과 GloVe 간 공통 어휘가 부족합니다.\")\n",
    "        \n",
    "compare_with_word2vec(w2v_model, glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092e608-4455-44bd-be1c-fb2874fb2952",
   "metadata": {},
   "source": [
    "### 7. FastText (Fast Text Representation)\n",
    "FastText 개념:\n",
    "- Facebook에서 개발한 단어 임베딩 기법\n",
    "- Word2Vec의 진화된 버전\n",
    "- 핵심 차이점: 하위 단어(subword) 정보를 활용\n",
    "- 단어를 문자 n-gram으로 분해해서 학습\n",
    "\n",
    "FastText의 혁신적 아이디어:\n",
    "- Word2Vec: \"apple\" → 하나의 벡터\n",
    "- FastText: \"apple\" → \"<ap\", \"app\", \"ppl\", \"ple\", \"le>\" + \"apple\" 전체\n",
    "- 장점: 훈련에 없던 단어도 하위 단어 조합으로 벡터 생성 가능!\n",
    "\n",
    "하위 단어(Subword) 예시:\n",
    "- \"running\" → \"<ru\", \"run\", \"unn\", \"nni\", \"nin\", \"ing\", \"ng>\"\n",
    "- \"unknown\" (새 단어) → \"<un\", \"unk\", \"nkn\", \"kno\", \"now\", \"own\", \"wn>\"\n",
    "- 비슷한 철자 패턴 → 비슷한 의미 추론\n",
    "\n",
    "<br>\n",
    "\n",
    "FastText vs Word2Vec 비교:\n",
    "<br>\n",
    "\n",
    "Word2Vec의 한계:\n",
    "OOV (Out-of-Vocabulary) 문제:\n",
    "- 훈련 데이터에 없는 단어 → 벡터 생성 불가\n",
    "- 예: 'smartphone' 학습했지만 'smartphones' 없으면 처리 못함\n",
    "- 오타, 신조어, 전문용어 처리 어려움\n",
    "\n",
    "<br>\n",
    "\n",
    "FastText의 해결책:\n",
    "하위 단어 기반 학습:\n",
    "- 단어를 작은 조각들로 분해\n",
    "- 조각들의 조합으로 새로운 단어 벡터 생성\n",
    "- 예: 'run' + 'ning' 패턴으로 'running' 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae3a67ab-4c76-46b0-83aa-f607e4027c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 하위 단어 분해 과정 시연:\n",
      "----------------------------------------\n",
      "📝 단어별 하위 단어 분해 (n-gram=3~6):\n",
      "\n",
      "  'natural' 분해:\n",
      "    마커 추가: <natural>\n",
      "    하위 단어들: ['<na', 'nat', 'atu', 'tur', 'ura', 'ral', 'al>', '<nat', 'natu', 'atur']...\n",
      "    총 23개 조각\n",
      "\n",
      "  'language' 분해:\n",
      "    마커 추가: <language>\n",
      "    하위 단어들: ['<la', 'lan', 'ang', 'ngu', 'gua', 'uag', 'age', 'ge>', '<lan', 'lang']...\n",
      "    총 27개 조각\n",
      "\n",
      "  'processing' 분해:\n",
      "    마커 추가: <processing>\n",
      "    하위 단어들: ['<pr', 'pro', 'roc', 'oce', 'ces', 'ess', 'ssi', 'sin', 'ing', 'ng>']...\n",
      "    총 35개 조각\n",
      "\n",
      "  'fascinating' 분해:\n",
      "    마커 추가: <fascinating>\n",
      "    하위 단어들: ['<fa', 'fas', 'asc', 'sci', 'cin', 'ina', 'nat', 'ati', 'tin', 'ing']...\n",
      "    총 39개 조각\n",
      "\n",
      "  'field' 분해:\n",
      "    마커 추가: <field>\n",
      "    하위 단어들: ['<fi', 'fie', 'iel', 'eld', 'ld>', '<fie', 'fiel', 'ield', 'eld>', '<fiel']...\n",
      "    총 15개 조각\n",
      "\n",
      "  'artificial' 분해:\n",
      "    마커 추가: <artificial>\n",
      "    하위 단어들: ['<ar', 'art', 'rti', 'tif', 'ifi', 'fic', 'ici', 'cia', 'ial', 'al>']...\n",
      "    총 35개 조각\n",
      "\n",
      "  'intelligence' 분해:\n",
      "    마커 추가: <intelligence>\n",
      "    하위 단어들: ['<in', 'int', 'nte', 'tel', 'ell', 'lli', 'lig', 'ige', 'gen', 'enc']...\n",
      "    총 43개 조각\n"
     ]
    }
   ],
   "source": [
    "def show_subword_process(example_words):\n",
    "    \"\"\"하위 단어 분해 과정 시연\"\"\"\n",
    "    print(f\"\\n🔍 하위 단어 분해 과정 시연:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 예시 단어들  \n",
    "    print(\"📝 단어별 하위 단어 분해 (n-gram=3~6):\")\n",
    "    \n",
    "    for word in example_words:\n",
    "        print(f\"\\n  '{word}' 분해:\")\n",
    "        \n",
    "        # 시작/끝 마커 추가\n",
    "        word_with_markers = f\"<{word}>\"\n",
    "        print(f\"    마커 추가: {word_with_markers}\")\n",
    "        \n",
    "        # 3-gram부터 6-gram까지 생성\n",
    "        subwords = []\n",
    "        \n",
    "        for n in range(3, min(7, len(word_with_markers) + 1)):\n",
    "            for i in range(len(word_with_markers) - n + 1):\n",
    "                subword = word_with_markers[i:i+n]\n",
    "                subwords.append(subword)\n",
    "        \n",
    "        # 전체 단어도 포함\n",
    "        subwords.append(word)\n",
    "        \n",
    "        print(f\"    하위 단어들: {subwords[:10]}...\")  # 처음 10개만 표시\n",
    "        print(f\"    총 {len(subwords)}개 조각\")\n",
    "\n",
    "show_subword_process(preprocessed_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95433668-4767-4f72-bc88-e6f5657d9547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 FastText 모델 학습 시작!\n",
      "==================================================\n",
      "🔄 FastText 모델 학습 중...\n",
      "⏱️  Word2Vec보다 약간 더 오래 걸릴 수 있습니다 (하위 단어 처리 때문)\n",
      "✅ FastText 모델 학습 완료!\n",
      "\n",
      "📊 FastText 모델 정보:\n",
      "  어휘 크기: 48개\n",
      "  벡터 차원: 100차원\n",
      "  학습된 단어들: ['nlp', 'text', 'models', 'language', 'learning', 'natural', 'many', 'process', 'revolutionized', 'tasks', 'deep', 'data', 'classification', 'amounts', 'large', 'analyze', 'algorithms', 'results', 'sentiment', 'machine', 'intelligence', 'artificial', 'field', 'fascinating', 'processing', 'analysis', 'understanding', 'preprocessing', 'essential', 'step', 'stateoftheart', 'achieve', 'gpt', 'bert', 'modern', 'architecture', 'dominant', 'become', 'transformers', 'words', 'relationships', 'semantic', 'capture', 'embeddings', 'word', 'pipeline', 'common', 'applications']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n🚀 FastText 모델 학습 시작!\")\n",
    "print(\"=\"*50)\n",
    "print(\"🔄 FastText 모델 학습 중...\")\n",
    "print(\"⏱️  Word2Vec보다 약간 더 오래 걸릴 수 있습니다 (하위 단어 처리 때문)\")\n",
    "\n",
    "fasttext_model = FastText(\n",
    "    sentences=preprocessed_texts,  # 학습 데이터 (토큰화된 문장들)\n",
    "    vector_size=100,              # 임베딩 벡터 차원 수\n",
    "    window=5,                     # 컨텍스트 윈도우 크기\n",
    "    min_count=1,                  # 최소 출현 빈도 (데이터가 적어서 1로 설정)\n",
    "    workers=4,                    # 병렬 처리 워커 수\n",
    "    sg=0,                        # 0=CBOW, 1=Skip-gram\n",
    "    epochs=10                    # 학습 반복 횟수\n",
    ")\n",
    "\n",
    "print(\"✅ FastText 모델 학습 완료!\")\n",
    "\n",
    "print(f\"\\n📊 FastText 모델 정보:\")\n",
    "print(f\"  어휘 크기: {len(fasttext_model.wv)}개\")\n",
    "print(f\"  벡터 차원: {fasttext_model.wv.vector_size}차원\")\n",
    "print(f\"  학습된 단어들: {list(fasttext_model.wv.index_to_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "844d3551-844a-46bb-9a31-63a60dcd8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 기본 기능 확인:\n",
      "----------------------------------------\n",
      "📝 'nlp' 벡터:\n",
      "  전체 차원: 100차원\n",
      "  처음 10차원: [ 0.00229694 -0.00158578  0.00267106  0.00239871 -0.0020173   0.00127126\n",
      "  0.00130897  0.00026072 -0.00232229 -0.00022632]\n",
      "  벡터 크기: 0.0253\n",
      "\n",
      "📐 단어 유사도:\n",
      "  'nlp' ↔ 'text': -0.0781\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_basic_functionality(fasttext_model):\n",
    "    \"\"\"기본 기능 시연\"\"\"\n",
    "    print(f\"\\n🔍 기본 기능 확인:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 학습된 단어의 벡터 확인\n",
    "    vocab = list(fasttext_model.wv.index_to_key)\n",
    "    \n",
    "    if vocab:\n",
    "        test_word = vocab[0]\n",
    "        vector = fasttext_model.wv[test_word]\n",
    "        print(f\"📝 '{test_word}' 벡터:\")\n",
    "        print(f\"  전체 차원: {len(vector)}차원\")\n",
    "        print(f\"  처음 10차원: {vector[:10]}\")\n",
    "        print(f\"  벡터 크기: {np.linalg.norm(vector):.4f}\")\n",
    "    \n",
    "    # 단어 유사도 계산\n",
    "    if len(vocab) >= 2:\n",
    "        word1, word2 = vocab[0], vocab[1]\n",
    "        try:\n",
    "            similarity = fasttext_model.wv.similarity(word1, word2)\n",
    "            print(f\"\\n📐 단어 유사도:\")\n",
    "            print(f\"  '{word1}' ↔ '{word2}': {similarity:.4f}\")\n",
    "        except:\n",
    "            print(f\"  유사도 계산 실패\")\n",
    "\n",
    "demonstrate_basic_functionality(fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d5a68fc-6c48-43c9-80cc-e565a48eb422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 FastText의 핵심 기능: OOV 단어 처리\n",
      "==================================================\n",
      "🔍 OOV (Out-of-Vocabulary) 문제란?\n",
      "  - 훈련 데이터에 없던 새로운 단어를 만났을 때\n",
      "  - Word2Vec: '단어를 모르겠다' → 오류\n",
      "  - FastText: '하위 단어로 추론해볼게' → 벡터 생성!\n",
      "\n",
      "📚 훈련된 어휘: {'models', 'learning', 'semantic', 'many', 'fascinating', 'architecture', 'natural', 'language', 'classification', 'modern', 'artificial', 'essential', 'results', 'intelligence', 'relationships', 'understanding', 'become', 'step', 'embeddings', 'word', 'field', 'words', 'dominant', 'preprocessing', 'bert', 'process', 'processing', 'analyze', 'achieve', 'applications', 'transformers', 'machine', 'gpt', 'common', 'sentiment', 'deep', 'pipeline', 'stateoftheart', 'algorithms', 'amounts', 'capture', 'text', 'revolutionized', 'nlp', 'analysis', 'tasks', 'data', 'large'}\n",
      "\n",
      "🧪 OOV 단어 테스트:\n",
      "훈련 데이터에 없는 단어들도 벡터 생성이 가능한지 확인해보겠습니다.\n",
      "\n",
      "✅ 'artificialintelligence' (OOV 단어):\n",
      "  벡터 생성 성공! 차원: 100\n",
      "  처음 5차원: [ 0.0001186   0.00144737  0.00098309 -0.00015574  0.00026451]\n",
      "  벡터 크기: 0.0056\n",
      "  가장 유사한 훈련 단어: 'semantic' (-0.1166)\n",
      "\n",
      "✅ 'machinelearning' (OOV 단어):\n",
      "  벡터 생성 성공! 차원: 100\n",
      "  처음 5차원: [ 5.9420051e-04  2.0748317e-05 -1.9436714e-04 -8.5463328e-04\n",
      " -1.5398222e-03]\n",
      "  벡터 크기: 0.0077\n",
      "  가장 유사한 훈련 단어: 'learning' (0.5638)\n",
      "\n",
      "✅ 'deeplearning' (OOV 단어):\n",
      "  벡터 생성 성공! 차원: 100\n",
      "  처음 5차원: [ 1.2643914e-03  4.4824515e-05 -1.5857724e-03  5.1677309e-04\n",
      "  1.7969652e-04]\n",
      "  벡터 크기: 0.0087\n",
      "  가장 유사한 훈련 단어: 'learning' (0.6751)\n",
      "\n",
      "✅ 'naturallanguage' (OOV 단어):\n",
      "  벡터 생성 성공! 차원: 100\n",
      "  처음 5차원: [-4.3414647e-04 -1.6823537e-03  1.3413278e-03 -9.1364775e-05\n",
      " -2.1338665e-04]\n",
      "  벡터 크기: 0.0083\n",
      "  가장 유사한 훈련 단어: 'semantic' (-0.0011)\n",
      "\n",
      "✅ 'smartphone' (OOV 단어):\n",
      "  벡터 생성 성공! 차원: 100\n",
      "  처음 5차원: [ 0.00082182 -0.00043519  0.0006153   0.00166517  0.00084064]\n",
      "  벡터 크기: 0.0103\n",
      "  가장 유사한 훈련 단어: 'learning' (-0.0228)\n",
      "\n",
      "✅ 'neuralnetwork' (OOV 단어):\n",
      "  벡터 생성 성공! 차원: 100\n",
      "  처음 5차원: [-7.3950487e-04 -3.9964975e-06 -4.3781038e-04 -2.6377998e-04\n",
      " -5.9242075e-04]\n",
      "  벡터 크기: 0.0079\n",
      "  가장 유사한 훈련 단어: 'models' (-0.0185)\n"
     ]
    }
   ],
   "source": [
    "# OOV 단어들 테스트\n",
    "oov_test_words = [\n",
    "    \"artificialintelligence\",  # 'artificial' + 'intelligence' 합성\n",
    "    \"machinelearning\",         # 'machine' + 'learning' 합성  \n",
    "    \"deeplearning\",           # 'deep' + 'learning' 합성\n",
    "    \"naturallanguage\",        # 'natural' + 'language' 합성\n",
    "    \"smartphone\",             # 일반적인 신조어\n",
    "    \"neuralnetwork\"           # 'neural' + 'network' 합성\n",
    "]\n",
    "\n",
    "def demonstrate_oov_handling(oov_test_words):\n",
    "    \"\"\"OOV (미등록 단어) 처리 시연 - FastText의 핵심 기능!\"\"\"\n",
    "    print(f\"\\n🎯 FastText의 핵심 기능: OOV 단어 처리\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"🔍 OOV (Out-of-Vocabulary) 문제란?\")\n",
    "    print(\"  - 훈련 데이터에 없던 새로운 단어를 만났을 때\")\n",
    "    print(\"  - Word2Vec: '단어를 모르겠다' → 오류\")\n",
    "    print(\"  - FastText: '하위 단어로 추론해볼게' → 벡터 생성!\")\n",
    "    \n",
    "    # 훈련 데이터에 있는 단어 확인\n",
    "    vocab = set(fasttext_model.wv.index_to_key)\n",
    "    print(f\"\\n📚 훈련된 어휘: {vocab}\")\n",
    "    print(f\"\\n🧪 OOV 단어 테스트:\")\n",
    "    print(\"훈련 데이터에 없는 단어들도 벡터 생성이 가능한지 확인해보겠습니다.\")\n",
    "    \n",
    "    for oov_word in oov_test_words:\n",
    "        try:\n",
    "            # FastText는 훈련에 없던 단어도 벡터 생성 가능!\n",
    "            oov_vector = fasttext_model.wv[oov_word]\n",
    "            \n",
    "            print(f\"\\n✅ '{oov_word}' (OOV 단어):\")\n",
    "            print(f\"  벡터 생성 성공! 차원: {len(oov_vector)}\")\n",
    "            print(f\"  처음 5차원: {oov_vector[:5]}\")\n",
    "            print(f\"  벡터 크기: {np.linalg.norm(oov_vector):.4f}\")\n",
    "            \n",
    "            # 훈련된 단어들과 유사도 계산\n",
    "            if vocab:\n",
    "                similarities = []\n",
    "                for known_word in list(vocab)[:3]:  # 처음 3개 단어와 비교\n",
    "                    try:\n",
    "                        sim = fasttext_model.wv.similarity(oov_word, known_word)\n",
    "                        similarities.append((known_word, sim))\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if similarities:\n",
    "                    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "                    print(f\"  가장 유사한 훈련 단어: '{similarities[0][0]}' ({similarities[0][1]:.4f})\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ '{oov_word}': 벡터 생성 실패 - {e}\")\n",
    "\n",
    "demonstrate_oov_handling(oov_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45aa1c6a-b1ec-43a1-8f2f-a5ee4b3c6638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚖️ Word2Vec vs FastText: OOV 처리 비교\n",
      "==================================================\n",
      "🧪 테스트 단어: 'artificialintelligence'\n",
      "(훈련 데이터에 없는 합성어)\n",
      "\n",
      "📊 Word2Vec 결과:\n",
      "  ❌ KeyError: 단어를 찾을 수 없습니다.\n",
      "  → Word2Vec은 훈련에 없던 단어를 처리할 수 없음\n",
      "\n",
      "📊 FastText 결과:\n",
      "  ✅ 벡터 생성 성공 (처음 5차원): [ 0.0001186   0.00144737  0.00098309 -0.00015574  0.00026451]\n",
      "  → FastText는 하위 단어 조합으로 벡터 생성!\n",
      "  🔍 추론 과정:\n",
      "    'artificial' 부분의 하위 단어들: <ar, art, rti, tif, ...\n",
      "    'intelligence' 부분의 하위 단어들: int, nte, tel, ell, ...\n",
      "    → 이런 조각들의 조합으로 전체 단어 의미 추론\n"
     ]
    }
   ],
   "source": [
    "test_oov_word = \"artificialintelligence\"\n",
    "\n",
    "def compare_with_word2vec(test_oov_word):\n",
    "    \"\"\"Word2Vec과 FastText OOV 처리 비교\"\"\"\n",
    "    print(f\"\\n⚖️ Word2Vec vs FastText: OOV 처리 비교\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"🧪 테스트 단어: '{test_oov_word}'\")\n",
    "    print(\"(훈련 데이터에 없는 합성어)\")\n",
    "    \n",
    "    # Word2Vec 테스트\n",
    "    if 'w2v_model' in globals():\n",
    "        print(f\"\\n📊 Word2Vec 결과:\")\n",
    "        try:\n",
    "            w2v_vector = w2v_model.wv[test_oov_word]\n",
    "            print(f\"  ✅ 벡터 생성 성공 (처음 5차원): {w2v_vector[:5]}\")\n",
    "        except KeyError:\n",
    "            print(f\"  ❌ KeyError: 단어를 찾을 수 없습니다.\")\n",
    "            print(f\"  → Word2Vec은 훈련에 없던 단어를 처리할 수 없음\")\n",
    "    else:\n",
    "        print(f\"\\n📊 Word2Vec 모델이 없어 비교할 수 없습니다.\")\n",
    "    \n",
    "    # FastText 테스트\n",
    "    print(f\"\\n📊 FastText 결과:\")\n",
    "    try:\n",
    "        ft_vector = fasttext_model.wv[test_oov_word]\n",
    "        print(f\"  ✅ 벡터 생성 성공 (처음 5차원): {ft_vector[:5]}\")\n",
    "        print(f\"  → FastText는 하위 단어 조합으로 벡터 생성!\")\n",
    "        \n",
    "        # 어떤 하위 단어들이 사용되었는지 추론\n",
    "        print(f\"  🔍 추론 과정:\")\n",
    "        print(f\"    'artificial' 부분의 하위 단어들: <ar, art, rti, tif, ...\")\n",
    "        print(f\"    'intelligence' 부분의 하위 단어들: int, nte, tel, ell, ...\")\n",
    "        print(f\"    → 이런 조각들의 조합으로 전체 단어 의미 추론\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 오류: {e}\")\n",
    "\n",
    "compare_with_word2vec(test_oov_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4987a84-a464-487c-9cf1-409833fff1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 FastText 실습 요약:\n",
      "==================================================\n",
      "📈 학습 결과:\n",
      "  어휘 크기: 48개 단어\n",
      "  벡터 차원: 100차원\n",
      "  핵심 특징: 하위 단어 기반 OOV 처리\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n💡 FastText 실습 요약:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"📈 학습 결과:\")\n",
    "print(f\"  어휘 크기: {len(fasttext_model.wv)}개 단어\")\n",
    "print(f\"  벡터 차원: {fasttext_model.wv.vector_size}차원\")\n",
    "print(f\"  핵심 특징: 하위 단어 기반 OOV 처리\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d165f-109a-439c-aeb3-265f8f6a8700",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fce562-1aa5-4a4e-b16f-4d47a5d49ca8",
   "metadata": {},
   "source": [
    "### 최근 텍스트 전처리 방법: Transformers (BERT 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c516b-660e-4d6e-b8d4-48306a366583",
   "metadata": {},
   "source": [
    "### 8. 현대적인 방법: Transformers (BERT)\n",
    "\n",
    "Transformers & BERT 소개:\n",
    "- 2017년 \"Attention Is All You Need\" 논문으로 시작된 혁명\n",
    "- BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- 기존 방법들과의 핵심 차이점: 문맥을 양방향으로 이해\n",
    "\n",
    "기존 방법들과의 비교:\n",
    "- Word2Vec/GloVe/FastText: 단어별 고정 벡터\n",
    "→ \"bank\" 단어는 항상 같은 벡터 (은행? 강둑?)\n",
    "  \n",
    "- BERT: 문맥에 따라 동적 벡터  \n",
    "→ \"I went to the bank\" vs \"river bank\"에서 다른 벡터!\n",
    "\n",
    "BERT의 혁신적 특징:\n",
    "- 1. 양방향성: 앞뒤 문맥을 모두 고려\n",
    "- 2. 문맥 의존성: 같은 단어도 문맥에 따라 다른 의미\n",
    "- 3. 전이 학습: 대용량 데이터로 미리 훈련 → 특정 태스크에 적용\n",
    "- 4. 문장 레벨 이해: 단어가 아닌 전체 문장의 의미 파악\n",
    "\n",
    "\n",
    "#### 왜 Transformers가 NLP 역사를 바꿨을까?\n",
    "성능의 비약적 향상:\n",
    "- 기존 모델: 특정 태스크별로 따로 설계\n",
    "- BERT: 하나의 모델로 여러 태스크 해결\n",
    "- 결과: 거의 모든 NLP 벤치마크에서 기록 경신\n",
    "\n",
    "인간과 유사한 언어 이해:\n",
    "- 단어 하나하나가 아니라 문장 전체 맥락 파악\n",
    "- '이 말이 이 상황에서 무슨 뜻일까?' 추론 가능\n",
    "- 암묵적 의미, 반어법, 문맥 의존적 의미도 어느 정도 이해\n",
    "\n",
    "전이 학습의 위력:\n",
    "- 1단계: 수십억 개 문장으로 언어 자체를 학습\n",
    "- 2단계: 특정 태스크 데이터로 미세 조정\n",
    "- 결과: 적은 데이터로도 높은 성능 달성\n",
    "\n",
    "#### BERT 구조 개요\n",
    "핵심 구성 요소:\n",
    "- 1. 토크나이저 (Tokenizer):\n",
    "     - 문장을 BERT가 이해할 수 있는 토큰으로 변환\n",
    "     - 예: 'Hello world' → ['[CLS]', 'Hello', 'world', '[SEP]']\n",
    "- 2. 임베딩 레이어:\n",
    "     - 토큰을 벡터로 변환 (단어 + 위치 + 문장 정보)\n",
    "- 3. 트랜스포머 블록들:\n",
    "     - 여러 층의 attention 메커니즘\n",
    "     - 각 단어가 다른 모든 단어들과 '대화'하며 의미 파악\n",
    "- 4. 출력:\n",
    "     - [CLS]: 전체 문장의 의미 (분류 태스크용)\n",
    "     - 각 토큰별: 문맥을 고려한 단어별 벡터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "411d4229-a6cc-4c41-ba1e-ba863ade096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6cd13-a99a-4497-97ef-db3b1448a44b",
   "metadata": {},
   "source": [
    "모델 선택: DistilBERT\n",
    "\n",
    "BERT 모델 크기 비교:\n",
    "- BERT-base: 110M 파라미터, 768차원, 12층\n",
    "- BERT-large: 340M 파라미터, 1024차원, 24층\n",
    "- DistilBERT: 66M 파라미터, 768차원, 6층 ← 우리가 사용\n",
    "\n",
    "- DistilBERT 선택 이유:\n",
    "  - 크기: BERT-base의 60% 크기\n",
    "  - 속도: 약 60% 빠름\n",
    "  - 성능: BERT-base의 97% 성능 유지\n",
    "  - 메모리: 일반 PC에서도 실행 가능\n",
    "<br>\n",
    "  → 학습용으로 최적!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "651cb4fa-93ad-475b-b2e3-fc8a113b59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 BERT 모델 로드 중...\n",
      "⏱️  처음 실행 시 인터넷에서 다운로드 (시간 소요)\n",
      "📥 사용 모델: distilbert-base-uncased\n",
      "  - distilbert: 경량화된 BERT\n",
      "  - base: 기본 크기 (large 대비)\n",
      "  - uncased: 대소문자 구분 안함\n",
      "\n",
      "1️⃣ 토크나이저 로드 중...\n",
      "✅ 토크나이저 로드 완료!\n",
      "\n",
      "2️⃣ BERT 모델 로드 중...\n",
      "✅ BERT 모델 로드 완료!\n",
      "\n",
      "📊 모델 정보:\n",
      "  모델 이름: distilbert-base-uncased\n",
      "  파라미터 수: 약 66M개\n",
      "  출력 차원: 768차원\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔄 BERT 모델 로드 중...\")\n",
    "print(\"⏱️  처음 실행 시 인터넷에서 다운로드 (시간 소요)\")\n",
    "\n",
    "# 모델 이름 지정\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "print(f\"📥 사용 모델: {model_name}\")\n",
    "print(\"  - distilbert: 경량화된 BERT\")\n",
    "print(\"  - base: 기본 크기 (large 대비)\")\n",
    "print(\"  - uncased: 대소문자 구분 안함\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"\\n1️⃣ 토크나이저 로드 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"✅ 토크나이저 로드 완료!\")\n",
    "\n",
    "# 모델 로드\n",
    "print(\"\\n2️⃣ BERT 모델 로드 중...\")\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(\"✅ BERT 모델 로드 완료!\")\n",
    "\n",
    "print(f\"\\n📊 모델 정보:\")\n",
    "print(f\"  모델 이름: {model_name}\")\n",
    "print(f\"  파라미터 수: 약 66M개\")\n",
    "print(f\"  출력 차원: 768차원\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a167c9cb-04c5-4950-bd50-63e5b8afbf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 토큰화 과정 시연\n",
      "------------------------------\n",
      "원본 문장: 'Natural language processing is amazing!'\n",
      "토큰화 결과: ['natural', 'language', 'processing', 'is', 'amazing', '!']\n",
      "토큰 ID: [3019, 2653, 6364, 2003, 6429, 999]\n",
      "특수 토큰 포함: tensor([[ 101, 3019, 2653, 6364, 2003, 6429,  999,  102]])\n",
      "디코딩 결과: '[CLS] natural language processing is amazing! [SEP]'\n",
      "\n",
      "💡 특수 토큰 설명:\n",
      "  [CLS]: 문장 시작 (Classification 토큰)\n",
      "  [SEP]: 문장 끝 (Separator 토큰)\n",
      "  [PAD]: 길이 맞춤용 패딩\n",
      "  [UNK]: 모르는 단어\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Natural language processing is amazing!\"\n",
    "\n",
    "def demonstrate_tokenization(example_text):\n",
    "    \"\"\"토큰화 과정 시연\"\"\"\n",
    "    print(\"\\n🔍 토큰화 과정 시연\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(f\"원본 문장: '{example_text}'\")\n",
    "    \n",
    "    # 토큰화 수행\n",
    "    tokens = tokenizer.tokenize(example_text)\n",
    "    print(f\"토큰화 결과: {tokens}\")\n",
    "    \n",
    "    # 토큰 ID로 변환\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"토큰 ID: {token_ids}\")\n",
    "    \n",
    "    # 특수 토큰 추가된 버전\n",
    "    encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "    print(f\"특수 토큰 포함: {encoded['input_ids']}\")\n",
    "    \n",
    "    # 디코딩 (복원)\n",
    "    decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "    print(f\"디코딩 결과: '{decoded}'\")\n",
    "    \n",
    "    print(\"\\n💡 특수 토큰 설명:\")\n",
    "    print(\"  [CLS]: 문장 시작 (Classification 토큰)\")\n",
    "    print(\"  [SEP]: 문장 끝 (Separator 토큰)\")\n",
    "    print(\"  [PAD]: 길이 맞춤용 패딩\")\n",
    "    print(\"  [UNK]: 모르는 단어\")\n",
    "\n",
    "demonstrate_tokenization(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceeec23b-d977-4f15-ae27-12b45791882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리할 문장들:\n",
      "  1. Natural language processing is a fascinating field of artificial intelligence.\n",
      "  2. Machine learning algorithms can process and analyze large amounts of text data.\n",
      "  3. Deep learning models have revolutionized natural language understanding.\n"
     ]
    }
   ],
   "source": [
    "test_sentences = sample_texts[:3]\n",
    "print(\"처리할 문장들:\")\n",
    "for i, sent in enumerate(test_sentences, 1):\n",
    "    print(f\"  {i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11fc3180-018b-46be-aeba-b19966d9b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 실제 BERT 임베딩 추출 시작\n",
      "==================================================\n",
      "\n",
      "🎯 BERT 임베딩 추출 함수 동작:\n",
      "  1단계: 각 문장을 토큰화\n",
      "  2단계: 토큰들을 BERT 모델에 입력\n",
      "  3단계: [CLS] 토큰의 출력을 문장 임베딩으로 사용\n",
      "\n",
      "📝 3개 문장 처리 중...\n",
      "  처리 중: 문장 1/3\n",
      "    원본: 'Natural language processing is a fascinating field of artificial intelligence.'\n",
      "    토큰 수: 13개\n",
      "    임베딩 차원: torch.Size([768])\n",
      "  처리 중: 문장 2/3\n",
      "    원본: 'Machine learning algorithms can process and analyze large amounts of text data.'\n",
      "    토큰 수: 15개\n",
      "    임베딩 차원: torch.Size([768])\n",
      "  처리 중: 문장 3/3\n",
      "    원본: 'Deep learning models have revolutionized natural language understanding.'\n",
      "    토큰 수: 12개\n",
      "    임베딩 차원: torch.Size([768])\n",
      "\n",
      "📊 BERT 임베딩 결과:\n",
      "  임베딩 배열 크기: (3, 768)\n",
      "  문장 수: 3개\n",
      "  각 임베딩 차원: 768차원\n",
      "\n",
      "🔍 첫 번째 문장 임베딩 상세:\n",
      "  전체 차원: 768\n",
      "  처음 10차원: [-0.2655637  -0.12168896 -0.39667323 -0.0114592  -0.04621242 -0.12133733\n",
      "  0.127688    0.3928875  -0.1328064  -0.50623184]\n",
      "  벡터 크기: 13.2355\n",
      "  최대값: 3.2072\n",
      "  최소값: -7.4128\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(texts, tokenizer, model):\n",
    "    \"\"\"\n",
    "    BERT 임베딩 추출 함수\n",
    "    \n",
    "    Args:\n",
    "        texts: 문장들의 리스트\n",
    "        tokenizer: BERT 토크나이저\n",
    "        model: BERT 모델\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: 각 문장의 임베딩 벡터들\n",
    "    \"\"\"\n",
    "    print(f\"\\n🎯 BERT 임베딩 추출 함수 동작:\")\n",
    "    print(\"  1단계: 각 문장을 토큰화\")\n",
    "    print(\"  2단계: 토큰들을 BERT 모델에 입력\")\n",
    "    print(\"  3단계: [CLS] 토큰의 출력을 문장 임베딩으로 사용\")\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # gradient 계산 비활성화 (추론 모드)\n",
    "    with torch.no_grad():\n",
    "        print(f\"\\n📝 {len(texts)}개 문장 처리 중...\")\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            print(f\"  처리 중: 문장 {i+1}/{len(texts)}\")\n",
    "            print(f\"    원본: '{text}'\")\n",
    "            \n",
    "            # 1단계: 토큰화 및 텐서 변환\n",
    "            inputs = tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\",     # PyTorch 텐서로 반환\n",
    "                padding=True,            # 길이 맞춤\n",
    "                truncation=True,         # 최대 길이 초과시 자름\n",
    "                max_length=512          # BERT 최대 입력 길이\n",
    "            )\n",
    "            \n",
    "            print(f\"    토큰 수: {inputs['input_ids'].shape[1]}개\")\n",
    "            \n",
    "            # 2단계: BERT 모델 실행\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # 3단계: [CLS] 토큰의 임베딩 추출\n",
    "            # last_hidden_state: [배치크기, 시퀀스길이, 차원수]\n",
    "            # [0]: 첫 번째 (유일한) 문장\n",
    "            # [0]: 첫 번째 토큰 ([CLS])\n",
    "            cls_embedding = outputs.last_hidden_state[0][0]\n",
    "            \n",
    "            print(f\"    임베딩 차원: {cls_embedding.shape}\")\n",
    "            \n",
    "            # numpy로 변환하여 저장\n",
    "            embeddings.append(cls_embedding.numpy())\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 실제 임베딩 추출 (첫 3개 문장만 - 시간 절약)\n",
    "print(f\"\\n🚀 실제 BERT 임베딩 추출 시작\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "bert_embeddings = get_bert_embeddings(test_sentences, tokenizer, model)\n",
    "\n",
    "print(f\"\\n📊 BERT 임베딩 결과:\")\n",
    "print(f\"  임베딩 배열 크기: {bert_embeddings.shape}\")\n",
    "print(f\"  문장 수: {bert_embeddings.shape[0]}개\")\n",
    "print(f\"  각 임베딩 차원: {bert_embeddings.shape[1]}차원\")\n",
    "\n",
    "print(f\"\\n🔍 첫 번째 문장 임베딩 상세:\")\n",
    "first_embedding = bert_embeddings[0]\n",
    "print(f\"  전체 차원: {len(first_embedding)}\")\n",
    "print(f\"  처음 10차원: {first_embedding[:10]}\")\n",
    "print(f\"  벡터 크기: {np.linalg.norm(first_embedding):.4f}\")\n",
    "print(f\"  최대값: {np.max(first_embedding):.4f}\")\n",
    "print(f\"  최소값: {np.min(first_embedding):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c082371-3858-41c1-a53e-7350b5dacb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📐 BERT 문장 유사도 분석\n",
      "----------------------------------------\n",
      "💡 코사인 유사도 해석:\n",
      "  1.0: 완전히 동일한 방향 (매우 유사)\n",
      "  0.0: 직교 (관련 없음)\n",
      "  -1.0: 완전히 반대 방향 (정반대)\n",
      "\n",
      "📊 3개 문장 간 유사도 매트릭스:\n",
      "     문장 1  문장 2  문장 3  \n",
      "문장 1  1.000   0.951   0.958  \n",
      "문장 2  0.951   1.000   0.951  \n",
      "문장 3  0.958   0.951   1.000  \n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    코사인 유사도 계산 함수\n",
    "    \n",
    "    Args:\n",
    "        a, b: 두 벡터\n",
    "        \n",
    "    Returns:\n",
    "        float: 코사인 유사도 (-1 ~ 1)\n",
    "    \"\"\"\n",
    "    # 코사인 유사도 = A·B / (|A| × |B|)\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "# 문장 간 유사도 분석\n",
    "print(f\"\\n📐 BERT 문장 유사도 분석\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"💡 코사인 유사도 해석:\")\n",
    "print(\"  1.0: 완전히 동일한 방향 (매우 유사)\")\n",
    "print(\"  0.0: 직교 (관련 없음)\")\n",
    "print(\"  -1.0: 완전히 반대 방향 (정반대)\")\n",
    "\n",
    "# 모든 문장 쌍의 유사도 계산\n",
    "n_sentences = len(bert_embeddings)\n",
    "\n",
    "print(f\"\\n📊 {n_sentences}개 문장 간 유사도 매트릭스:\")\n",
    "print(\"     \", end=\"\")\n",
    "for j in range(n_sentences):\n",
    "    print(f\"문장{j+1:2d}\", end=\"  \")\n",
    "print()\n",
    "\n",
    "for i in range(n_sentences):\n",
    "    print(f\"문장{i+1:2d}\", end=\" \")\n",
    "    for j in range(n_sentences):\n",
    "        if i == j:\n",
    "            sim = 1.0  # 자기 자신과는 완전 유사\n",
    "        else:\n",
    "            sim = cosine_similarity(bert_embeddings[i], bert_embeddings[j])\n",
    "        print(f\"{sim:6.3f}\", end=\"  \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3766253a-c807-46e6-a8ac-dbed40d0c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 가장 유사한 문장 쌍:\n",
      "  문장 1: 'Natural language processing is a fascinating field of artificial intelligence.'\n",
      "  문장 3: 'Deep learning models have revolutionized natural language understanding.'\n",
      "  유사도: 0.9581\n",
      "\n",
      "🔻 가장 다른 문장 쌍:\n",
      "  문장 2: 'Machine learning algorithms can process and analyze large amounts of text data.'\n",
      "  문장 3: 'Deep learning models have revolutionized natural language understanding.'\n",
      "  유사도: 0.9507\n"
     ]
    }
   ],
   "source": [
    "# 가장 유사한/다른 문장 쌍 찾기\n",
    "max_sim = -2.0\n",
    "min_sim = 2.0\n",
    "max_pair = None\n",
    "min_pair = None\n",
    "\n",
    "for i in range(n_sentences):\n",
    "    for j in range(i+1, n_sentences):\n",
    "        sim = cosine_similarity(bert_embeddings[i], bert_embeddings[j])\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            max_pair = (i, j)\n",
    "        if sim < min_sim:\n",
    "            min_sim = sim\n",
    "            min_pair = (i, j)\n",
    "\n",
    "if max_pair:\n",
    "    print(f\"\\n🏆 가장 유사한 문장 쌍:\")\n",
    "    print(f\"  문장 {max_pair[0]+1}: '{test_sentences[max_pair[0]]}'\")\n",
    "    print(f\"  문장 {max_pair[1]+1}: '{test_sentences[max_pair[1]]}'\")\n",
    "    print(f\"  유사도: {max_sim:.4f}\")\n",
    "\n",
    "if min_pair:\n",
    "    print(f\"\\n🔻 가장 다른 문장 쌍:\")\n",
    "    print(f\"  문장 {min_pair[0]+1}: '{test_sentences[min_pair[0]]}'\")\n",
    "    print(f\"  문장 {min_pair[1]+1}: '{test_sentences[min_pair[1]]}'\")\n",
    "    print(f\"  유사도: {min_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9f753-7fc3-4145-8de3-046013e80499",
   "metadata": {},
   "source": [
    "#### BERT vs 전통적 방법들 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "346d8c7c-5f86-4751-a16b-1ac68c3d884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 임베딩 차원 비교:\n",
      "  TF-IDF: 48차원 (희소)\n",
      "  Word2Vec: 100차원 (밀집)\n",
      "  FastText: 100차원 (밀집)\n",
      "  BERT: 768차원 (밀집, 문맥적)\n",
      "\n",
      "🎯 각 방법의 특징:\n",
      "  TF-IDF:\n",
      "    ✅ 빠름, 해석 용이\n",
      "    ❌ 단어 순서 무시, 의미 관계 부족\n",
      "  Word2Vec/FastText:\n",
      "    ✅ 의미적 유사성, 효율적\n",
      "    ❌ 문맥 의존성 부족\n",
      "  BERT:\n",
      "    ✅ 문맥 이해, 양방향성, 높은 성능\n",
      "    ❌ 계산 비용 높음, 복잡함\n",
      "\n",
      "💼 실제 사용 가이드:\n",
      "  🚀 빠른 프로토타입: TF-IDF\n",
      "  🎯 일반적 NLP: Word2Vec/FastText\n",
      "  🏆 최고 성능 필요: BERT/Transformers\n"
     ]
    }
   ],
   "source": [
    "print(\"📊 임베딩 차원 비교:\")\n",
    "print(f\"  TF-IDF: {len(tfidf_vectorizer.vocabulary_)}차원 (희소)\")\n",
    "if 'w2v_model' in globals():\n",
    "    print(f\"  Word2Vec: {w2v_model.wv.vector_size}차원 (밀집)\")\n",
    "if 'fasttext_model' in globals():\n",
    "    print(f\"  FastText: {fasttext_model.wv.vector_size}차원 (밀집)\")\n",
    "print(f\"  BERT: {bert_embeddings.shape[1]}차원 (밀집, 문맥적)\")\n",
    "\n",
    "print(f\"\\n🎯 각 방법의 특징:\")\n",
    "print(\"  TF-IDF:\")\n",
    "print(\"    ✅ 빠름, 해석 용이\")\n",
    "print(\"    ❌ 단어 순서 무시, 의미 관계 부족\")\n",
    "\n",
    "print(\"  Word2Vec/FastText:\")\n",
    "print(\"    ✅ 의미적 유사성, 효율적\")\n",
    "print(\"    ❌ 문맥 의존성 부족\")\n",
    "\n",
    "print(\"  BERT:\")\n",
    "print(\"    ✅ 문맥 이해, 양방향성, 높은 성능\")\n",
    "print(\"    ❌ 계산 비용 높음, 복잡함\")\n",
    "\n",
    "print(f\"\\n💼 실제 사용 가이드:\")\n",
    "print(\"  🚀 빠른 프로토타입: TF-IDF\")\n",
    "print(\"  🎯 일반적 NLP: Word2Vec/FastText\")\n",
    "print(\"  🏆 최고 성능 필요: BERT/Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f76137-4b07-4a54-bf21-7a7515eed3f7",
   "metadata": {},
   "source": [
    "#### Transformers 실습 요약\n",
    "\n",
    "✅ BERT 실습 성공!\n",
    "- 처리된 문장 수: 3개\n",
    "- 임베딩 차원: 768차원\n",
    "- 핵심 특징: 문맥 의존적 임베딩\n",
    "\n",
    "🎯 Transformers의 혁신:\n",
    "  1. 문맥 이해 → 같은 단어도 상황에 따라 다른 의미\n",
    "  2. 양방향성 → 앞뒤 문맥 모두 고려\n",
    "  3. 전이 학습 → 대용량 사전 훈련 + 태스크별 미세 조정\n",
    "  4. 범용성 → 하나의 모델로 여러 NLP 태스크 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de907d-d6f3-4301-882e-813d117e55f2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f613c3-dc9f-4cac-804d-5eca8c3d2361",
   "metadata": {},
   "source": [
    "### **방법별 특징 요약**\n",
    "<br>\n",
    "BoW:<br>\n",
    "- 장점: 간단, 빠름, 해석 가능<br>\n",
    "- 단점: 희소 벡터, 단어 순서 무시, 의미 정보 부족<br>\n",
    "- 사용처: 문서 분류, 텍스트 검색<br>\n",
    "<br>\n",
    "N-gram:<br>\n",
    "- 장점: 지역적 단어 순서 고려, BoW 확장<br>\n",
    "- 단점: 차원 폭발, 희소성 증가<br>\n",
    "- 사용처: 언어 모델링, 구문 패턴 분석<br>\n",
    "<br>\n",
    "TF-IDF:<br>\n",
    "- 장점: 단어 중요도 반영, 일반적인 단어 억제<br>\n",
    "- 단점: 여전히 희소 벡터, 의미 정보 제한<br>\n",
    "- 사용처: 정보 검색, 문서 유사도<br>\n",
    "<br>\n",
    "Word2Vec:<br>\n",
    "- 장점: 밀집 벡터, 의미적 유사도 포착<br>\n",
    "- 단점: OOV 문제, 정적 임베딩<br>\n",
    "- 사용처: 단어 유사도, 의미적 추론<br>\n",
    "<br>\n",
    "GloVe:<br>\n",
    "- 장점: 전역 통계 정보 활용, 좋은 성능<br>\n",
    "- 단점: OOV 문제, 정적 임베딩<br>\n",
    "- 사용처: 단어 임베딩, 전이 학습<br>\n",
    "<br>\n",
    "FastText:<br>\n",
    "- 장점: OOV 처리 가능, 하위 단어 정보 활용<br>\n",
    "- 단점: 메모리 사용량 많음<br>\n",
    "- 사용처: 형태학이 복잡한 언어, OOV가 많은 상황<br>\n",
    "<br>\n",
    "Transformers:<br>\n",
    "- 장점: 문맥적 임베딩, 최고 성능, 다양한 작업 가능<br>\n",
    "- 단점: 계산 비용 높음, 메모리 사용량 많음<br>\n",
    "- 사용처: 최신 NLP 작업 전반<br>\n",
    "<br>\n",
    "각 방법은 상황에 따라 장단점이 있으므로, 데이터 특성과 작업 목표에 맞게 선택하세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Course (Python 3.9)",
   "language": "python",
   "name": "ai_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
