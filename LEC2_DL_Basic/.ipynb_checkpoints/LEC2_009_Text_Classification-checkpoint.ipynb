{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa61081b-8b84-4d24-9cf5-f189c5c86baf",
   "metadata": {},
   "source": [
    "### Created on 2025\n",
    "### @author: S.W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358ebe7-8628-4f93-af3b-b9d0b87ce441",
   "metadata": {},
   "source": [
    "### 0. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52da7d53-665b-4519-8f31-3e6d9e9f118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b58166-df2a-4378-9d72-1424c540c902",
   "metadata": {},
   "source": [
    "#### 1. 데이터 로드  (IMDb 리뷰: label 0=부정, 1=긍정)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483e8d69-4729-4837-903c-1990538e85a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  데이터셋 기본 정보:\n",
      "  전체 구조: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "  훈련 세트 크기: 25,000개\n",
      "  테스트 세트 크기: 25,000개\n"
     ]
    }
   ],
   "source": [
    "raw_ds = load_dataset(\"imdb\")                 # train / test 분리돼 제공\n",
    "\n",
    "# 데이터셋 구조 확인\n",
    "print(f\"  데이터셋 기본 정보:\")\n",
    "print(f\"  전체 구조: {raw_ds}\")\n",
    "print(f\"  훈련 세트 크기: {len(raw_ds['train']):,}개\")\n",
    "print(f\"  테스트 세트 크기: {len(raw_ds['test']):,}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21fe1d1d-ebb2-4f6d-bbd1-0c1d28a71f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_ds[\"train\"].shuffle(SEED).select(range(8000))   # 데모용 소규모 샘플\n",
    "test_ds  = raw_ds[\"test\"].shuffle(SEED).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbe6fe5-dfb6-4a73-a8c6-a69cbd63ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 첫 번째 샘플 확인:\n",
      "  리뷰 내용 (처음 200자): 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. F...'\n",
      "  감정 레이블: 1 (긍정)\n",
      "\n",
      "📈 훈련 데이터 레이블 분포 확인:\n",
      "  긍정 리뷰 (label=1): 12,500개 (50.0%)\n",
      "  부정 리뷰 (label=0): 12,500개 (50.0%)\n",
      "  → 균형잡힌 데이터셋입니다!\n"
     ]
    }
   ],
   "source": [
    "# 데이터 샘플 확인\n",
    "print(f\"\\n🔍 첫 번째 샘플 확인:\")\n",
    "first_sample = train_ds[0]\n",
    "print(f\"  리뷰 내용 (처음 200자): '{first_sample['text'][:200]}...'\")\n",
    "print(f\"  감정 레이블: {first_sample['label']} ({'긍정' if first_sample['label'] == 1 else '부정'})\")\n",
    "\n",
    "# 레이블 분포 확인\n",
    "print(f\"\\n📈 훈련 데이터 레이블 분포 확인:\")\n",
    "train_labels = [sample['label'] for sample in raw_ds[\"train\"]]\n",
    "positive_count = sum(train_labels)\n",
    "negative_count = len(train_labels) - positive_count\n",
    "\n",
    "print(f\"  긍정 리뷰 (label=1): {positive_count:,}개 ({positive_count/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"  부정 리뷰 (label=0): {negative_count:,}개 ({negative_count/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"  → 균형잡힌 데이터셋입니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77e915-b246-428b-9710-42b7d007914b",
   "metadata": {},
   "source": [
    "#### 2. 토크나이저 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65265247-3bd4-4302-ab84-acda4f63d068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델의 체크포인트 이름 설정\n",
    "# \"bert-base-uncased\": 소문자로 변환된 기본 BERT 모델\n",
    "# - base: 12개 레이어, 768차원 히든 사이즈 (small 버전도 있음)\n",
    "# - uncased: 대소문자를 구분하지 않음 (모든 텍스트가 소문자로 변환됨)\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "\n",
    "# 사전 훈련된 BERT 토크나이저 로드\n",
    "# 토크나이저: 텍스트를 모델이 이해할 수 있는 숫자(토큰 ID)로 변환하는 도구\n",
    "# BERT는 WordPiece 토크나이저를 사용 (단어를 서브워드로 분할)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize(batch):\n",
    "    \"\"\"\n",
    "    배치 단위로 텍스트를 토크나이즈하는 함수\n",
    "    \n",
    "    Args:\n",
    "        batch (dict): 'text' 키를 가진 배치 데이터\n",
    "        \n",
    "    Returns:\n",
    "        dict: 토크나이즈된 결과 (input_ids, attention_mask 등)\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],           # 토크나이즈할 텍스트 리스트\n",
    "        truncation=True         # 최대 길이를 초과하면 자동으로 잘라내기\n",
    "                               # BERT의 기본 최대 길이는 512 토큰\n",
    "    )\n",
    "\n",
    "# 훈련 데이터셋에 토크나이제이션 적용\n",
    "# map 함수: 데이터셋의 모든 샘플에 대해 tokenize 함수 적용\n",
    "train_ds = train_ds.map(\n",
    "    tokenize,                   # 적용할 함수\n",
    "    batched=True,              # 배치 단위로 처리 (속도 향상)\n",
    "    remove_columns=[\"text\"]    # 원본 텍스트 컬럼 제거 (메모리 절약)\n",
    ")\n",
    "\n",
    "# 테스트 데이터셋에도 동일하게 적용\n",
    "test_ds = test_ds.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# 데이터 콜레이터 설정\n",
    "# DataCollatorWithPadding: 배치 내에서 길이가 다른 시퀀스들을 동일한 길이로 맞춤\n",
    "# - 짧은 시퀀스에는 패딩 토큰([PAD])을 추가\n",
    "# - attention_mask도 자동으로 생성 (실제 토큰은 1, 패딩은 0)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf5e8b-74ef-48fb-96d2-b887b90d3669",
   "metadata": {},
   "source": [
    "#### 3. 모델 초기화 (출력 라벨 2개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78177da7-07f2-4d33-a9aa-dfedf61c90a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 사전 훈련된 BERT 모델을 분류 작업용으로 로드\n",
    "# AutoModelForSequenceClassification: 시퀀스 분류를 위한 모델 클래스\n",
    "# - 기존 BERT 모델 위에 분류용 헤드(classifier head)가 추가됨\n",
    "# - 분류 헤드: 768차원 → num_labels차원으로 변환하는 선형 레이어\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt,          # 사용할 사전 훈련 모델 (\"bert-base-uncased\")\n",
    "    num_labels=2         # 출력 라벨의 개수 (이진 분류이므로 2개)\n",
    "                        # 예: 긍정(1), 부정(0) 또는 스팸(1), 정상(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdbebf0-70e0-42f3-905a-c1f75bf5d641",
   "metadata": {},
   "source": [
    "#### 4. 평가지표 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da683bef-518c-45fd-9931-6751f019b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    모델의 성능을 평가하는 함수\n",
    "    Trainer에서 validation 중에 자동으로 호출됨\n",
    "    \n",
    "    Args:\n",
    "        eval_pred (EvalPrediction): 예측 결과와 정답 라벨을 담은 객체\n",
    "            - predictions: 모델의 예측값 (로짓/점수)\n",
    "            - label_ids: 실제 정답 라벨\n",
    "    \n",
    "    Returns:\n",
    "        dict: 평가 지표들의 딕셔너리\n",
    "    \"\"\"\n",
    "    \n",
    "    # eval_pred에서 예측값(로짓)과 정답 라벨 추출\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # 로짓을 실제 예측 클래스로 변환\n",
    "    # argmax: 가장 큰 값의 인덱스를 반환\n",
    "    # axis=-1: 마지막 차원(클래스 차원)에서 최대값 찾기\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    \n",
    "    # 평가 지표 계산 및 반환\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17c328-ecf4-4bb3-963f-1854f4878e0b",
   "metadata": {},
   "source": [
    "#### 5. 학습 세팅 및 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd57bc6-c40e-43ff-a88f-ae561bc398a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212452</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.922167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9210,  F1: 0.9222\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-imdb\",\n",
    "    num_train_epochs=1,              # 데모용 1 epoch\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),  # GPU-FP16 가속\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"\\nTest accuracy: {metrics['eval_accuracy']:.4f},  F1: {metrics['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1bb59-277b-4a75-922c-76a2214a7fde",
   "metadata": {},
   "source": [
    "#### 6. 추론(Inference) 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b68508d6-904e-4b84-a5d9-faed105bc7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/crocus/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/crocus/.cache/torch_extensions/py39_cu118/cuda_kernel/build.ninja...\n",
      "Building extension module cuda_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] /usr/local/cuda-11.8/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/TH -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda-11.8/include -isystem /home/crocus/anaconda3/envs/keri_test/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -std=c++17 -c /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/transformers/kernels/mra/cuda_kernel.cu -o cuda_kernel.cuda.o \n",
      "[2/4] c++ -MMD -MF torch_extension.o.d -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/TH -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda-11.8/include -isystem /home/crocus/anaconda3/envs/keri_test/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/transformers/kernels/mra/torch_extension.cpp -o torch_extension.o \n",
      "[3/4] /usr/local/cuda-11.8/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/TH -isystem /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda-11.8/include -isystem /home/crocus/anaconda3/envs/keri_test/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -std=c++17 -c /home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/transformers/kernels/mra/cuda_launch.cu -o cuda_launch.cuda.o \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cuda_kernel...\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/4] c++ cuda_kernel.cuda.o cuda_launch.cuda.o torch_extension.o -shared -L/home/crocus/anaconda3/envs/keri_test/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-11.8/lib64 -lcudart -o cuda_kernel.so\n",
      "\n",
      "=== Prediction Examples ===\n",
      "This movie was an absolute masterpiece. Brilliant acting!  ->  {'label': 'LABEL_1', 'score': 0.9814226627349854}\n",
      "I wish I could get my two hours back. It was painfully boring.  ->  {'label': 'LABEL_0', 'score': 0.9322379231452942}\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "samples = [\n",
    "    \"This movie was an absolute masterpiece. Brilliant acting!\",\n",
    "    \"I wish I could get my two hours back. It was painfully boring.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Prediction Examples ===\")\n",
    "for s in samples:\n",
    "    print(f\"{s}  ->  {clf(s)[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Course (Python 3.9)",
   "language": "python",
   "name": "ai_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
